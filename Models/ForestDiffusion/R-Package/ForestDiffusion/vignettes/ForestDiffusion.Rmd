---
title: "How to use Diffusion Forests to generate and impute missing data (from basic to advanced usage)"
author: "Alexia Jolicoeur-Martineau"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Latent Environmental & Genetic InTeraction (LEGIT) modelling}
  %\VignetteEncoding{UTF-8}
---

You can cite this work as:

*Jolicoeur-Martineau, A, Fatras, K., Kachman, T. (2023). Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees. arXiv preprint arXiv:2309.09968.*

## Score-based diffusion and flow-based models (high-level idea)

You can refer to the paper if you want to know more about the mathematics and the algorithm. In this vignette, I will stay at a high level. 

The idea behind score-based diffusion models is that we can define a forward process that adds increasing amounts of Gaussian noise over time to slowly move from a real data sample (at $t=0$) to pure Gaussian noise (at $t=1$). The magic is that it can be shown that this process is reversible, which means that we can go in reverse from pure noise ($t=1$) to real data ($t=0$) and thus generate new data samples from pure noise. To reverse the process, we need to learn the score-function (gradient log density) with a function approximator (XGBoost is used in this case). 

Alternatively, flow-based models define a deterministic forward process moving from real data to pure Gaussian noise. Then, they learn the gradient flow, which can be used to move in reverse (from noise to data). 

Both diffusion (stochastic SDE-based) and flow (deterministic ODE-based) methods are available in this package. To my knowledge, this is the first R package implementing diffusion and flow models.

## Hyperparameters

You must set the maximum amount of cores used by ForestDiffusion through the argument $n_{cores}$. The amount of cores you have depends on your CPU. The training of the $pn_t$ models (where $p$ is the number of variables and $n_t$ the number of noise levels, which defaults to 50) is parallelized over the $n_{cores}$ cores. The get the maximum performance, set the argument $n_{cores}$ to NULL in order to use all available cores. The more cores you use, the faster it will be. On the other hand, memory can be a problem, especially if you have a lot of cores; please see the section below on memory management.

We list the important hyperparameters below, their default values, and how to choose them:

```{r, eval=FALSE}
n_cores = NULL # maximum amount of cores used; leaving it at NULL will use all availables cores; higher values increase training speed, but also memory cost
X # your dataset 
label_y = None # provide the outcome variable if it is categorical for improved performance by training separate models per class (training will be slower); cannot contain missing values
name_y = 'y' # Name of label_y variable if provided
n_t = 50 # number of noise levels (and sampling steps); increasing it could (maybe) improve performance, but it slows down training and sampling
flow = TRUE # type of process (flow = ODE, vp = SDE); vp generally has slightly worse performance, but it is the only method that can be used for imputation
duplicate_K = 100 # number of noise per sample (or equivalently the number of times the rows of the dataset are duplicated); higher values lead to better performance, but also increase the memory demand
seed = 666 # random seed value
max_depth = 7 # max depth of the tree
n_estimators = 100 # number of trees per XGBoost model
```

Regarding the imputation with REPAINT, there are two important hyperparameters:
```{r, eval=FALSE}
r = 10 # number of repaints, 5 or 10 is good
j = 0.1 # percentage of the jump size; should be around 10% of n_t
```

## Potential memory problems and solutions ðŸ˜­

Our method trains $pn_t$ models in parallel using CPU cores, where p is the number of variables and n_t is the number of noise levels. Furthermore, we make the dataset much bigger by duplicating the rows many times (100 times is the default). To speed up the training, you will need as many cores as possible. Training the multiple models using only 4 cores could take a long time. However, the more cores you use, the higher the memory cost will be! This is because each worker/CPU will train its own model, which will require its own amount of memory (RAM). So, there is a balance to be reached between enough cores for speed but not too much so that it doesn't blow up the memory.

We provide below some hyperparameters that can be changed to reduce the memory load:
```
n_cores = NULL # this can be used to limit the maximum number of cores and thus the memory
duplicate_K = 100 # lowering this value will reduce memory demand and possibly performance (memory is proportional to this value)
n_t = 50 # reducing this value could reduce memory demand and performance (ideally stay at n_t=50 or higher)
label_y = None # using None will reduce memory demand (since using this will train n_classes times more models)
max_depth = 7 # reducing the depth of trees will reduce memory demand
n_estimators = 100 # reducing the number of trees will reduce memory demand
```


## Generating data

Let's use the Iris dataset as an example. Since it performs better, we will use the flow method to generate fake samples. Note that the dataset can contain missing values since XGBoost can handle NAs, yet the generated data will never have missing values (isn't it great?).

```{r, fig.width=6, fig.height=6, eval=FALSE}
library(ForestDiffusion)

# Load iris
data(iris)
# variables 1 to 4 are the input X
# variable 5 (iris$Species) is the outcome (class with 3 labels)

# Add NAs (but not to label) to emulate having a dataset with missing values
iris[,1:4] = missForest::prodNA(iris[,1:4], noNA = 0.2)

# Setup data
X = data.frame(iris[,1:4])
y = iris$Species
Xy = iris
plot(Xy)

# When you do not want to train a seperate model per model (or you have a regression problem), you can provide the dataset together
forest_model = ForestDiffusion(X=Xy, n_cores=4, n_t=50, duplicate_K=100, flow=TRUE, seed=666)

Xy_fake = ForestDiffusion.generate(forest_model, batch_size=NROW(Xy), seed=666)
plot(Xy_fake)

# When the outcome y is categorical, you can provide it seperately to construct a seperate model per label (this can improve performance, but it will be slower)
forest_model = ForestDiffusion(X=X, n_cores=4, label_y=y, name_y='Species', n_t=50, duplicate_K=100, flow=TRUE, seed=666)

Xy_fake = ForestDiffusion.generate(forest_model, batch_size=NROW(Xy), seed=666)
plot(Xy_fake)

```

Now that you have your fake data, you can use it in your own models directly or combine it with the real data.

```{r, eval=FALSE}
# Use the real data to fit a GLM
fit = glm(Species ~ Sepal.Length, family = 'binomial', data=Xy)
summary(fit)

# Use fake data to fit a GLM
fit = glm(Species ~ Sepal.Length, family = 'binomial', data=Xy_fake)
summary(fit)

# Use data augmentation (equal real with equal fake data) to fit a GLM
X_combined = data.frame(rbind(Xy, Xy_fake))
fit = glm(Species ~ Sepal.Length, family = 'binomial', data=X_combined)
summary(fit)
```

## Data augmentation

One possible application of our method is data augmentation (augmenting real data with additional fake samples) to improve various methods. Here is an example of data augmentation with missForest.

```{r, eval=FALSE}
library(missForest)

# Normally, you would use missForest as follows
mf = missForest::missForest(Xy, verbose = TRUE)

# Instead, you can now use data augmentation
Xy_fake = ForestDiffusion.generate(forest_model, batch_size=NROW(Xy), seed=666) # generates as much fake as real data
X_combined = data.frame(rbind(Xy, Xy_fake)) # combine real and fake data
mf_dataug = missForest(X_combined, verbose = TRUE) # train missForest with augmented data

```

## Accounting for uncertainty using multiple fake datasets (akin to multiple imputations)

Training a single model with fake or data-augmented data is nice, but it might not account for uncertainty since you trained a single model. When imputing data, we generally want to use multiple imputed datasets, train our model on each imputed dataset, and then pool the results in order to account for the different possible imputations. We can apply the same idea here but with fake data! Let me show you how.

```{r, eval=FALSE}
library(mice)

# Generate fake data
ngen = 9 # number of generated datasets we want
Xy_fake = ForestDiffusion.generate(forest_model, batch_size=ngen*NROW(Xy), seed=666)

# Make a list of fake datasets
data_list = split(Xy_fake, rep(1:ngen, each=NROW(Xy)))

# Fit a model per fake dataset
fits <- with_datasets(data_list, glm(Species ~ Sepal.Length, family = 'binomial'))

# Pool the results
mice::pool(fits) 

```

## Multiple imputation

Below, we show how to impute missing data using ForestDiffusion.

```{r, eval=FALSE}
library(mice)

nimp = 5 # number of imputations needed

# Must train a VP diffusion model (instead of a Flow model) to be able to impute data
forest_model_vp = ForestDiffusion(X=Xy, n_cores=4, n_t=50, duplicate_K=100, flow=FALSE, seed=666)

Xy_imp = ForestDiffusion.impute(forest_model_vp, k=nimp, seed=666) # regular imputations (fast)
Xy_imp = ForestDiffusion.impute(forest_model_vp, repaint=TRUE, r=10, j=5, k=nimp, seed=666) # REPAINT imputations (slow, but better)
plot(Xy_imp[[1]]) # plot the first imputed dataset

# When the outcome y is categorical, you can provide it seperately to construct a seperate model per label (this can improve performance, but it will be slower)
forest_model_vp = ForestDiffusion(X=X, n_cores=4, label_y=y, name_y='Species', n_t=50, duplicate_K=100, flow=TRUE, seed=666)

Xy_imp = ForestDiffusion.impute(forest_model_vp, k=nimp, seed=666) # regular imputations (fast)
Xy_imp = ForestDiffusion.impute(forest_model_vp, repaint=TRUE, r=10, j=5, k=nimp, seed=666) # REPAINT imputations (slow, but better)
plot(Xy_imp[[1]]) # plot the first imputed dataset
```

Now that you have created multiple imputations, you can use fit one model per imputation and pool the results.

```{r, eval=FALSE}
# Fit a model per imputed dataset
fits <- with_datasets(Xy_imp, glm(Species ~ Sepal.Length, family = 'binomial'))

# Pool the results
mice::pool(fits) 

```


