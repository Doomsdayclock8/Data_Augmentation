{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook tries to augment data using forest diffusion on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla ForestDiffusion on Oils dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 4000, 1: 50}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "#file_path = 'creditcard.csv'  # Update this path to your local CSV file\n",
    "dataset = 'creditcard_sampled'\n",
    "file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect the data and check for class imbalance\n",
    "# Assuming the last column is the label, and the rest are features\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "# # Step 3: Plot the original imbalanced data (first two features for visualization)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', label='Original Data', s=1)\n",
    "# plt.title('Original Imbalanced Data')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.show()\n",
    "\n",
    "# Separate the minority class\n",
    "X_minority = X[y == 1]\n",
    "y_minority = y[y==1]\n",
    "# Identify integer columns\n",
    "int_columns = data.select_dtypes(include=['int']).columns\n",
    "int_indexes = []\n",
    "for col in int_columns:\n",
    "    col_index = data.columns.get_loc(col)\n",
    "    int_indexes.append(col_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 30)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "y_minority.shape\n",
    "# y_minority1=np.expand_dims(y_minority, axis=1)\n",
    "print(X_minority.shape)\n",
    "print(y_minority.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Step 4: Upsample the minority class using ForestDiffusionModel\n",
    "forest_model = ForestDiffusionModel(X_minority, label_y=y_minority, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)\n",
    "Xy_minority_fake = forest_model.generate(batch_size=100 )  # Adjust the batch size to create a balanced dataset\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add generated samples to the main imbalanced dataset\n",
    "X_minority_fake = Xy_minority_fake[:, :-1]   # Features\n",
    "y_minority_fake = Xy_minority_fake[:, -1] # Labels (binary classification)\n",
    "X_balanced = np.concatenate((X, X_minority_fake), axis=0)\n",
    "y_balanced = np.concatenate((y, y_minority_fake), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after augmentation: {0.0: 4000, 1.0: 150}\n",
      "Precision score (original data): 0.8667\n",
      "Precision score (generated data): 0.8824\n",
      "Recall score (original data): 0.7647\n",
      "Recall score (generated data): 0.8824\n",
      "F1 score (original data): 0.8125\n",
      "F1 score (generated data): 0.8824\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.93      0.88      0.90      1215\n",
      "weighted avg       0.99      1.00      0.99      1215\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.88      0.88      0.88        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.94      0.94      0.94      1215\n",
      "weighted avg       1.00      1.00      1.00      1215\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "# # Step 5: Plot the generated data (first two features for visualization)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(X_balanced[:, 0], X_balanced[:, 1], c=y_balanced, cmap='viridis', label='Generated Data', s=1)\n",
    "# plt.title('Data After Generation')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.show()\n",
    "\n",
    "# Check and print the class distribution after augmentation\n",
    "unique_bal, counts_bal = np.unique(y_balanced, return_counts=True)\n",
    "class_dist_after = dict(zip(unique_bal, counts_bal))\n",
    "print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "# Step 10: Print the number of fake samples generated\n",
    "print(f\"Number of fake samples generated: {len(X_minority_fake)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
