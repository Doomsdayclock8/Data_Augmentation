{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook tries to augment data using forest diffusion on datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla ForestDiffusion on Oils dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Result Metrics for Vanilla ForestDiffusion for mammography dataset\n",
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Class distribution after augmentation: {-1.0: 10923, 1.0: 360}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'real_minortiy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 106\u001b[0m\n\u001b[0;32m     94\u001b[0m f1_bal \u001b[38;5;241m=\u001b[39m f1_score(y_test_orig, y_pred_bal)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Step 9: Print the performance metrics\u001b[39;00m\n\u001b[0;32m     97\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset,\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision_Original\u001b[39m\u001b[38;5;124m\"\u001b[39m: prec_orig,\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall_Original\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall_orig,\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_Original\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_orig,\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision_Generated\u001b[39m\u001b[38;5;124m\"\u001b[39m: prec_bal,\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecall_Generated\u001b[39m\u001b[38;5;124m\"\u001b[39m: recalls_bal,\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_Generated\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1_bal,\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNum_Fake_Samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(X_balanced) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(X),\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthetic/Original_Ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_balanced) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(X))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mreal_minortiy\u001b[49m)\n\u001b[0;32m    107\u001b[0m }\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Append the dictionary to the list\u001b[39;00m\n\u001b[0;32m    110\u001b[0m metrics_list\u001b[38;5;241m.\u001b[39mappend(metrics)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'real_minortiy' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "\n",
    "metrics_list = []\n",
    "# Step 1: Load the CSV file\n",
    "#file_path = 'creditcard.csv'  # Update this path to your local CSV file\n",
    "# strings_set = {'diabetes','oil','yeast_ml8_dataset','creditcard_sampled','HTRU','mammography'}\n",
    "strings_set = {'diabetes','oil','creditcard_sampled','HTRU','mammography'}\n",
    "for dataset in strings_set:\n",
    "    print(f\"# Result Metrics for Vanilla ForestDiffusion for {dataset} dataset\")\n",
    "    file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Step 2: Inspect the data and check for class imbalance\n",
    "    # Assuming the last column is the label, and the rest are features\n",
    "    X = data.iloc[:, :-1].values  # Features\n",
    "    y = data.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "    # Check and print the original class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist_before = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "    # # Step 3: Plot the original imbalanced data (first two features for visualization)\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', label='Original Data', s=1)\n",
    "    # plt.title('Original Imbalanced Data')\n",
    "    # plt.xlabel('Feature 1')\n",
    "    # plt.ylabel('Feature 2')\n",
    "    # plt.show()\n",
    "\n",
    "    # Separate the minority class\n",
    "    X_minority = X[y == 1]\n",
    "    y_minority = y[y==1]\n",
    "    # Identify integer columns\n",
    "    int_columns = data.select_dtypes(include=['int']).columns\n",
    "    int_indexes = []\n",
    "    for col in int_columns:\n",
    "        col_index = data.columns.get_loc(col)\n",
    "        int_indexes.append(col_index)\n",
    "    import pandas as pd\n",
    "    # Step 4: Upsample the minority class using ForestDiffusionModel\n",
    "    forest_model = ForestDiffusionModel(X_minority, label_y=y_minority, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)\n",
    "    Xy_minority_fake = forest_model.generate(batch_size=100 )  # Adjust the batch size to create a balanced dataset\n",
    "    # Add generated samples to the main imbalanced dataset\n",
    "    X_minority_fake = Xy_minority_fake[:, :-1]   # Features\n",
    "    y_minority_fake = Xy_minority_fake[:, -1] # Labels (binary classification)\n",
    "    X_balanced = np.concatenate((X, X_minority_fake), axis=0)\n",
    "    y_balanced = np.concatenate((y, y_minority_fake), axis=0)\n",
    "    \n",
    "    # # Step 5: Plot the generated data (first two features for visualization)\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.scatter(X_balanced[:, 0], X_balanced[:, 1], c=y_balanced, cmap='viridis', label='Generated Data', s=1)\n",
    "    # plt.title('Data After Generation')\n",
    "    # plt.xlabel('Feature 1')\n",
    "    # plt.ylabel('Feature 2')\n",
    "    # plt.show()\n",
    "\n",
    "    # Check and print the class distribution after augmentation\n",
    "    unique_bal, counts_bal = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique_bal, counts_bal))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Train a simple classifier on both original and generated datasets\n",
    "    clf_orig = RandomForestClassifier(random_state=42)\n",
    "    clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Step 8: Predict and calculate recall and F1 scores\n",
    "    y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "    y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "    prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "    prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "\n",
    "    recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "    recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "    f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    # Step 9: Print the performance metrics\n",
    "    metrics = {\n",
    "        \"Dataset\": dataset,\n",
    "        \"Precision_Original\": prec_orig,\n",
    "        \"Recall_Original\": recall_orig,\n",
    "        \"F1_Original\": f1_orig,\n",
    "        \"Precision_Generated\": prec_bal,\n",
    "        \"Recall_Generated\": recalls_bal,\n",
    "        \"F1_Generated\": f1_bal,\n",
    "        \"Num_Fake_Samples\": len(X_balanced) - len(X),\n",
    "        \"Synthetic/Original_Ratio\":100*(len(X_balanced) - len(X))/len(Xy_minority_fake)\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    metrics_list.append(metrics)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "    print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "    print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "    print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "    print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "    print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "    print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "    print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "    # Step 10: Print the number of fake samples generated\n",
    "    print(f\"Number of fake samples generated: {len(X_minority_fake)}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mmetrics_list\u001b[49m)\n\u001b[0;32m      3\u001b[0m metrics_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert the list of dictionaries into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(\"Vanilla_Forest_different_datasets_metric.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
