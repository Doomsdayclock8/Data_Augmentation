{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4494999,"sourceType":"datasetVersion","datasetId":2628764}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/SamsungSAILMontreal/ForestDiffusion\n!pip install ForestDiffusion\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:50:18.746246Z","iopub.execute_input":"2024-12-05T11:50:18.747258Z","iopub.status.idle":"2024-12-05T11:50:33.145269Z","shell.execute_reply.started":"2024-12-05T11:50:18.747215Z","shell.execute_reply":"2024-12-05T11:50:33.143712Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'ForestDiffusion'...\nremote: Enumerating objects: 447, done.\u001b[K\nremote: Counting objects: 100% (148/148), done.\u001b[K\nremote: Compressing objects: 100% (63/63), done.\u001b[K\nremote: Total 447 (delta 82), reused 129 (delta 72), pack-reused 299 (from 1)\u001b[K\nReceiving objects: 100% (447/447), 901.47 KiB | 26.51 MiB/s, done.\nResolving deltas: 100% (215/215), done.\nCollecting ForestDiffusion\n  Downloading ForestDiffusion-1.0.6-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (1.2.2)\nRequirement already satisfied: xgboost>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (2.0.3)\nRequirement already satisfied: lightgbm in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (4.2.0)\nRequirement already satisfied: catboost in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (1.2.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from ForestDiffusion) (2.2.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost>=2.0.0->ForestDiffusion) (1.14.1)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost->ForestDiffusion) (0.20.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from catboost->ForestDiffusion) (3.7.5)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost->ForestDiffusion) (5.22.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost->ForestDiffusion) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->ForestDiffusion) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->ForestDiffusion) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->ForestDiffusion) (2024.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->ForestDiffusion) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->ForestDiffusion) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->catboost->ForestDiffusion) (3.1.2)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost->ForestDiffusion) (8.3.0)\nDownloading ForestDiffusion-1.0.6-py3-none-any.whl (14 kB)\nInstalling collected packages: ForestDiffusion\nSuccessfully installed ForestDiffusion-1.0.6\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!cp /kaggle/input/oil-spill-detection/oil_spill.csv /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:51:47.973474Z","iopub.execute_input":"2024-12-05T11:51:47.974024Z","iopub.status.idle":"2024-12-05T11:51:49.070212Z","shell.execute_reply.started":"2024-12-05T11:51:47.973974Z","shell.execute_reply":"2024-12-05T11:51:49.068624Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# ##Equal Opportunity","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nfrom ForestDiffusion import ForestDiffusionModel\n\n# Load data\ndata = pd.read_csv('/kaggle/working/oil_spill.csv')\n\n# Extract features and target\nX = data.iloc[:, 1:-1].values\ny = data.iloc[:, -1].values\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    cm = confusion_matrix(y_true, y_pred, labels=[-1, 1])  # Specify class order\n    index = 1 if target_class == 1 else 0  # Map 1 -> minority, -1 -> majority\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=-1)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\nint_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=int_indexes,\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0])), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=-1)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T06:22:45.271284Z","iopub.execute_input":"2024-12-05T06:22:45.271716Z","iopub.status.idle":"2024-12-05T07:09:20.772663Z","shell.execute_reply.started":"2024-12-05T06:22:45.271677Z","shell.execute_reply":"2024-12-05T07:09:20.771069Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {0: 896, 1: 41}\nRecall score (original data): 0.2500\nF1 score (original data): 0.3750\nTPR (minority class) before augmentation: 1.0000\nTPR (majority class) before augmentation: 0.0000\nClass distribution after augmentation: {0.0: 896, 1.0: 228}\nRecall score (generated data): 0.9167\nF1 score (generated data): 0.9167\nTPR (minority class) after augmentation: 1.0000\nTPR (majority class) after augmentation: 0.0000\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98       270\n           1       0.75      0.25      0.38        12\n\n    accuracy                           0.96       282\n   macro avg       0.86      0.62      0.68       282\nweighted avg       0.96      0.96      0.96       282\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       270\n           1       0.92      0.92      0.92        12\n\n    accuracy                           0.99       282\n   macro avg       0.96      0.96      0.96       282\nweighted avg       0.99      0.99      0.99       282\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# RandomForest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nfrom ForestDiffusion import ForestDiffusionModel\n\n# Load data\ndata = pd.read_csv('/kaggle/working/oil_spill.csv')\n\n# Extract features and target\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate the True Positive Rate (TPR) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR\n\n    Returns:\n    - TPR value\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Use [0, 1] since 0 is majority and 1 is minority\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=0)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\nint_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=int_indexes,\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=0)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T07:19:14.435768Z","iopub.execute_input":"2024-12-05T07:19:14.437369Z","iopub.status.idle":"2024-12-05T08:07:35.031212Z","shell.execute_reply.started":"2024-12-05T07:19:14.437316Z","shell.execute_reply":"2024-12-05T08:07:35.029694Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {0: 896, 1: 41}\nRecall score (original data): 0.3333\nF1 score (original data): 0.4706\nTPR (minority class) before augmentation: 0.3333\nTPR (majority class) before augmentation: 0.9963\nClass distribution after augmentation: {0: 896, 1: 228}\nRecall score (generated data): 0.8333\nF1 score (generated data): 0.8333\nTPR (minority class) after augmentation: 0.8333\nTPR (majority class) after augmentation: 0.9926\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98       270\n           1       0.80      0.33      0.47        12\n\n    accuracy                           0.97       282\n   macro avg       0.89      0.66      0.73       282\nweighted avg       0.96      0.97      0.96       282\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       270\n           1       0.83      0.83      0.83        12\n\n    accuracy                           0.99       282\n   macro avg       0.91      0.91      0.91       282\nweighted avg       0.99      0.99      0.99       282\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nimport xgboost as xgb\nfrom ForestDiffusion import ForestDiffusionModel\n\n# Load data\ndata = pd.read_csv('/kaggle/working/oil_spill.csv')\n\n# Extract features and target\nX = data.iloc[:, 1:-1].values\ny = data.iloc[:, -1].values\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data using XGBoost\nclf_orig = xgb.XGBClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate the True Positive Rate (TPR) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR\n\n    Returns:\n    - TPR value\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Use [0, 1] since 0 is majority and 1 is minority\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=0)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\nint_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=int_indexes,\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data using XGBoost\nclf_bal = xgb.XGBClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=0)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T08:24:41.025230Z","iopub.execute_input":"2024-12-05T08:24:41.025923Z","iopub.status.idle":"2024-12-05T09:11:11.634173Z","shell.execute_reply.started":"2024-12-05T08:24:41.025857Z","shell.execute_reply":"2024-12-05T09:11:11.632292Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {0: 896, 1: 41}\nRecall score (original data): 0.3333\nF1 score (original data): 0.4706\nTPR (minority class) before augmentation: 0.3333\nTPR (majority class) before augmentation: 0.9963\nClass distribution after augmentation: {0: 896, 1: 228}\nRecall score (generated data): 0.7500\nF1 score (generated data): 0.7500\nTPR (minority class) after augmentation: 0.7500\nTPR (majority class) after augmentation: 0.9889\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n           0       0.97      1.00      0.98       270\n           1       0.80      0.33      0.47        12\n\n    accuracy                           0.97       282\n   macro avg       0.89      0.66      0.73       282\nweighted avg       0.96      0.97      0.96       282\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n           0       0.99      0.99      0.99       270\n           1       0.75      0.75      0.75        12\n\n    accuracy                           0.98       282\n   macro avg       0.87      0.87      0.87       282\nweighted avg       0.98      0.98      0.98       282\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom ForestDiffusion import ForestDiffusionModel\n\n# Load data\ndata = pd.read_csv('/kaggle/working/oil_spill.csv')\n\n# Extract features and target\nX = data.iloc[:, 1:-1].values\ny = data.iloc[:, -1].values\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data using DecisionTreeClassifier\nclf_orig = DecisionTreeClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate the True Positive Rate (TPR) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR\n\n    Returns:\n    - TPR value\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Use [0, 1] since 0 is majority and 1 is minority\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=0)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\nint_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=int_indexes,\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data using DecisionTreeClassifier\nclf_bal = DecisionTreeClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=0)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T09:44:05.923226Z","iopub.execute_input":"2024-12-05T09:44:05.923674Z","iopub.status.idle":"2024-12-05T10:30:42.974430Z","shell.execute_reply.started":"2024-12-05T09:44:05.923626Z","shell.execute_reply":"2024-12-05T10:30:42.972807Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {0: 896, 1: 41}\nRecall score (original data): 0.3333\nF1 score (original data): 0.3810\nTPR (minority class) before augmentation: 0.3333\nTPR (majority class) before augmentation: 0.9815\nClass distribution after augmentation: {0: 896, 1: 228}\nRecall score (generated data): 0.7500\nF1 score (generated data): 0.5455\nTPR (minority class) after augmentation: 0.7500\nTPR (majority class) after augmentation: 0.9556\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n           0       0.97      0.98      0.98       270\n           1       0.44      0.33      0.38        12\n\n    accuracy                           0.95       282\n   macro avg       0.71      0.66      0.68       282\nweighted avg       0.95      0.95      0.95       282\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n           0       0.99      0.96      0.97       270\n           1       0.43      0.75      0.55        12\n\n    accuracy                           0.95       282\n   macro avg       0.71      0.85      0.76       282\nweighted avg       0.96      0.95      0.95       282\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# #Mammography Dataset","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom imblearn.datasets import fetch_datasets\nmammography = fetch_datasets()['mammography']\nmammography.data.shape\nprint(sorted(Counter(mammography.target).items()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T10:46:09.827497Z","iopub.execute_input":"2024-12-05T10:46:09.827953Z","iopub.status.idle":"2024-12-05T10:46:10.242904Z","shell.execute_reply.started":"2024-12-05T10:46:09.827914Z","shell.execute_reply":"2024-12-05T10:46:10.241646Z"}},"outputs":[{"name":"stdout","text":"[(-1, 10923), (1, 260)]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# RandomForest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate the True Positive Rate (TPR) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR\n\n    Returns:\n    - TPR value\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[-1, 1])  # Use [-1, 1] since -1 is majority and 1 is minority\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=-1)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\nprint(f\"Number of synthetic samples generated: {X_minority_fake.shape[0]}\")\n\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=-1)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:31:49.381270Z","iopub.execute_input":"2024-12-05T12:31:49.381744Z","iopub.status.idle":"2024-12-05T12:33:08.355125Z","shell.execute_reply.started":"2024-12-05T12:31:49.381704Z","shell.execute_reply":"2024-12-05T12:33:08.353582Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nRecall score (original data): 0.5000\nF1 score (original data): 0.6240\nTPR (minority class) before augmentation: 0.5000\nTPR (majority class) before augmentation: 0.9976\nClass distribution after augmentation: {-1: 10923, 1: 2496}\nNumber of synthetic samples generated: 2236\nRecall score (generated data): 0.9103\nF1 score (generated data): 0.7889\nTPR (minority class) after augmentation: 0.9103\nTPR (majority class) after augmentation: 0.9905\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n          -1       0.99      1.00      0.99      3277\n           1       0.83      0.50      0.62        78\n\n    accuracy                           0.99      3355\n   macro avg       0.91      0.75      0.81      3355\nweighted avg       0.98      0.99      0.98      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n          -1       1.00      0.99      0.99      3277\n           1       0.70      0.91      0.79        78\n\n    accuracy                           0.99      3355\n   macro avg       0.85      0.95      0.89      3355\nweighted avg       0.99      0.99      0.99      3355\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix\nfrom xgboost import XGBClassifier\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Remap target labels from [-1, 1] to [0, 1]\ny = np.where(y == -1, 0, 1)\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\nrecall_orig = recall_score(y_test, y_pred_orig, pos_label=1)\nf1_orig = f1_score(y_test, y_pred_orig, pos_label=1)\nprint(f\"Recall score (original data): {recall_orig:.4f}\")\nprint(f\"F1 score (original data): {f1_orig:.4f}\")\n\n# Define TPR calculation function\ndef calculate_tpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate the True Positive Rate (TPR) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR\n\n    Returns:\n    - TPR value\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Use [0, 1] since labels were remapped\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Avoid division by zero\n    return tpr\n\n# Calculate TPR before augmentation\ntpr_orig_minority = calculate_tpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority = calculate_tpr(y_test, y_pred_orig, target_class=0)\nprint(f\"TPR (minority class) before augmentation: {tpr_orig_minority:.4f}\")\nprint(f\"TPR (majority class) before augmentation: {tpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Print the number of generated samples\nprint(f\"Number of synthetic samples generated: {X_minority_fake.shape[0]}\")\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Check class distribution after augmentation\nunique, counts = np.unique(y_balanced, return_counts=True)\nclass_dist_after = dict(zip(unique, counts))\nprint(f\"Class distribution after augmentation: {class_dist_after}\")\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\nrecall_bal = recall_score(y_test, y_pred_bal, pos_label=1)\nf1_bal = f1_score(y_test, y_pred_bal, pos_label=1)\nprint(f\"Recall score (generated data): {recall_bal:.4f}\")\nprint(f\"F1 score (generated data): {f1_bal:.4f}\")\n\n# Calculate TPR after augmentation\ntpr_bal_minority = calculate_tpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority = calculate_tpr(y_test, y_pred_bal, target_class=0)\nprint(f\"TPR (minority class) after augmentation: {tpr_bal_minority:.4f}\")\nprint(f\"TPR (majority class) after augmentation: {tpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:22:06.322534Z","iopub.execute_input":"2024-12-05T12:22:06.323713Z","iopub.status.idle":"2024-12-05T12:23:20.408803Z","shell.execute_reply.started":"2024-12-05T12:22:06.323657Z","shell.execute_reply":"2024-12-05T12:23:20.407620Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {0: 10923, 1: 260}\nRecall score (original data): 0.6410\nF1 score (original data): 0.7042\nTPR (minority class) before augmentation: 0.6410\nTPR (majority class) before augmentation: 0.9957\nNumber of synthetic samples generated: 2236\nClass distribution after augmentation: {0: 10923, 1: 2496}\nRecall score (generated data): 0.9103\nF1 score (generated data): 0.8161\nTPR (minority class) after augmentation: 0.9103\nTPR (majority class) after augmentation: 0.9924\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n           0       0.99      1.00      0.99      3277\n           1       0.78      0.64      0.70        78\n\n    accuracy                           0.99      3355\n   macro avg       0.89      0.82      0.85      3355\nweighted avg       0.99      0.99      0.99      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00      3277\n           1       0.74      0.91      0.82        78\n\n    accuracy                           0.99      3355\n   macro avg       0.87      0.95      0.91      3355\nweighted avg       0.99      0.99      0.99      3355\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# ##Equalized Odds","metadata":{}},{"cell_type":"markdown","source":"# Mammography Dataset","metadata":{}},{"cell_type":"markdown","source":"# RandomForest","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\n\n# Define function to calculate TPR and FPR\ndef calculate_tpr_fpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate TPR and FPR for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR and FPR\n\n    Returns:\n    - TPR and FPR values\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[-1, 1])  # Majority is -1, Minority is 1\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    fp = cm[:, index].sum() - tp  # False Positives\n    tn = cm.sum() - (tp + fn + fp)  # True Negatives\n    \n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n    return tpr, fpr\n\n# Calculate TPR and FPR before augmentation\ntpr_orig_minority, fpr_orig_minority = calculate_tpr_fpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority, fpr_orig_majority = calculate_tpr_fpr(y_test, y_pred_orig, target_class=-1)\nprint(f\"Before augmentation: TPR (minority) = {tpr_orig_minority:.4f}, FPR (minority) = {fpr_orig_minority:.4f}\")\nprint(f\"Before augmentation: TPR (majority) = {tpr_orig_majority:.4f}, FPR (majority) = {fpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\n\n# Calculate TPR and FPR after augmentation\ntpr_bal_minority, fpr_bal_minority = calculate_tpr_fpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority, fpr_bal_majority = calculate_tpr_fpr(y_test, y_pred_bal, target_class=-1)\nprint(f\"After augmentation: TPR (minority) = {tpr_bal_minority:.4f}, FPR (minority) = {fpr_bal_minority:.4f}\")\nprint(f\"After augmentation: TPR (majority) = {tpr_bal_majority:.4f}, FPR (majority) = {fpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n\n# Evaluate Equalized Odds\ndef evaluate_equalized_odds(tpr1, fpr1, tpr2, fpr2, tolerance=0.05):\n    \"\"\"\n    Evaluate if Equalized Odds condition is met.\n\n    Parameters:\n    - tpr1, fpr1: TPR and FPR for group 1 (minority)\n    - tpr2, fpr2: TPR and FPR for group 2 (majority)\n    - tolerance: Allowed difference between groups\n\n    Returns:\n    - Boolean indicating if Equalized Odds is satisfied\n    \"\"\"\n    tpr_diff = abs(tpr1 - tpr2)\n    fpr_diff = abs(fpr1 - fpr2)\n    return tpr_diff <= tolerance and fpr_diff <= tolerance\n\nequalized_odds_before = evaluate_equalized_odds(tpr_orig_minority, fpr_orig_minority, tpr_orig_majority, fpr_orig_majority)\nequalized_odds_after = evaluate_equalized_odds(tpr_bal_minority, fpr_bal_minority, tpr_bal_majority, fpr_bal_majority)\n\nprint(f\"Equalized Odds satisfied before augmentation: {equalized_odds_before}\")\nprint(f\"Equalized Odds satisfied after augmentation: {equalized_odds_after}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:35:39.135571Z","iopub.execute_input":"2024-12-05T12:35:39.136035Z","iopub.status.idle":"2024-12-05T12:36:54.754106Z","shell.execute_reply.started":"2024-12-05T12:35:39.135992Z","shell.execute_reply":"2024-12-05T12:36:54.752476Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nBefore augmentation: TPR (minority) = 0.5000, FPR (minority) = 0.0024\nBefore augmentation: TPR (majority) = 0.9976, FPR (majority) = 0.5000\nAfter augmentation: TPR (minority) = 0.9103, FPR (minority) = 0.0095\nAfter augmentation: TPR (majority) = 0.9905, FPR (majority) = 0.0897\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n          -1       0.99      1.00      0.99      3277\n           1       0.83      0.50      0.62        78\n\n    accuracy                           0.99      3355\n   macro avg       0.91      0.75      0.81      3355\nweighted avg       0.98      0.99      0.98      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n          -1       1.00      0.99      0.99      3277\n           1       0.70      0.91      0.79        78\n\n    accuracy                           0.99      3355\n   macro avg       0.85      0.95      0.89      3355\nweighted avg       0.99      0.99      0.99      3355\n\nEqualized Odds satisfied before augmentation: False\nEqualized Odds satisfied after augmentation: False\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# ##Demographic Parity","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\n\n# Define function to calculate TPR and FPR\ndef calculate_tpr_fpr(y_true, y_pred, target_class):\n    \"\"\"\n    Calculate TPR and FPR for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - target_class: The class for which to calculate TPR and FPR\n\n    Returns:\n    - TPR and FPR values\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=[-1, 1])  # Majority is -1, Minority is 1\n    index = 1 if target_class == 1 else 0  # If target_class is 1 (minority), index is 1\n    tp = cm[index, index]  # True Positives\n    fn = cm[index, :].sum() - tp  # False Negatives\n    fp = cm[:, index].sum() - tp  # False Positives\n    tn = cm.sum() - (tp + fn + fp)  # True Negatives\n    \n    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n    return tpr, fpr\n\n# Calculate TPR and FPR before augmentation\ntpr_orig_minority, fpr_orig_minority = calculate_tpr_fpr(y_test, y_pred_orig, target_class=1)\ntpr_orig_majority, fpr_orig_majority = calculate_tpr_fpr(y_test, y_pred_orig, target_class=-1)\nprint(f\"Before augmentation: TPR (minority) = {tpr_orig_minority:.4f}, FPR (minority) = {fpr_orig_minority:.4f}\")\nprint(f\"Before augmentation: TPR (majority) = {tpr_orig_majority:.4f}, FPR (majority) = {fpr_orig_majority:.4f}\")\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\n\n# Calculate TPR and FPR after augmentation\ntpr_bal_minority, fpr_bal_minority = calculate_tpr_fpr(y_test, y_pred_bal, target_class=1)\ntpr_bal_majority, fpr_bal_majority = calculate_tpr_fpr(y_test, y_pred_bal, target_class=-1)\nprint(f\"After augmentation: TPR (minority) = {tpr_bal_minority:.4f}, FPR (minority) = {fpr_bal_minority:.4f}\")\nprint(f\"After augmentation: TPR (majority) = {tpr_bal_majority:.4f}, FPR (majority) = {fpr_bal_majority:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n\n# Demographic Parity Evaluation\ndef calculate_demographic_parity(y_true, y_pred):\n    \"\"\"\n    Calculate the demographic parity (positive prediction rate) for each class.\n    \n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    \n    Returns:\n    - Demographic parity gap\n    \"\"\"\n    # Positive prediction rate for majority and minority classes\n    pos_rate_majority = np.mean(y_pred[y_true == -1] == 1)\n    pos_rate_minority = np.mean(y_pred[y_true == 1] == 1)\n    \n    print(f\"Proportion of positive predictions (majority class): {pos_rate_majority:.4f}\")\n    print(f\"Proportion of positive predictions (minority class): {pos_rate_minority:.4f}\")\n    \n    # Demographic Parity gap\n    demographic_parity_gap = abs(pos_rate_majority - pos_rate_minority)\n    print(f\"Demographic Parity Gap: {demographic_parity_gap:.4f}\")\n    \n    return demographic_parity_gap\n\n# Evaluate Demographic Parity before and after augmentation\ndemographic_parity_gap_before = calculate_demographic_parity(y_test, y_pred_orig)\ndemographic_parity_gap_after = calculate_demographic_parity(y_test, y_pred_bal)\n\n# Check if Demographic Parity is satisfied (gap <= tolerance)\ntolerance = 0.05\ndemographic_parity_satisfied_before = demographic_parity_gap_before <= tolerance\ndemographic_parity_satisfied_after = demographic_parity_gap_after <= tolerance\n\nprint(f\"Demographic Parity satisfied before augmentation: {demographic_parity_satisfied_before}\")\nprint(f\"Demographic Parity satisfied after augmentation: {demographic_parity_satisfied_after}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T13:01:51.723639Z","iopub.execute_input":"2024-12-05T13:01:51.724090Z","iopub.status.idle":"2024-12-05T13:03:11.563063Z","shell.execute_reply.started":"2024-12-05T13:01:51.724051Z","shell.execute_reply":"2024-12-05T13:03:11.561539Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nBefore augmentation: TPR (minority) = 0.5000, FPR (minority) = 0.0024\nBefore augmentation: TPR (majority) = 0.9976, FPR (majority) = 0.5000\nAfter augmentation: TPR (minority) = 0.9103, FPR (minority) = 0.0095\nAfter augmentation: TPR (majority) = 0.9905, FPR (majority) = 0.0897\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n          -1       0.99      1.00      0.99      3277\n           1       0.83      0.50      0.62        78\n\n    accuracy                           0.99      3355\n   macro avg       0.91      0.75      0.81      3355\nweighted avg       0.98      0.99      0.98      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n          -1       1.00      0.99      0.99      3277\n           1       0.70      0.91      0.79        78\n\n    accuracy                           0.99      3355\n   macro avg       0.85      0.95      0.89      3355\nweighted avg       0.99      0.99      0.99      3355\n\nProportion of positive predictions (majority class): 0.0024\nProportion of positive predictions (minority class): 0.5000\nDemographic Parity Gap: 0.4976\nProportion of positive predictions (majority class): 0.0095\nProportion of positive predictions (minority class): 0.9103\nDemographic Parity Gap: 0.9008\nDemographic Parity satisfied before augmentation: False\nDemographic Parity satisfied after augmentation: False\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\n\n# Function to calculate positive prediction proportions (Statistical Parity)\ndef calculate_statistical_parity(y_true, y_pred, group_label):\n    \"\"\"\n    Calculate the proportion of positive predictions for a specific group.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - group_label: The class label (majority or minority group) to calculate for\n\n    Returns:\n    - Proportion of positive predictions for the specified group\n    \"\"\"\n    positive_predictions = (y_pred == 1)  # Positive predictions are labeled as 1\n    group_mask = (y_true == group_label)\n    return np.mean(positive_predictions[group_mask])\n\n# Calculate Statistical Parity before augmentation\nprop_pos_majority_before = calculate_statistical_parity(y_test, y_pred_orig, group_label=-1)\nprop_pos_minority_before = calculate_statistical_parity(y_test, y_pred_orig, group_label=1)\n\nstatistical_parity_gap_before = abs(prop_pos_majority_before - prop_pos_minority_before)\n\nprint(f\"Proportion of positive predictions (majority class) before augmentation: {prop_pos_majority_before:.4f}\")\nprint(f\"Proportion of positive predictions (minority class) before augmentation: {prop_pos_minority_before:.4f}\")\nprint(f\"Statistical Parity Gap before augmentation: {statistical_parity_gap_before:.4f}\")\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\n\n# Calculate Statistical Parity after augmentation\nprop_pos_majority_after = calculate_statistical_parity(y_test, y_pred_bal, group_label=-1)\nprop_pos_minority_after = calculate_statistical_parity(y_test, y_pred_bal, group_label=1)\n\nstatistical_parity_gap_after = abs(prop_pos_majority_after - prop_pos_minority_after)\n\nprint(f\"Proportion of positive predictions (majority class) after augmentation: {prop_pos_majority_after:.4f}\")\nprint(f\"Proportion of positive predictions (minority class) after augmentation: {prop_pos_minority_after:.4f}\")\nprint(f\"Statistical Parity Gap after augmentation: {statistical_parity_gap_after:.4f}\")\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n\n# Evaluate Statistical Parity\ndef evaluate_statistical_parity(gap, tolerance=0.05):\n    \"\"\"\n    Evaluate if Statistical Parity condition is met.\n\n    Parameters:\n    - gap: The difference in positive prediction proportions between the majority and minority classes\n    - tolerance: The acceptable difference for statistical parity\n\n    Returns:\n    - Boolean indicating if Statistical Parity is satisfied\n    \"\"\"\n    return gap <= tolerance\n\nstatistical_parity_before = evaluate_statistical_parity(statistical_parity_gap_before)\nstatistical_parity_after = evaluate_statistical_parity(statistical_parity_gap_after)\n\nprint(f\"Statistical Parity satisfied before augmentation: {statistical_parity_before}\")\nprint(f\"Statistical Parity satisfied after augmentation: {statistical_parity_after}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T13:14:56.661418Z","iopub.execute_input":"2024-12-05T13:14:56.661863Z","iopub.status.idle":"2024-12-05T13:16:15.837400Z","shell.execute_reply.started":"2024-12-05T13:14:56.661813Z","shell.execute_reply":"2024-12-05T13:16:15.836055Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nProportion of positive predictions (majority class) before augmentation: 0.0024\nProportion of positive predictions (minority class) before augmentation: 0.5000\nStatistical Parity Gap before augmentation: 0.4976\nProportion of positive predictions (majority class) after augmentation: 0.0095\nProportion of positive predictions (minority class) after augmentation: 0.9103\nStatistical Parity Gap after augmentation: 0.9008\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n          -1       0.99      1.00      0.99      3277\n           1       0.83      0.50      0.62        78\n\n    accuracy                           0.99      3355\n   macro avg       0.91      0.75      0.81      3355\nweighted avg       0.98      0.99      0.98      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n          -1       1.00      0.99      0.99      3277\n           1       0.70      0.91      0.79        78\n\n    accuracy                           0.99      3355\n   macro avg       0.85      0.95      0.89      3355\nweighted avg       0.99      0.99      0.99      3355\n\nStatistical Parity satisfied before augmentation: False\nStatistical Parity satisfied after augmentation: False\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\n\n# Classification reports\nprint(\"Classification Report (original data):\\n\", classification_report(y_test, y_pred_orig))\n\n# Augment data using Forest Diffusion\nint_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=int_indexes,\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\n\n# Classification reports\nprint(\"Classification Report (generated data):\\n\", classification_report(y_test, y_pred_bal))\n\n# --- Causal Inference Tests ---\n# Causal Inference: Estimate Propensity Scores\n\n# Estimate propensity scores using logistic regression (for treatment effect estimation)\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train_orig, y_train_orig)\n\n# Predict the propensity scores (probability of being treated)\npropensity_scores = log_reg.predict_proba(X_train_orig)[:, 1]\n\n# Nearest Neighbors Matching (for estimating treatment effects)\nneighbors = NearestNeighbors(n_neighbors=1)\nneighbors.fit(propensity_scores.reshape(-1, 1))\n\n# Match treatment groups with the closest neighbors (propensity score matching)\nmatched_indices = neighbors.kneighbors(propensity_scores.reshape(-1, 1), return_distance=False).flatten()\n\n# Treatment effect estimation (difference in outcomes between treated and matched control)\ntreated_outcome = clf_bal.predict(X_train_bal)[matched_indices]  # Outcome after treatment (augmentation)\ncontrol_outcome = clf_orig.predict(X_train_orig)[matched_indices]  # Outcome before treatment (original)\n\n# Calculate treatment effect\ntreatment_effect = np.mean(treated_outcome != control_outcome)  # Difference in outcomes\nprint(f\"Estimated Treatment Effect (difference in prediction outcomes): {treatment_effect:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T13:49:31.476570Z","iopub.execute_input":"2024-12-05T13:49:31.477067Z","iopub.status.idle":"2024-12-05T13:50:46.907778Z","shell.execute_reply.started":"2024-12-05T13:49:31.477023Z","shell.execute_reply":"2024-12-05T13:50:46.906599Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nClassification Report (original data):\n               precision    recall  f1-score   support\n\n          -1       0.99      1.00      0.99      3277\n           1       0.83      0.50      0.62        78\n\n    accuracy                           0.99      3355\n   macro avg       0.91      0.75      0.81      3355\nweighted avg       0.98      0.99      0.98      3355\n\nClassification Report (generated data):\n               precision    recall  f1-score   support\n\n          -1       1.00      0.99      0.99      3277\n           1       0.70      0.91      0.79        78\n\n    accuracy                           0.99      3355\n   macro avg       0.85      0.95      0.89      3355\nweighted avg       0.99      0.99      0.99      3355\n\nEstimated Treatment Effect (difference in prediction outcomes): 0.1483\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, f1_score\nfrom ForestDiffusion import ForestDiffusionModel\nfrom imblearn.datasets import fetch_datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import NearestNeighbors\n\n# Load data\ndata = fetch_datasets()['mammography']\n\n# Extract features and target\nX = data['data']\ny = data['target']\n\n# Check initial class distribution\nunique, counts = np.unique(y, return_counts=True)\nclass_dist_before = dict(zip(unique, counts))\nprint(f\"Class distribution before augmentation: {class_dist_before}\")\n\n# Separate minority class\nX_minority = X[y == 1]\n\n# Split data into training and testing sets\nX_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Train model on original data\nclf_orig = RandomForestClassifier(random_state=42)\nclf_orig.fit(X_train_orig, y_train_orig)\n\n# Predict on test set (original data)\ny_pred_orig = clf_orig.predict(X_test)\n\n# Augment data using Forest Diffusion\n#int_indexes = [i for i in range(X.shape[1]) if np.issubdtype(X[:, i].dtype, np.integer)]\nforest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n                                    bin_indexes=[], cat_indexes=[], int_indexes=[],\n                                    diffusion_type='flow', n_jobs=-1)\n\nX_minority_fake = forest_model.generate(batch_size=len(X) // 5)\n\n# Combine original and synthetic data\nX_balanced = np.concatenate((X, X_minority_fake), axis=0)\ny_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0], dtype=int)), axis=0)\n\n# Split augmented data into training and testing sets\nX_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n\n# Train model on augmented data\nclf_bal = RandomForestClassifier(random_state=42)\nclf_bal.fit(X_train_bal, y_train_bal)\n\n# Predict on test set (augmented data)\ny_pred_bal = clf_bal.predict(X_test)\n\n# --- Causal Inference: Estimating Treatment Effect by Group ---\n# For minority class (y == 1) and majority class (y == -1), compare treatment effects\ndef calculate_treatment_effect_by_group(y_true, y_pred_orig, y_pred_bal, target_class):\n    \"\"\"\n    Calculate treatment effect (difference in predictions) for the specified class.\n\n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred_orig: Predictions from the model trained on original data\n    - y_pred_bal: Predictions from the model trained on augmented data\n    - target_class: The class for which to calculate the treatment effect (1 for minority, -1 for majority)\n\n    Returns:\n    - Treatment effect for the target class\n    \"\"\"\n    # Get the indices for the target class\n    class_indices = np.where(y_true == target_class)[0]\n\n    # Calculate the treatment effect: difference in predictions for the target class\n    treatment_effect = np.mean(y_pred_bal[class_indices] != y_pred_orig[class_indices])  # Difference in outcomes\n\n    return treatment_effect\n\n# Treatment effect for minority class (1)\ntreatment_effect_minority = calculate_treatment_effect_by_group(y_test, y_pred_orig, y_pred_bal, target_class=1)\n# Treatment effect for majority class (-1)\ntreatment_effect_majority = calculate_treatment_effect_by_group(y_test, y_pred_orig, y_pred_bal, target_class=-1)\n\nprint(f\"Estimated Treatment Effect for Minority Class: {treatment_effect_minority:.4f}\")\nprint(f\"Estimated Treatment Effect for Majority Class: {treatment_effect_majority:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T14:04:48.309196Z","iopub.execute_input":"2024-12-05T14:04:48.309605Z","iopub.status.idle":"2024-12-05T14:06:07.386786Z","shell.execute_reply.started":"2024-12-05T14:04:48.309560Z","shell.execute_reply":"2024-12-05T14:06:07.385279Z"}},"outputs":[{"name":"stdout","text":"Class distribution before augmentation: {-1: 10923, 1: 260}\nEstimated Treatment Effect for Minority Class: 0.4103\nEstimated Treatment Effect for Majority Class: 0.0070\n","output_type":"stream"}],"execution_count":13}]}