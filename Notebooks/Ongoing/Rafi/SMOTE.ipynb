{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('pen_digits.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, 1:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = 2154\n",
    "\n",
    "# Ensure the target size is greater than the current minority class size\n",
    "if desired_minority_class_size > minority_class_count:\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "\n",
    "    # Apply SMOTE with the desired number of synthetic samples\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "\n",
    "# Print class distribution after SMOTE\n",
    "print(\"\\nClass distribution after augmentation (on training data):\")\n",
    "print(Counter(y_train_res))\n",
    "\n",
    "# Print the number of samples generated\n",
    "num_generated_samples = len(X_train_res) - len(X_train_orig)\n",
    "print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_res, y_train_res)\n",
    "y_pred_test_res = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation (on test data):\")\n",
    "print(classification_report(y_test, y_pred_test_res))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_test_res, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_test_res, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_test_res, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from openTSNE import TSNE\n",
    "\n",
    "\n",
    "# Apply openTSNE for t-SNE visualization\n",
    "tsne = TSNE(\n",
    "    n_components=2, perplexity=30, metric=\"euclidean\",\n",
    "    n_jobs=-1  # Use all available CPU cores for speedup\n",
    ")\n",
    "\n",
    "# Fit openTSNE on the PCA-reduced minority class data\n",
    "X_tsne_minority = tsne.fit(X_minority)\n",
    "X_tsne_res_minority = tsne.fit(X_train_res[y_train_res == 1])\n",
    "\n",
    "# Create t-SNE Plot for minority class before and after SMOTE\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "\n",
    "# Plot for original minority class data\n",
    "ax.scatter(X_tsne_minority[:, 0], X_tsne_minority[:, 1], c='blue', label='Minority Class (Original)', alpha=0.6)\n",
    "# Plot for SMOTE-generated synthetic minority class data\n",
    "ax.scatter(X_tsne_res_minority[:, 0], X_tsne_res_minority[:, 1], c='orange', label='Minority Class (SMOTE)', alpha=0.6)\n",
    "\n",
    "# Add labels and legends\n",
    "ax.set_title('t-SNE of Minority Class (Before and After SMOTE)')\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mammography Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('mammography_dataset.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, 1:-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_train_res = X_train_orig[combined_indices]\n",
    "    y_train_res = y_train_orig[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_train_res))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_train_res))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_train_res) - len(X_train_orig)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_train_res, y_train_res = X_train_orig, y_train_orig\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_res, y_train_res)\n",
    "y_pred_test_res = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation (on test data):\")\n",
    "print(classification_report(y_test, y_pred_test_res))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_test_res, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_test_res, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_test_res, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mammography Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Recall score (original data): 0.5000\n",
      "F1 score (original data): 0.6240\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 10923, 1: 390})\n",
      "\n",
      "Number of samples generated by SMOTE: 130\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      0.99      3277\n",
      "           1       0.82      0.71      0.76        78\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.91      0.85      0.88      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Precision after augmentation: 0.8209\n",
      "Recall after augmentation: 0.7051\n",
      "F1 Score after augmentation: 0.7586\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('mammography_dataset.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Recall score (original data): 0.5000\n",
      "F1 score (original data): 0.6240\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 10923, 1: 520})\n",
      "\n",
      "Number of samples generated by SMOTE: 260\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00      3277\n",
      "           1       0.80      0.82      0.81        78\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.90      0.91      0.90      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Precision after augmentation: 0.8000\n",
      "Recall after augmentation: 0.8205\n",
      "F1 Score after augmentation: 0.8101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('mammography_dataset.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Recall score (original data): 0.5000\n",
      "F1 score (original data): 0.6240\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 10923, 1: 780})\n",
      "\n",
      "Number of samples generated by SMOTE: 520\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00      3277\n",
      "           1       0.81      0.85      0.83        78\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.91      0.92      0.91      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Precision after augmentation: 0.8148\n",
      "Recall after augmentation: 0.8462\n",
      "F1 Score after augmentation: 0.8302\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('mammography_dataset.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+((len(X_minority))*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTRU_2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 16258, 1: 1639}\n",
      "Recall score (original data): 0.8415\n",
      "F1 score (original data): 0.8743\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 16258, 1: 2458})\n",
      "\n",
      "Number of samples generated by SMOTE: 819\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4878\n",
      "           1       0.91      0.92      0.92       492\n",
      "\n",
      "    accuracy                           0.98      5370\n",
      "   macro avg       0.95      0.96      0.95      5370\n",
      "weighted avg       0.98      0.98      0.98      5370\n",
      "\n",
      "Precision after augmentation: 0.9100\n",
      "Recall after augmentation: 0.9248\n",
      "F1 Score after augmentation: 0.9173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('HTRU_2.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 16258, 1: 1639}\n",
      "Recall score (original data): 0.8415\n",
      "F1 score (original data): 0.8743\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 16258, 1: 3278})\n",
      "\n",
      "Number of samples generated by SMOTE: 1639\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4878\n",
      "           1       0.89      0.94      0.92       492\n",
      "\n",
      "    accuracy                           0.98      5370\n",
      "   macro avg       0.94      0.96      0.95      5370\n",
      "weighted avg       0.98      0.98      0.98      5370\n",
      "\n",
      "Precision after augmentation: 0.8936\n",
      "Recall after augmentation: 0.9390\n",
      "F1 Score after augmentation: 0.9158\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('HTRU_2.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+len(X_minority)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 16258, 1: 1639}\n",
      "Recall score (original data): 0.8415\n",
      "F1 score (original data): 0.8743\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 16258, 1: 4917})\n",
      "\n",
      "Number of samples generated by SMOTE: 3278\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4878\n",
      "           1       0.89      0.95      0.92       492\n",
      "\n",
      "    accuracy                           0.98      5370\n",
      "   macro avg       0.94      0.97      0.96      5370\n",
      "weighted avg       0.99      0.98      0.98      5370\n",
      "\n",
      "Precision after augmentation: 0.8866\n",
      "Recall after augmentation: 0.9533\n",
      "F1 Score after augmentation: 0.9187\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('HTRU_2.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 284315, 1: 492}\n",
      "Recall score (original data): 0.7568\n",
      "F1 score (original data): 0.8453\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 284315, 1: 738})\n",
      "\n",
      "Number of samples generated by SMOTE: 246\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.96      0.86      0.91       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.98      0.93      0.95     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Precision after augmentation: 0.9552\n",
      "Recall after augmentation: 0.8649\n",
      "F1 Score after augmentation: 0.9078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "#data.head()\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 284315, 1: 492}\n",
      "Recall score (original data): 0.7568\n",
      "F1 score (original data): 0.8453\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 284315, 1: 984})\n",
      "\n",
      "Number of samples generated by SMOTE: 492\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.93      0.88      0.90       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.96      0.94      0.95     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Precision after augmentation: 0.9286\n",
      "Recall after augmentation: 0.8784\n",
      "F1 Score after augmentation: 0.9028\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 284315, 1: 492}\n",
      "Recall score (original data): 0.7568\n",
      "F1 score (original data): 0.8453\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 284315, 1: 1476})\n",
      "\n",
      "Number of samples generated by SMOTE: 984\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85295\n",
      "           1       0.91      0.93      0.92       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.96      0.96      0.96     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Precision after augmentation: 0.9133\n",
      "Recall after augmentation: 0.9257\n",
      "F1 Score after augmentation: 0.9195\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Diabetes Data (Reduced minority: 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after balancing:\n",
      "Outcome\n",
      "0    500\n",
      "1    200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of X: (700, 8)\n",
      "Shape of y: (700,)\n",
      "Class distribution before augmentation: {0: 500, 1: 200}\n",
      "Recall score (original data): 0.5333\n",
      "F1 score (original data): 0.5872\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 500, 1: 300})\n",
      "\n",
      "Number of samples generated by SMOTE: 100\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89       150\n",
      "           1       0.71      0.80      0.75        60\n",
      "\n",
      "    accuracy                           0.85       210\n",
      "   macro avg       0.81      0.83      0.82       210\n",
      "weighted avg       0.86      0.85      0.85       210\n",
      "\n",
      "Precision after augmentation: 0.7059\n",
      "Recall after augmentation: 0.8000\n",
      "F1 Score after augmentation: 0.7500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data['Outcome'] == 1]\n",
    "majority_class = data[data['Outcome'] == 0]\n",
    "\n",
    "# Randomly sample 68 rows to drop from the minority class\n",
    "minority_class_reduced = minority_class.sample(n=len(minority_class) - 68, random_state=42)\n",
    "\n",
    "# Combine the reduced minority class with the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class_reduced])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X (features) and y (target)\n",
    "X = balanced_data.drop(columns=['Outcome'])  # Drop the target column to get features\n",
    "y = balanced_data['Outcome']  # Target variable\n",
    "\n",
    "# Print results\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(balanced_data['Outcome'].value_counts())\n",
    "print(\"\\nShape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after balancing:\n",
      "Outcome\n",
      "0    500\n",
      "1    200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of X: (700, 8)\n",
      "Shape of y: (700,)\n",
      "Class distribution before augmentation: {0: 500, 1: 200}\n",
      "Recall score (original data): 0.5333\n",
      "F1 score (original data): 0.5872\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 500, 1: 400})\n",
      "\n",
      "Number of samples generated by SMOTE: 200\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88       150\n",
      "           1       0.68      0.87      0.76        60\n",
      "\n",
      "    accuracy                           0.84       210\n",
      "   macro avg       0.81      0.85      0.82       210\n",
      "weighted avg       0.86      0.84      0.85       210\n",
      "\n",
      "Precision after augmentation: 0.6753\n",
      "Recall after augmentation: 0.8667\n",
      "F1 Score after augmentation: 0.7591\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data['Outcome'] == 1]\n",
    "majority_class = data[data['Outcome'] == 0]\n",
    "\n",
    "# Randomly sample 68 rows to drop from the minority class\n",
    "minority_class_reduced = minority_class.sample(n=len(minority_class) - 68, random_state=42)\n",
    "\n",
    "# Combine the reduced minority class with the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class_reduced])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X (features) and y (target)\n",
    "X = balanced_data.drop(columns=['Outcome'])  # Drop the target column to get features\n",
    "y = balanced_data['Outcome']  # Target variable\n",
    "\n",
    "# Print results\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(balanced_data['Outcome'].value_counts())\n",
    "print(\"\\nShape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after balancing:\n",
      "Outcome\n",
      "0    500\n",
      "1    200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of X: (700, 8)\n",
      "Shape of y: (700,)\n",
      "Class distribution before augmentation: {0: 500, 1: 200}\n",
      "Recall score (original data): 0.5333\n",
      "F1 score (original data): 0.5872\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 600, 0: 500})\n",
      "\n",
      "Number of samples generated by SMOTE: 400\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.78      0.87       150\n",
      "           1       0.63      0.95      0.76        60\n",
      "\n",
      "    accuracy                           0.83       210\n",
      "   macro avg       0.80      0.86      0.81       210\n",
      "weighted avg       0.88      0.83      0.84       210\n",
      "\n",
      "Precision after augmentation: 0.6333\n",
      "Recall after augmentation: 0.9500\n",
      "F1 Score after augmentation: 0.7600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data['Outcome'] == 1]\n",
    "majority_class = data[data['Outcome'] == 0]\n",
    "\n",
    "# Randomly sample 68 rows to drop from the minority class\n",
    "minority_class_reduced = minority_class.sample(n=len(minority_class) - 68, random_state=42)\n",
    "\n",
    "# Combine the reduced minority class with the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class_reduced])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X (features) and y (target)\n",
    "X = balanced_data.drop(columns=['Outcome'])  # Drop the target column to get features\n",
    "y = balanced_data['Outcome']  # Target variable\n",
    "\n",
    "# Print results\n",
    "print(\"Class distribution after balancing:\")\n",
    "print(balanced_data['Outcome'].value_counts())\n",
    "print(\"\\nShape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 896, 1: 41}\n",
      "Recall score (original data): 0.3333\n",
      "F1 score (original data): 0.4706\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 896, 1: 61})\n",
      "\n",
      "Number of samples generated by SMOTE: 20\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.99      0.99       270\n",
      "           1       0.75      0.50      0.60        12\n",
      "\n",
      "    accuracy                           0.97       282\n",
      "   macro avg       0.86      0.75      0.79       282\n",
      "weighted avg       0.97      0.97      0.97       282\n",
      "\n",
      "Precision after augmentation: 0.7500\n",
      "Recall after augmentation: 0.5000\n",
      "F1 Score after augmentation: 0.6000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('oil.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 896, 1: 41}\n",
      "Recall score (original data): 0.3333\n",
      "F1 score (original data): 0.4706\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 896, 1: 82})\n",
      "\n",
      "Number of samples generated by SMOTE: 41\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.99      0.98       270\n",
      "           1       0.64      0.58      0.61        12\n",
      "\n",
      "    accuracy                           0.97       282\n",
      "   macro avg       0.81      0.78      0.80       282\n",
      "weighted avg       0.97      0.97      0.97       282\n",
      "\n",
      "Precision after augmentation: 0.6364\n",
      "Recall after augmentation: 0.5833\n",
      "F1 Score after augmentation: 0.6087\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('oil.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 896, 1: 41}\n",
      "Recall score (original data): 0.3333\n",
      "F1 score (original data): 0.4706\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({-1: 896, 1: 123})\n",
      "\n",
      "Number of samples generated by SMOTE: 82\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      0.99      0.99       270\n",
      "           1       0.73      0.67      0.70        12\n",
      "\n",
      "    accuracy                           0.98       282\n",
      "   macro avg       0.86      0.83      0.84       282\n",
      "weighted avg       0.97      0.98      0.97       282\n",
      "\n",
      "Precision after augmentation: 0.7273\n",
      "Recall after augmentation: 0.6667\n",
      "F1 Score after augmentation: 0.6957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('oil.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced Spambase (Reduced minority: 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1000}\n",
      "Recall score (original data): 0.8600\n",
      "F1 score (original data): 0.8897\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 2788, 1: 1500})\n",
      "\n",
      "Number of samples generated by SMOTE: 500\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       837\n",
      "           1       0.91      0.92      0.92       300\n",
      "\n",
      "    accuracy                           0.96      1137\n",
      "   macro avg       0.94      0.94      0.94      1137\n",
      "weighted avg       0.96      0.96      0.96      1137\n",
      "\n",
      "Precision after augmentation: 0.9109\n",
      "Recall after augmentation: 0.9200\n",
      "F1 Score after augmentation: 0.9154\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data.iloc[:, -1] == 1]  # Rows where the last column (target) is 1\n",
    "majority_class = data[data.iloc[:, -1] == 0]  # Rows where the last column (target) is 0\n",
    "\n",
    "# Randomly select 813 samples to drop from the minority class\n",
    "drop_indices = np.random.choice(minority_class.index, 813, replace=False)\n",
    "minority_class = minority_class.drop(drop_indices)\n",
    "\n",
    "# Combine the modified minority class and the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class])\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X and y\n",
    "X = balanced_data.iloc[:, :-1]  \n",
    "y = balanced_data.iloc[:, -1]   \n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1000}\n",
      "Recall score (original data): 0.9067\n",
      "F1 score (original data): 0.9189\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 2788, 1: 2000})\n",
      "\n",
      "Number of samples generated by SMOTE: 1000\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       837\n",
      "           1       0.93      0.96      0.95       300\n",
      "\n",
      "    accuracy                           0.97      1137\n",
      "   macro avg       0.96      0.97      0.96      1137\n",
      "weighted avg       0.97      0.97      0.97      1137\n",
      "\n",
      "Precision after augmentation: 0.9323\n",
      "Recall after augmentation: 0.9633\n",
      "F1 Score after augmentation: 0.9475\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data.iloc[:, -1] == 1]  # Rows where the last column (target) is 1\n",
    "majority_class = data[data.iloc[:, -1] == 0]  # Rows where the last column (target) is 0\n",
    "\n",
    "# Randomly select 813 samples to drop from the minority class\n",
    "drop_indices = np.random.choice(minority_class.index, 813, replace=False)\n",
    "minority_class = minority_class.drop(drop_indices)\n",
    "\n",
    "# Combine the modified minority class and the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class])\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X and y\n",
    "X = balanced_data.iloc[:, :-1]  \n",
    "y = balanced_data.iloc[:, -1]   \n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1000}\n",
      "Recall score (original data): 0.9267\n",
      "F1 score (original data): 0.9298\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 3000, 0: 2788})\n",
      "\n",
      "Number of samples generated by SMOTE: 2000\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       837\n",
      "           1       0.92      0.98      0.95       300\n",
      "\n",
      "    accuracy                           0.97      1137\n",
      "   macro avg       0.96      0.98      0.97      1137\n",
      "weighted avg       0.97      0.97      0.97      1137\n",
      "\n",
      "Precision after augmentation: 0.9245\n",
      "Recall after augmentation: 0.9800\n",
      "F1 Score after augmentation: 0.9515\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "# Separate the minority and majority classes\n",
    "minority_class = data[data.iloc[:, -1] == 1]  # Rows where the last column (target) is 1\n",
    "majority_class = data[data.iloc[:, -1] == 0]  # Rows where the last column (target) is 0\n",
    "\n",
    "# Randomly select 813 samples to drop from the minority class\n",
    "drop_indices = np.random.choice(minority_class.index, 813, replace=False)\n",
    "minority_class = minority_class.drop(drop_indices)\n",
    "\n",
    "# Combine the modified minority class and the majority class\n",
    "balanced_data = pd.concat([majority_class, minority_class])\n",
    "\n",
    "# Shuffle the dataset to ensure randomness\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into X and y\n",
    "X = balanced_data.iloc[:, :-1]  \n",
    "y = balanced_data.iloc[:, -1]   \n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spambase Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1813}\n",
      "Recall score (original data): 0.9283\n",
      "F1 score (original data): 0.9430\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 2788, 1: 2719})\n",
      "\n",
      "Number of samples generated by SMOTE: 906\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       837\n",
      "           1       0.95      0.97      0.96       544\n",
      "\n",
      "    accuracy                           0.97      1381\n",
      "   macro avg       0.97      0.97      0.97      1381\n",
      "weighted avg       0.97      0.97      0.97      1381\n",
      "\n",
      "Precision after augmentation: 0.9514\n",
      "Recall after augmentation: 0.9724\n",
      "F1 Score after augmentation: 0.9618\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1813}\n",
      "Recall score (original data): 0.9283\n",
      "F1 score (original data): 0.9430\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 3626, 0: 2788})\n",
      "\n",
      "Number of samples generated by SMOTE: 1813\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       837\n",
      "           1       0.94      0.98      0.96       544\n",
      "\n",
      "    accuracy                           0.97      1381\n",
      "   macro avg       0.96      0.97      0.97      1381\n",
      "weighted avg       0.97      0.97      0.97      1381\n",
      "\n",
      "Precision after augmentation: 0.9368\n",
      "Recall after augmentation: 0.9816\n",
      "F1 Score after augmentation: 0.9587\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 2788, 1: 1813}\n",
      "Recall score (original data): 0.9283\n",
      "F1 score (original data): 0.9430\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 5439, 0: 2788})\n",
      "\n",
      "Number of samples generated by SMOTE: 3626\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       837\n",
      "           1       0.93      0.99      0.96       544\n",
      "\n",
      "    accuracy                           0.97      1381\n",
      "   macro avg       0.96      0.97      0.96      1381\n",
      "weighted avg       0.97      0.97      0.97      1381\n",
      "\n",
      "Precision after augmentation: 0.9277\n",
      "Recall after augmentation: 0.9908\n",
      "F1 Score after augmentation: 0.9582\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('Spambase.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.5309\n",
      "F1 score (original data): 0.5972\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({0: 500, 1: 402})\n",
      "\n",
      "Number of samples generated by SMOTE: 134\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.82      0.86       150\n",
      "           1       0.72      0.84      0.77        81\n",
      "\n",
      "    accuracy                           0.83       231\n",
      "   macro avg       0.81      0.83      0.82       231\n",
      "weighted avg       0.84      0.83      0.83       231\n",
      "\n",
      "Precision after augmentation: 0.7158\n",
      "Recall after augmentation: 0.8395\n",
      "F1 Score after augmentation: 0.7727\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)//2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.5309\n",
      "F1 score (original data): 0.5972\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 536, 0: 500})\n",
      "\n",
      "Number of samples generated by SMOTE: 268\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86       150\n",
      "           1       0.70      0.93      0.80        81\n",
      "\n",
      "    accuracy                           0.84       231\n",
      "   macro avg       0.83      0.86      0.83       231\n",
      "weighted avg       0.86      0.84      0.84       231\n",
      "\n",
      "Precision after augmentation: 0.7009\n",
      "Recall after augmentation: 0.9259\n",
      "F1 Score after augmentation: 0.7979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority))\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200% of minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.5309\n",
      "F1 score (original data): 0.5972\n",
      "\n",
      "Class distribution after augmentation :\n",
      "Counter({1: 804, 0: 500})\n",
      "\n",
      "Number of samples generated by SMOTE: 536\n",
      "\n",
      "Classification report after augmentation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86       150\n",
      "           1       0.69      0.95      0.80        81\n",
      "\n",
      "    accuracy                           0.84       231\n",
      "   macro avg       0.83      0.86      0.83       231\n",
      "weighted avg       0.87      0.84      0.84       231\n",
      "\n",
      "Precision after augmentation: 0.6937\n",
      "Recall after augmentation: 0.9506\n",
      "F1 Score after augmentation: 0.8021\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "\n",
    "\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "#Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "\n",
    "majority_class_count = np.sum(y_train_orig == -1)\n",
    "minority_class_count = np.sum(y_train_orig == 1)\n",
    "\n",
    "# Set the target number of samples for the minority class\n",
    "desired_minority_class_size = len(X_minority)+(len(X_minority)*2)\n",
    "\n",
    "# Adjust sampling based on the desired size\n",
    "if desired_minority_class_size < minority_class_count:\n",
    "    # Reduce the size of the minority class\n",
    "    minority_indices = np.where(y_train_orig == 1)[0]\n",
    "    majority_indices = np.where(y_train_orig != 1)[0]\n",
    "\n",
    "    # Randomly select the desired number of samples\n",
    "    reduced_minority_indices = np.random.choice(\n",
    "        minority_indices, \n",
    "        size=desired_minority_class_size, \n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    # Combine reduced minority with majority class\n",
    "    combined_indices = np.concatenate([reduced_minority_indices, majority_indices])\n",
    "    X_balanced = X[combined_indices]\n",
    "    y_balanced = y[combined_indices]\n",
    "\n",
    "    print(\"\\nClass distribution after reduction (on training data):\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "elif desired_minority_class_size > minority_class_count:\n",
    "    # Use SMOTE to oversample if the desired size is greater\n",
    "    smote_strategy = {1: desired_minority_class_size}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "    # Print class distribution after SMOTE\n",
    "    print(\"\\nClass distribution after augmentation :\")\n",
    "    print(Counter(y_balanced))\n",
    "\n",
    "    # Print the number of samples generated\n",
    "    num_generated_samples = len(X_balanced) - len(X)\n",
    "    print(f\"\\nNumber of samples generated by SMOTE: {num_generated_samples}\")\n",
    "\n",
    "else:\n",
    "    # If the desired size is equal to the current size, use the original data\n",
    "    X_balanced, y_balanced= X, y\n",
    "    print(\"\\nNo augmentation needed. Class distribution remains unchanged.\")\n",
    "\n",
    "# Train a Random Forest Classifier after augmentation\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "clf_res = RandomForestClassifier(random_state=42)\n",
    "clf_res.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = clf_res.predict(X_test)\n",
    "\n",
    "# Print classification report, precision, recall, and F1 score after SMOTE (on test data)\n",
    "print(\"\\nClassification report after augmentation:\")\n",
    "print(classification_report(y_test, y_pred_bal))\n",
    "\n",
    "precision_after = precision_score(y_test, y_pred_bal, pos_label=1)\n",
    "recall_after = recall_score(y_test, y_pred_bal, pos_label=1)\n",
    "f1_after = f1_score(y_test, y_pred_bal, pos_label=1)\n",
    "\n",
    "print(f\"Precision after augmentation: {precision_after:.4f}\")\n",
    "print(f\"Recall after augmentation: {recall_after:.4f}\")\n",
    "print(f\"F1 Score after augmentation: {f1_after:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
