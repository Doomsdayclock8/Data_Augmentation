{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion' \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as ae\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from ForestDiffusion import ForestDiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Metrics for AutoDiff Autoencoder & ForestDIffusion for mammography dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8257cc2e8340a68c10b6e90fac3fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strings_set = ['mammography']\n",
    "Model = 'AutoDiff'\n",
    "metrics_list = []\n",
    "for dataset in strings_set:\n",
    "    print(f\"Result Metrics for AutoDiff Autoencoder & ForestDIffusion for {dataset} dataset\")\n",
    "    file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    # Read dataframe\n",
    "    # print(file_path)\n",
    "    real_df = pd.read_csv(file_path)\n",
    "    #real_df = real_df.drop('url', axis=1)\n",
    "    # # Step 2: Inspect the data and check for class imbalance\n",
    "    # # Assuming the last column is the label, and the rest are features\n",
    "    X = real_df.iloc[:, :-1].values  # Features\n",
    "    y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "    #  # Separate the minority class\n",
    "    # Find the minority class\n",
    "    \n",
    "    real_minortiy = real_df[y == 1]\n",
    "\n",
    "    threshold = 0.01 # Threshold for mixed-type variables\n",
    "    parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder hyper-parameters\n",
    "    device = 'cuda' #@param {'type':'string'}\n",
    "    n_epochs = 10 #@param {'type':'integer'}\n",
    "    eps = 1e-5 #@param {type:\"number\"}\n",
    "    weight_decay = 1e-6 #@param {'type':'number'}\n",
    "    maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "    lr = 2e-4 #@param {'type':'number'}\n",
    "    hidden_size = 250\n",
    "    num_layers = 3\n",
    "    batch_size = real_minortiy.shape[0] # Full batch\n",
    "\n",
    "    ds = ae.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "    latent_features = ds[1].detach()\n",
    "\n",
    "    from ForestDiffusion import ForestDiffusionModel\n",
    "\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    array = latent_features.detach().cpu().numpy()\n",
    "    forest_model = ForestDiffusionModel(array, label_y=None, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)\n",
    "    minority_fake = forest_model.generate(batch_size=len(real_minortiy)) # Adjust the batch size to create a balanced dataset\n",
    "    sample=torch.tensor(minority_fake, dtype=torch.float32)\n",
    "    sample.shape\n",
    "    gen_output = ds[0](sample, ds[2], ds[3])\n",
    "    gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "\n",
    "    output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Synthetic Data\\\\'\n",
    "    filename = f'{Model}+Forest_{dataset}_Synthetic.csv'\n",
    "    output_file = os.path.join(output_directory, filename)\n",
    "    gen_df.to_csv(output_file, index=False) \n",
    "\n",
    "\n",
    "    # Select a random sample of the generated data\n",
    "    selected_samples = gen_df.sample(n=min(100,gen_df.shape[0]), random_state=42)  # For reproducibility\n",
    "    # Syn _df will be the dataset after augmentation\n",
    "    syn_df = pd.concat([real_df, selected_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(260, 13)\n",
      "(11183, 7)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(array))\n",
    "print(array.shape)\n",
    "print(real_df.shape)\n",
    "print(type(minority_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Metrics for AutoDiff Autoencoder & ForestDIffusion for mammography dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#real_df = real_df.drop('url', axis=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# # Step 2: Inspect the data and check for class imbalance\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# # Assuming the last column is the label, and the rest are features\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X \u001b[38;5;241m=\u001b[39m real_df\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 17\u001b[0m X_minority\u001b[38;5;241m=\u001b[39mreal_df[\u001b[43my\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[0;32m     18\u001b[0m y \u001b[38;5;241m=\u001b[39m real_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     19\u001b[0m Y_minority\u001b[38;5;241m=\u001b[39mreal_df[y\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;66;03m# Labels (binary classification)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# strings_set = ['diabetes','oil','yeast_ml8_dataset','creditcard_sampled','HTRU','mammography']\n",
    "strings_set = ['mammography']\n",
    "Model = 'AutoDiff'\n",
    "metrics_list = []\n",
    "\n",
    "\n",
    "for dataset in strings_set:\n",
    "    print(f\"Result Metrics for AutoDiff Autoencoder & ForestDIffusion for {dataset} dataset\")\n",
    "    file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    # Read dataframe\n",
    "    # print(file_path)\n",
    "    real_df = pd.read_csv(file_path)\n",
    "    #real_df = real_df.drop('url', axis=1)\n",
    "    # # Step 2: Inspect the data and check for class imbalance\n",
    "    # # Assuming the last column is the label, and the rest are features\n",
    "    X = real_df.iloc[:, :-1].values\n",
    "    X_minority=real_df[y==1].iloc[:, :-1].values# Features\n",
    "    y = real_df.iloc[:, -1].values\n",
    "    Y_minority=real_df[y==1].iloc[:, :-1].values# Labels (binary classification)\n",
    "    #  # Separate the minority class\n",
    "    # Find the minority class\n",
    "    \n",
    "   \n",
    "\n",
    "    threshold = 0.01 # Threshold for mixed-type variables\n",
    "    parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder hyper-parameters\n",
    "    device = 'cuda' #@param {'type':'string'}\n",
    "    n_epochs = 10 #@param {'type':'integer'}\n",
    "    eps = 1e-5 #@param {type:\"number\"}\n",
    "    weight_decay = 1e-6 #@param {'type':'number'}\n",
    "    maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "    lr = 2e-4 #@param {'type':'number'}\n",
    "    hidden_size = 250\n",
    "    num_layers = 3\n",
    "    batch_size = real_minortiy.shape[0] # Full batch\n",
    "\n",
    "    ds_X_minority = ae.train_autoencoder(X_minority, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "    latent_features = ds[1].detach()\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    array = latent_features.detach().cpu().numpy()\n",
    "    forest_model = ForestDiffusionModel(array, label_y=None, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)\n",
    "    minority_fake = forest_model.generate(batch_size=len(real_minortiy)) # Adjust the batch size to create a balanced dataset\n",
    "    # sample=torch.tensor(minority_fake, dtype=torch.float32)\n",
    "    # sample.shape\n",
    "    # gen_output = ds[0](sample, ds[2], ds[3])\n",
    "    # gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "\n",
    "\n",
    "\n",
    "    # augmented_output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Augmented Data\\\\'\n",
    "    # filename = f'{Model}+Forest_{dataset}_Augmented.csv'\n",
    "    # augmented_output_file = os.path.join(augmented_output_directory, filename)\n",
    "    # syn_df.to_csv(augmented_output_file, index=False) \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(260, 13)\n",
      "(260, 13)\n"
     ]
    }
   ],
   "source": [
    "print(type(array))\n",
    "print(array.shape)\n",
    "# print(real_df.shape)\n",
    "print(minority_fake.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8476851   0.6754753   1.1448116  ... -0.61594456 -1.0673927\n",
      "  -0.7629952 ]\n",
      " [ 0.6765012   0.4272185  -0.22333136 ... -1.0574458  -1.107088\n",
      "   0.02813436]\n",
      " [ 0.77710074  0.10258555  0.78371805 ... -1.2053951  -0.1259919\n",
      "  -1.2822082 ]\n",
      " ...\n",
      " [-0.0473737   0.08445081  0.4095962  ... -0.56679124 -0.00832558\n",
      "  -0.4477823 ]\n",
      " [ 0.40814197  0.5695954   0.09218815 ... -1.2956736  -0.35899812\n",
      "  -0.7206599 ]\n",
      " [ 0.39317212  0.612952   -0.41354346 ... -1.4080838  -1.0621437\n",
      "  -0.40539533]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.077980</td>\n",
       "      <td>-15.055771</td>\n",
       "      <td>-12.541447</td>\n",
       "      <td>-8.873666</td>\n",
       "      <td>-4.389857</td>\n",
       "      <td>7.341290</td>\n",
       "      <td>-10.188528</td>\n",
       "      <td>7.184702</td>\n",
       "      <td>-14.179997</td>\n",
       "      <td>7.638948</td>\n",
       "      <td>7.483530</td>\n",
       "      <td>-8.648089</td>\n",
       "      <td>4.066612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.161444</td>\n",
       "      <td>-24.312395</td>\n",
       "      <td>-14.273751</td>\n",
       "      <td>-10.290889</td>\n",
       "      <td>-2.314354</td>\n",
       "      <td>8.544633</td>\n",
       "      <td>-6.862887</td>\n",
       "      <td>6.513667</td>\n",
       "      <td>-20.340391</td>\n",
       "      <td>3.810999</td>\n",
       "      <td>17.796349</td>\n",
       "      <td>-9.882925</td>\n",
       "      <td>1.272065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.442517</td>\n",
       "      <td>-4.512757</td>\n",
       "      <td>-5.885982</td>\n",
       "      <td>-0.427331</td>\n",
       "      <td>-5.220022</td>\n",
       "      <td>-4.780949</td>\n",
       "      <td>-4.378103</td>\n",
       "      <td>13.856998</td>\n",
       "      <td>-3.597708</td>\n",
       "      <td>9.397711</td>\n",
       "      <td>-6.992277</td>\n",
       "      <td>-5.273534</td>\n",
       "      <td>11.595331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.622749</td>\n",
       "      <td>-47.598675</td>\n",
       "      <td>-8.035440</td>\n",
       "      <td>-4.906667</td>\n",
       "      <td>-9.887181</td>\n",
       "      <td>16.637634</td>\n",
       "      <td>8.108945</td>\n",
       "      <td>6.682512</td>\n",
       "      <td>-5.649014</td>\n",
       "      <td>4.487415</td>\n",
       "      <td>9.396755</td>\n",
       "      <td>-3.408261</td>\n",
       "      <td>11.982435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.068099</td>\n",
       "      <td>-8.651458</td>\n",
       "      <td>-7.440869</td>\n",
       "      <td>-1.279265</td>\n",
       "      <td>-8.687286</td>\n",
       "      <td>-5.127207</td>\n",
       "      <td>-8.518230</td>\n",
       "      <td>13.493593</td>\n",
       "      <td>-3.051555</td>\n",
       "      <td>11.790753</td>\n",
       "      <td>0.275342</td>\n",
       "      <td>-1.655885</td>\n",
       "      <td>6.305118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1.245036</td>\n",
       "      <td>-38.705284</td>\n",
       "      <td>-4.749161</td>\n",
       "      <td>-5.972995</td>\n",
       "      <td>-9.175056</td>\n",
       "      <td>8.476085</td>\n",
       "      <td>5.486655</td>\n",
       "      <td>12.370555</td>\n",
       "      <td>-3.316094</td>\n",
       "      <td>1.311978</td>\n",
       "      <td>1.307320</td>\n",
       "      <td>-2.815952</td>\n",
       "      <td>12.672902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-6.881784</td>\n",
       "      <td>-14.146931</td>\n",
       "      <td>-9.094768</td>\n",
       "      <td>-12.008457</td>\n",
       "      <td>3.567058</td>\n",
       "      <td>5.734353</td>\n",
       "      <td>-0.568215</td>\n",
       "      <td>14.993578</td>\n",
       "      <td>-2.334203</td>\n",
       "      <td>-1.499555</td>\n",
       "      <td>0.053442</td>\n",
       "      <td>-6.648342</td>\n",
       "      <td>11.487378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.482998</td>\n",
       "      <td>-59.255409</td>\n",
       "      <td>-12.081550</td>\n",
       "      <td>-16.564106</td>\n",
       "      <td>-6.138309</td>\n",
       "      <td>19.849867</td>\n",
       "      <td>4.292315</td>\n",
       "      <td>9.117833</td>\n",
       "      <td>-15.240891</td>\n",
       "      <td>-5.854153</td>\n",
       "      <td>17.994995</td>\n",
       "      <td>-6.251715</td>\n",
       "      <td>6.015072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>3.322240</td>\n",
       "      <td>-15.822894</td>\n",
       "      <td>-14.267989</td>\n",
       "      <td>-9.964271</td>\n",
       "      <td>-5.580717</td>\n",
       "      <td>4.163420</td>\n",
       "      <td>-15.139224</td>\n",
       "      <td>3.119954</td>\n",
       "      <td>-19.759991</td>\n",
       "      <td>8.025823</td>\n",
       "      <td>11.851574</td>\n",
       "      <td>-11.476839</td>\n",
       "      <td>-1.463071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-6.691861</td>\n",
       "      <td>-44.558380</td>\n",
       "      <td>-11.446699</td>\n",
       "      <td>-13.179346</td>\n",
       "      <td>-2.314366</td>\n",
       "      <td>19.940943</td>\n",
       "      <td>9.816315</td>\n",
       "      <td>15.033955</td>\n",
       "      <td>-5.961756</td>\n",
       "      <td>-2.084843</td>\n",
       "      <td>8.304293</td>\n",
       "      <td>-4.992063</td>\n",
       "      <td>15.914901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3         4          5   \\\n",
       "0    3.077980 -15.055771 -12.541447  -8.873666 -4.389857   7.341290   \n",
       "1   -2.161444 -24.312395 -14.273751 -10.290889 -2.314354   8.544633   \n",
       "2   -3.442517  -4.512757  -5.885982  -0.427331 -5.220022  -4.780949   \n",
       "3   -2.622749 -47.598675  -8.035440  -4.906667 -9.887181  16.637634   \n",
       "4   -3.068099  -8.651458  -7.440869  -1.279265 -8.687286  -5.127207   \n",
       "..        ...        ...        ...        ...       ...        ...   \n",
       "263  1.245036 -38.705284  -4.749161  -5.972995 -9.175056   8.476085   \n",
       "264 -6.881784 -14.146931  -9.094768 -12.008457  3.567058   5.734353   \n",
       "265 -0.482998 -59.255409 -12.081550 -16.564106 -6.138309  19.849867   \n",
       "266  3.322240 -15.822894 -14.267989  -9.964271 -5.580717   4.163420   \n",
       "267 -6.691861 -44.558380 -11.446699 -13.179346 -2.314366  19.940943   \n",
       "\n",
       "            6          7          8          9          10         11  \\\n",
       "0   -10.188528   7.184702 -14.179997   7.638948   7.483530  -8.648089   \n",
       "1    -6.862887   6.513667 -20.340391   3.810999  17.796349  -9.882925   \n",
       "2    -4.378103  13.856998  -3.597708   9.397711  -6.992277  -5.273534   \n",
       "3     8.108945   6.682512  -5.649014   4.487415   9.396755  -3.408261   \n",
       "4    -8.518230  13.493593  -3.051555  11.790753   0.275342  -1.655885   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "263   5.486655  12.370555  -3.316094   1.311978   1.307320  -2.815952   \n",
       "264  -0.568215  14.993578  -2.334203  -1.499555   0.053442  -6.648342   \n",
       "265   4.292315   9.117833 -15.240891  -5.854153  17.994995  -6.251715   \n",
       "266 -15.139224   3.119954 -19.759991   8.025823  11.851574 -11.476839   \n",
       "267   9.816315  15.033955  -5.961756  -2.084843   8.304293  -4.992063   \n",
       "\n",
       "            12  \n",
       "0     4.066612  \n",
       "1     1.272065  \n",
       "2    11.595331  \n",
       "3    11.982435  \n",
       "4     6.305118  \n",
       "..         ...  \n",
       "263  12.672902  \n",
       "264  11.487378  \n",
       "265   6.015072  \n",
       "266  -1.463071  \n",
       "267  15.914901  \n",
       "\n",
       "[268 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(array)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ForestDiffusion import ForestDiffusionModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "\n",
    "    X = real_df.iloc[:, :-1].values  # Features\n",
    "    y = real_df.iloc[:, -1].values \n",
    "    # Check and print the original class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist_before = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "    X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "    y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "    # Check and print the Augmented class distribution\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Train a simple classifier on both original and generated datasets\n",
    "    clf_orig = RandomForestClassifier(random_state=42)\n",
    "    clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Step 8: Predict and calculate recall and F1 scores\n",
    "    y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "    y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "    prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "    prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "    \n",
    "    recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "    recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "    f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    # Step 9: Print and store the performance metrics\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "    \"Dataset\": dataset,\n",
    "    \"Precision_Original\": prec_orig,\n",
    "    \"Precision_Generated\": prec_bal,\n",
    "    \"Recall_Original\": recall_orig,\n",
    "    \"Recall_Generated\": recalls_bal,\n",
    "    \"F1_Original\": f1_orig,   \n",
    "    \"F1_Generated\": f1_bal,\n",
    "    \"Num_Fake_Samples\": len(augmented_df) - len(real_df),\n",
    "    \"Synthetic/Original_Ratio\": 100*(len(augmented_df) - len(real_df))/len(real_minortiy)\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "    print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "    print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "    print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "    print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "    print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "    print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "    print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "    print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(\"Auto_Diff_Forest_different_datasets_metric.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision_Original</th>\n",
       "      <th>Recall_Original</th>\n",
       "      <th>F1_Original</th>\n",
       "      <th>Precision_Generated</th>\n",
       "      <th>Recall_Generated</th>\n",
       "      <th>F1_Generated</th>\n",
       "      <th>Num_Fake_Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yeast_ml8_dataset</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oil</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mammography</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HTRU</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.835391</td>\n",
       "      <td>0.883569</td>\n",
       "      <td>0.979358</td>\n",
       "      <td>0.878601</td>\n",
       "      <td>0.926247</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.774566</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>creditcard_sampled</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dataset  Precision_Original  Recall_Original  F1_Original  \\\n",
       "0   yeast_ml8_dataset            0.000000         0.000000     0.000000   \n",
       "1                 oil            0.600000         0.272727     0.375000   \n",
       "2         mammography            0.916667         0.594595     0.721311   \n",
       "3                HTRU            0.937644         0.835391     0.883569   \n",
       "4            diabetes            0.638554         0.662500     0.650307   \n",
       "5  creditcard_sampled            0.866667         0.764706     0.812500   \n",
       "\n",
       "   Precision_Generated  Recall_Generated  F1_Generated  Num_Fake_Samples  \n",
       "0             1.000000          0.551020      0.710526               100  \n",
       "1             0.875000          0.636364      0.736842                41  \n",
       "2             0.966102          0.770270      0.857143               100  \n",
       "3             0.979358          0.878601      0.926247               100  \n",
       "4             0.720430          0.837500      0.774566               100  \n",
       "5             0.875000          0.823529      0.848485                50  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "metrics_df=pd.read_csv(\"Auto_Diff_Forest_different_datasets_metric.csv\")\n",
    "metrics_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision_Original</th>\n",
       "      <th>Recall_Original</th>\n",
       "      <th>F1_Original</th>\n",
       "      <th>Precision_Generated</th>\n",
       "      <th>Recall_Generated</th>\n",
       "      <th>F1_Generated</th>\n",
       "      <th>Num_Fake_Samples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>creditcard_sampled</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dataset  Precision_Original  Recall_Original  F1_Original  \\\n",
       "5  creditcard_sampled            0.866667         0.764706       0.8125   \n",
       "\n",
       "   Precision_Generated  Recall_Generated  F1_Generated  Num_Fake_Samples  \n",
       "5                0.875          0.823529      0.848485                50  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[metrics_df['Dataset']=='creditcard_sampled'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
