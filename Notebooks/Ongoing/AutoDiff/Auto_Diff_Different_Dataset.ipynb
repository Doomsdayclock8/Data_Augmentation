{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3424a997",
   "metadata": {},
   "source": [
    "# AutoDiff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0191ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion' \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17485749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as ae\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4075527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Datasets\\Original Data\\diabetes.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9803b39003fc41ea9fb620adf1d81f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b118777461e74c3ab0e5640ad5fa961b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6577ccff53a9437fb37e6edb6a10d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Class distribution after augmentation: {0.0: 500, 1.0: 368}\n",
      "Precision score (original data): 0.6386\n",
      "Precision score (generated data): 0.7470\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.7750\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7607\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87       151\n",
      "           1       0.75      0.78      0.76        80\n",
      "\n",
      "    accuracy                           0.83       231\n",
      "   macro avg       0.81      0.82      0.82       231\n",
      "weighted avg       0.83      0.83      0.83       231\n",
      "\n",
      "Number of fake samples generated: 100\n",
      "..\\..\\..\\Datasets\\Original Data\\creditcard_sampled.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffad9629ed9e46dab64fde1dc2a8472c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eadf3c70cf4994bf3789b6539264c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34c93098ce6407b819cde1e42b76f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 4000, 1: 50}\n",
      "Class distribution after augmentation: {0.0: 4000, 1.0: 100}\n",
      "Precision score (original data): 0.8667\n",
      "Precision score (generated data): 0.8750\n",
      "Recall score (original data): 0.7647\n",
      "Recall score (generated data): 0.8235\n",
      "F1 score (original data): 0.8125\n",
      "F1 score (generated data): 0.8485\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.93      0.88      0.90      1215\n",
      "weighted avg       0.99      1.00      0.99      1215\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.94      0.91      0.92      1215\n",
      "weighted avg       1.00      1.00      1.00      1215\n",
      "\n",
      "Number of fake samples generated: 50\n",
      "..\\..\\..\\Datasets\\Original Data\\HTRU.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a90ac7e94c482485cd1a586d9e35af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d522650c9a4a4f99a2767a85332e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1962229b46448e48e33d3f358b66d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 16259, 1: 1639}\n",
      "Class distribution after augmentation: {0.0: 16259, 1.0: 1739}\n",
      "Precision score (original data): 0.9376\n",
      "Precision score (generated data): 0.9724\n",
      "Recall score (original data): 0.8354\n",
      "Recall score (generated data): 0.8704\n",
      "F1 score (original data): 0.8836\n",
      "F1 score (generated data): 0.9186\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4884\n",
      "           1       0.94      0.84      0.88       486\n",
      "\n",
      "    accuracy                           0.98      5370\n",
      "   macro avg       0.96      0.91      0.94      5370\n",
      "weighted avg       0.98      0.98      0.98      5370\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      4884\n",
      "           1       0.97      0.87      0.92       486\n",
      "\n",
      "    accuracy                           0.99      5370\n",
      "   macro avg       0.98      0.93      0.96      5370\n",
      "weighted avg       0.99      0.99      0.99      5370\n",
      "\n",
      "Number of fake samples generated: 100\n",
      "..\\..\\..\\Datasets\\Original Data\\mammography.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b495551cec4a4fb0fa0d854edc316f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e622d2b0e494886b8878eb487348c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86e4be037484bf6b8d71e123acfd20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Class distribution after augmentation: {-1.0: 10923, 1.0: 360}\n",
      "Precision score (original data): 0.9167\n",
      "Precision score (generated data): 0.8966\n",
      "Recall score (original data): 0.5946\n",
      "Recall score (generated data): 0.7027\n",
      "F1 score (original data): 0.7213\n",
      "F1 score (generated data): 0.7879\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      0.99      3281\n",
      "           1       0.92      0.59      0.72        74\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.95      0.80      0.86      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      1.00      3281\n",
      "           1       0.90      0.70      0.79        74\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.94      0.85      0.89      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "# strings_set = {'diabetes','oil','yeast_ml8_dataset','creditcard_sampled','HTRU','mammography'}\n",
    "strings_set = {'diabetes','creditcard_sampled','HTRU','mammography'}\n",
    "Model = 'AutoDiff'\n",
    "# dataset = 'diabetes'\n",
    "metrics_list = []\n",
    "\n",
    "for dataset in strings_set:\n",
    "    \n",
    "    filename = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    # Read dataframe\n",
    "    print(filename)\n",
    "    real_df = pd.read_csv(filename)\n",
    "    #real_df = real_df.drop('url', axis=1)\n",
    "    # Step 2: Inspect the data and check for class imbalance\n",
    "    # Assuming the last column is the label, and the rest are features\n",
    "    X = real_df.iloc[:, :-1].values  # Features\n",
    "    y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "     # Separate the minority class\n",
    "    real_minortiy = real_df[y == 1]\n",
    "    \n",
    "    threshold = 0.01 # Threshold for mixed-type variables\n",
    "    parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder hyper-parameters\n",
    "    device = 'cuda' #@param {'type':'string'}\n",
    "    n_epochs = 2000 #@param {'type':'integer'}\n",
    "    eps = 1e-5 #@param {type:\"number\"}\n",
    "    weight_decay = 1e-6 #@param {'type':'number'}\n",
    "    maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "    lr = 2e-4 #@param {'type':'number'}\n",
    "    hidden_size = 250\n",
    "    num_layers = 3\n",
    "    batch_size = 50\n",
    "\n",
    "    ds = ae.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "    latent_features = ds[1].detach()\n",
    "\n",
    "    ################################################################################################################\n",
    "    # diffusion hyper-parameters\n",
    "    diff_n_epochs = 2000 #@param {'type':'integer'}\n",
    "    hidden_dims = (256, 512, 1024, 512, 256) #@param {type:\"raw\"}\n",
    "    converted_table_dim = latent_features.shape[1] #@param {'type':'integer'}\n",
    "    sigma = 20  #@param {'type':'integer'} \n",
    "    num_batches_per_epoch = 50 #@param {'type':'number'}\n",
    "    batch_size = 50 #@param {'type':'integer'}\n",
    "    T = 100  #@param {'type':'integer'}\n",
    "\n",
    "    score = TabDiff.train_diffusion(latent_features, T, eps, sigma, lr, \\\n",
    "                        num_batches_per_epoch, maximum_learning_rate, weight_decay, diff_n_epochs, batch_size)\n",
    "\n",
    "############################################################################################################### \n",
    "\n",
    "    # Generate fake tabular datasets\n",
    "    T = 300; N = latent_features.shape[0]; P = latent_features.shape[1]\n",
    "\n",
    "\n",
    "    sample = diff.Euler_Maruyama_sampling(score, T, N, P, device)\n",
    "\n",
    "\n",
    "    # duration = end_time - start_time\n",
    "\n",
    "    # time_duration.append(duration)\n",
    "\n",
    "    gen_output = ds[0](sample, ds[2], ds[3])\n",
    "    gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "    selected_samples = gen_df.sample(n=min(100,gen_df.shape[0]), random_state=42)  # For reproducibility\n",
    "    # Syn _df will be the dataset after augmentation\n",
    "    syn_df = pd.concat([real_df, selected_samples], ignore_index=True)\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ForestDiffusion import ForestDiffusionModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import recall_score, f1_score\n",
    "    \n",
    "    augmented_df = syn_df\n",
    "\n",
    "    # Check and print the original class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist_before = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "    X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "    y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "    # Check and print the Augmented class distribution\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Train a simple classifier on both original and generated datasets\n",
    "    clf_orig = RandomForestClassifier(random_state=42)\n",
    "    clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Step 8: Predict and calculate recall and F1 scores\n",
    "    y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "    y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "    prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "    prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "    recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "    f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    # Step 9: Print and store the performance metrics\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "    \"Dataset\": dataset,\n",
    "    \"Precision_Original\": prec_orig,\n",
    "    \"Precision_Generated\": prec_bal,\n",
    "    \"Recall_Original\": recall_orig,\n",
    "    \"Recall_Generated\": recalls_bal,\n",
    "    \"F1_Original\": f1_orig,   \n",
    "    \"F1_Generated\": f1_bal,\n",
    "    \"Num_Fake_Samples\": len(augmented_df) - len(real_df),\n",
    "    \"Synthetic/Original_Ratio\":100*(len(augmented_df) - len(real_df))/len(real_minortiy)\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "    print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "    print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "    print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "    print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "    print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "    print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "    print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "    print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233d259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Dataset  Precision_Original  Precision_Generated  \\\n",
      "0            diabetes            0.638554             0.746988   \n",
      "1  creditcard_sampled            0.866667             0.875000   \n",
      "2                HTRU            0.937644             0.972414   \n",
      "3         mammography            0.916667             0.896552   \n",
      "\n",
      "   Recall_Original  Recall_Generated  F1_Original  F1_Generated  \\\n",
      "0         0.662500          0.775000     0.650307      0.760736   \n",
      "1         0.764706          0.823529     0.812500      0.848485   \n",
      "2         0.835391          0.870370     0.883569      0.918567   \n",
      "3         0.594595          0.702703     0.721311      0.787879   \n",
      "\n",
      "   Num_Fake_Samples  Synthetic/Original_Ratio  \n",
      "0               100                 37.313433  \n",
      "1                50                100.000000  \n",
      "2               100                  6.101281  \n",
      "3               100                 38.461538  \n"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(metrics_df.head())\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(\"Auto_Diff_different_datasets_metric.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
