{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3424a997",
   "metadata": {},
   "source": [
    "# AutoDiff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0191ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion' \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17485749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as ae\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4075527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Datasets\\Original Data\\diabetes.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a199cae2cb942699b89d80c306f1753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e5f70a41c456492d7535d68c479b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d147cca31ff24597bd9c31198e340dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Class distribution after augmentation: {0.0: 500, 1.0: 368}\n",
      "Precision score (original data): 0.6386\n",
      "Precision score (generated data): 0.6875\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8250\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7500\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.80      0.85       151\n",
      "           1       0.69      0.82      0.75        80\n",
      "\n",
      "    accuracy                           0.81       231\n",
      "   macro avg       0.79      0.81      0.80       231\n",
      "weighted avg       0.82      0.81      0.81       231\n",
      "\n",
      "Number of fake samples generated: 100\n",
      "..\\..\\..\\Datasets\\Original Data\\creditcard_sampled.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d530a7783ee4ccc89cfaee19b8f9191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50fad97549d43888d0e0a8d8577bb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b59107e589482ca3790ee2b0cf5e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 4000, 1: 50}\n",
      "Class distribution after augmentation: {0.0: 4000, 1.0: 100}\n",
      "Precision score (original data): 0.8667\n",
      "Precision score (generated data): 0.8750\n",
      "Recall score (original data): 0.7647\n",
      "Recall score (generated data): 0.8235\n",
      "F1 score (original data): 0.8125\n",
      "F1 score (generated data): 0.8485\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.87      0.76      0.81        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.93      0.88      0.90      1215\n",
      "weighted avg       0.99      1.00      0.99      1215\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1198\n",
      "           1       0.88      0.82      0.85        17\n",
      "\n",
      "    accuracy                           1.00      1215\n",
      "   macro avg       0.94      0.91      0.92      1215\n",
      "weighted avg       1.00      1.00      1.00      1215\n",
      "\n",
      "Number of fake samples generated: 50\n",
      "..\\..\\..\\Datasets\\Original Data\\HTRU.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e755412d03084583a173cce0751738e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e791b0c72a4f78bee8538c5cc0f597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79056075bd5744979bd6c7161348d9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 16259, 1: 1639}\n",
      "Class distribution after augmentation: {0.0: 16259, 1.0: 1739}\n",
      "Precision score (original data): 0.9376\n",
      "Precision score (generated data): 0.9747\n",
      "Recall score (original data): 0.8354\n",
      "Recall score (generated data): 0.8704\n",
      "F1 score (original data): 0.8836\n",
      "F1 score (generated data): 0.9196\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4884\n",
      "           1       0.94      0.84      0.88       486\n",
      "\n",
      "    accuracy                           0.98      5370\n",
      "   macro avg       0.96      0.91      0.94      5370\n",
      "weighted avg       0.98      0.98      0.98      5370\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      4884\n",
      "           1       0.97      0.87      0.92       486\n",
      "\n",
      "    accuracy                           0.99      5370\n",
      "   macro avg       0.98      0.93      0.96      5370\n",
      "weighted avg       0.99      0.99      0.99      5370\n",
      "\n",
      "Number of fake samples generated: 100\n",
      "..\\..\\..\\Datasets\\Original Data\\mammography.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5108043a594f0f8acaef185cd81bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ff8b0a9eea4d3b855c9601be64f5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a21829c797483ea17dea8a7e01375e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\diffusion.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {-1: 10923, 1: 260}\n",
      "Class distribution after augmentation: {-1.0: 10923, 1.0: 360}\n",
      "Precision score (original data): 0.9167\n",
      "Precision score (generated data): 0.9483\n",
      "Recall score (original data): 0.5946\n",
      "Recall score (generated data): 0.7432\n",
      "F1 score (original data): 0.7213\n",
      "F1 score (generated data): 0.8333\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      0.99      3281\n",
      "           1       0.92      0.59      0.72        74\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.95      0.80      0.86      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.99      1.00      1.00      3281\n",
      "           1       0.95      0.74      0.83        74\n",
      "\n",
      "    accuracy                           0.99      3355\n",
      "   macro avg       0.97      0.87      0.91      3355\n",
      "weighted avg       0.99      0.99      0.99      3355\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "# strings_set = {'diabetes','oil','yeast_ml8_dataset','creditcard_sampled','HTRU','mammography'}\n",
    "strings_set = {'diabetes','creditcard_sampled','HTRU','mammography'}\n",
    "Model = 'AutoDiff'\n",
    "# dataset = 'diabetes'\n",
    "metrics_list = []\n",
    "\n",
    "for dataset in strings_set:\n",
    "    \n",
    "    filename = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    # Read dataframe\n",
    "    print(filename)\n",
    "    real_df = pd.read_csv(filename)\n",
    "    #real_df = real_df.drop('url', axis=1)\n",
    "    # Step 2: Inspect the data and check for class imbalance\n",
    "    # Assuming the last column is the label, and the rest are features\n",
    "    X = real_df.iloc[:, :-1].values  # Features\n",
    "    y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "     # Separate the minority class\n",
    "    real_minortiy = real_df[y == 1]\n",
    "    \n",
    "    threshold = 0.01 # Threshold for mixed-type variables\n",
    "    parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder hyper-parameters\n",
    "    device = 'cuda' #@param {'type':'string'}\n",
    "    n_epochs = 2000 #@param {'type':'integer'}\n",
    "    eps = 1e-5 #@param {type:\"number\"}\n",
    "    weight_decay = 1e-6 #@param {'type':'number'}\n",
    "    maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "    lr = 2e-4 #@param {'type':'number'}\n",
    "    hidden_size = 250\n",
    "    num_layers = 3\n",
    "    batch_size = 50\n",
    "\n",
    "    ds = ae.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "    latent_features = ds[1].detach()\n",
    "\n",
    "    ################################################################################################################\n",
    "    # diffusion hyper-parameters\n",
    "    diff_n_epochs = 2000 #@param {'type':'integer'}\n",
    "    hidden_dims = (256, 512, 1024, 512, 256) #@param {type:\"raw\"}\n",
    "    converted_table_dim = latent_features.shape[1] #@param {'type':'integer'}\n",
    "    sigma = 20  #@param {'type':'integer'} \n",
    "    num_batches_per_epoch = 50 #@param {'type':'number'}\n",
    "    batch_size = 50 #@param {'type':'integer'}\n",
    "    T = 100  #@param {'type':'integer'}\n",
    "\n",
    "    score = TabDiff.train_diffusion(latent_features, T, eps, sigma, lr, \\\n",
    "                        num_batches_per_epoch, maximum_learning_rate, weight_decay, diff_n_epochs, batch_size)\n",
    "\n",
    "############################################################################################################### \n",
    "\n",
    "    # Generate fake tabular datasets\n",
    "    T = 300; N = latent_features.shape[0]; P = latent_features.shape[1]\n",
    "\n",
    "\n",
    "    sample = diff.Euler_Maruyama_sampling(score, T, N, P, device)\n",
    "\n",
    "\n",
    "    # duration = end_time - start_time\n",
    "\n",
    "    # time_duration.append(duration)\n",
    "\n",
    "    gen_output = ds[0](sample, ds[2], ds[3])\n",
    "    gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "    selected_samples = gen_df.sample(n=min(100,gen_df.shape[0]), random_state=42)  # For reproducibility\n",
    "    # Syn _df will be the dataset after augmentation\n",
    "    syn_df = pd.concat([real_df, selected_samples], ignore_index=True)\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ForestDiffusion import ForestDiffusionModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import recall_score, f1_score\n",
    "    \n",
    "    augmented_df = syn_df\n",
    "\n",
    "    # Check and print the original class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist_before = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "    X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "    y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "    # Check and print the Augmented class distribution\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Train a simple classifier on both original and generated datasets\n",
    "    clf_orig = RandomForestClassifier(random_state=42)\n",
    "    clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Step 8: Predict and calculate recall and F1 scores\n",
    "    y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "    y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "    prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "    prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "    recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "    f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    # Step 9: Print and store the performance metrics\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "    \"Dataset\": dataset,\n",
    "    \"Precision_Original\": prec_orig,\n",
    "    \"Precision_Generated\": prec_bal,\n",
    "    \"Recall_Original\": recall_orig,\n",
    "    \"Recall_Generated\": recalls_bal,\n",
    "    \"F1_Original\": f1_orig,   \n",
    "    \"F1_Generated\": f1_bal,\n",
    "    \"Num_Fake_Samples\": len(augmented_df) - len(real_df),\n",
    "    \"Synthetic/Original_Ratio\":100*(len(augmented_df) - len(real_df))/len(real_minortiy)\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "    print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "    print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "    print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "    print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "    print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "    print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "    print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "    print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233d259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Dataset  Precision_Original  Precision_Generated  \\\n",
      "0            diabetes            0.638554             0.687500   \n",
      "1  creditcard_sampled            0.866667             0.875000   \n",
      "2                HTRU            0.937644             0.974654   \n",
      "3         mammography            0.916667             0.948276   \n",
      "\n",
      "   Recall_Original  Recall_Generated  F1_Original  F1_Generated  \\\n",
      "0         0.662500          0.825000     0.650307      0.750000   \n",
      "1         0.764706          0.823529     0.812500      0.848485   \n",
      "2         0.835391          0.870370     0.883569      0.919565   \n",
      "3         0.594595          0.743243     0.721311      0.833333   \n",
      "\n",
      "   Num_Fake_Samples  Synthetic/Original_Ratio  \n",
      "0               100                 37.313433  \n",
      "1                50                100.000000  \n",
      "2               100                  6.101281  \n",
      "3               100                 38.461538  \n"
     ]
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "print(metrics_df.head())\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(\"Auto_Diff_different_datasets_metric.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7736c7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision_Original</th>\n",
       "      <th>Precision_Generated</th>\n",
       "      <th>Recall_Original</th>\n",
       "      <th>Recall_Generated</th>\n",
       "      <th>F1_Original</th>\n",
       "      <th>F1_Generated</th>\n",
       "      <th>Num_Fake_Samples</th>\n",
       "      <th>Synthetic/Original_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>diabetes</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.650307</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>100</td>\n",
       "      <td>37.313433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>creditcard_sampled</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>50</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTRU</td>\n",
       "      <td>0.937644</td>\n",
       "      <td>0.974654</td>\n",
       "      <td>0.835391</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.883569</td>\n",
       "      <td>0.919565</td>\n",
       "      <td>100</td>\n",
       "      <td>6.101281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mammography</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.743243</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>100</td>\n",
       "      <td>38.461538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dataset  Precision_Original  Precision_Generated  \\\n",
       "0            diabetes            0.638554             0.687500   \n",
       "1  creditcard_sampled            0.866667             0.875000   \n",
       "2                HTRU            0.937644             0.974654   \n",
       "3         mammography            0.916667             0.948276   \n",
       "\n",
       "   Recall_Original  Recall_Generated  F1_Original  F1_Generated  \\\n",
       "0         0.662500          0.825000     0.650307      0.750000   \n",
       "1         0.764706          0.823529     0.812500      0.848485   \n",
       "2         0.835391          0.870370     0.883569      0.919565   \n",
       "3         0.594595          0.743243     0.721311      0.833333   \n",
       "\n",
       "   Num_Fake_Samples  Synthetic/Original_Ratio  \n",
       "0               100                 37.313433  \n",
       "1                50                100.000000  \n",
       "2               100                  6.101281  \n",
       "3               100                 38.461538  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db7773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision_Original</th>\n",
       "      <th>Precision_Generated</th>\n",
       "      <th>Recall_Original</th>\n",
       "      <th>Recall_Generated</th>\n",
       "      <th>F1_Original</th>\n",
       "      <th>F1_Generated</th>\n",
       "      <th>Num_Fake_Samples</th>\n",
       "      <th>Synthetic/Original_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>creditcard_sampled</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>50</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Dataset  Precision_Original  Precision_Generated  \\\n",
       "1  creditcard_sampled            0.866667                0.875   \n",
       "\n",
       "   Recall_Original  Recall_Generated  F1_Original  F1_Generated  \\\n",
       "1         0.764706          0.823529       0.8125      0.848485   \n",
       "\n",
       "   Num_Fake_Samples  Synthetic/Original_Ratio  \n",
       "1                50                     100.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df[metrics_df['Dataset']=='creditcard_sampled'] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
