{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3424a997",
   "metadata": {},
   "source": [
    "# AutoDiff Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0191ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion' \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17485749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as ae\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4075527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Datasets\\Original Data\\diabetes.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e7325eecaa49a8b16304ad7bb97861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5b93dae8804c74ac653d642081e9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:270: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n",
      "c:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\AutoDiff\\..\\..\\..\\Models\\AutoDiffusion\\TabDDPMdiff.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t)\n"
     ]
    }
   ],
   "source": [
    "# strings_set = {'diabetes','oil','yeast_ml8_dataset','creditcard_sampled','HTRU','mammography'}\n",
    "# strings_set = {'diabetes','oil','creditcard_sampled','HTRU','mammography'}\n",
    "strings_set = {'diabetes'}\n",
    "Model = 'AutoDiff'\n",
    "# dataset = 'diabetes'\n",
    "metrics_list = []\n",
    "\n",
    "for dataset in strings_set:\n",
    "    \n",
    "    filename = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    # Read dataframe\n",
    "    print(filename)\n",
    "    real_df = pd.read_csv(filename)\n",
    "    #real_df = real_df.drop('url', axis=1)\n",
    "    # Step 2: Inspect the data and check for class imbalance\n",
    "    # Assuming the last column is the label, and the rest are features\n",
    "    X = real_df.iloc[:, :-1].values  # Features\n",
    "    y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "     # Separate the minority class\n",
    "    real_minortiy = real_df[y == 1]\n",
    "    \n",
    "    threshold = 0.01 # Threshold for mixed-type variables\n",
    "    parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "    ################################################################################################################\n",
    "    # Auto-encoder hyper-parameters\n",
    "    device = 'cuda' #@param {'type':'string'}\n",
    "    n_epochs = 100 #@param {'type':'integer'}\n",
    "    eps = 1e-5 #@param {type:\"number\"}\n",
    "    weight_decay = 1e-6 #@param {'type':'number'}\n",
    "    maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "    lr = 2e-4 #@param {'type':'number'}\n",
    "    hidden_size = 250\n",
    "    num_layers = 3\n",
    "    batch_size = 50\n",
    "\n",
    "    ds = ae.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "    latent_features = ds[1].detach()\n",
    "\n",
    "    ################################################################################################################\n",
    "    # diffusion hyper-parameters\n",
    "    diff_n_epochs = 100 #@param {'type':'integer'}\n",
    "    hidden_dims = (256, 512, 1024, 512, 256) #@param {type:\"raw\"}\n",
    "    converted_table_dim = latent_features.shape[1] #@param {'type':'integer'}\n",
    "    sigma = 20  #@param {'type':'integer'} \n",
    "    num_batches_per_epoch = 50 #@param {'type':'number'}\n",
    "    batch_size = 50 #@param {'type':'integer'}\n",
    "    T = 100  #@param {'type':'integer'}\n",
    "\n",
    "    score = TabDiff.train_diffusion(latent_features, T, eps, sigma, lr, \\\n",
    "                        num_batches_per_epoch, maximum_learning_rate, weight_decay, diff_n_epochs, batch_size)\n",
    "\n",
    "############################################################################################################### \n",
    "\n",
    "    # Generate fake tabular datasets\n",
    "    T = 300; N = latent_features.shape[0]; P = latent_features.shape[1]\n",
    "\n",
    "\n",
    "    sample = diff.Euler_Maruyama_sampling(score, T, N, P, device)\n",
    "\n",
    "\n",
    "    # duration = end_time - start_time\n",
    "\n",
    "    # time_duration.append(duration)\n",
    "\n",
    "    gen_output = ds[0](sample, ds[2], ds[3])\n",
    "    gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "    selected_samples = gen_df.sample(n=100, random_state=42)  # For reproducibility\n",
    "    # Syn _df will be the dataset after augmentation\n",
    "    syn_df = pd.concat([real_df, selected_samples], ignore_index=True)\n",
    "    output_directory = os.getcwd()+f'/{string}/'\n",
    "    filename = f'{Model}_{string}_Augmented.csv'\n",
    "\n",
    "    output_file = os.path.join(output_directory, filename)\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ForestDiffusion import ForestDiffusionModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import recall_score, f1_score\n",
    "    \n",
    "    augmented_df = syn_df\n",
    "\n",
    "    # Check and print the original class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_dist_before = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "    X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "    y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "    # Check and print the Augmented class distribution\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Step 7: Train a simple classifier on both original and generated datasets\n",
    "    clf_orig = RandomForestClassifier(random_state=42)\n",
    "    clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Step 8: Predict and calculate recall and F1 scores\n",
    "    y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "    y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "    prec_orig = precision_score(y_test_orig, y_pred_orig)\n",
    "    prec_bal = precision_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "    recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "    f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "    # Step 9: Print and store the performance metrics\n",
    "    # Store metrics in a dictionary\n",
    "    metrics = {\n",
    "    \"Dataset\": dataset,\n",
    "    \"Precision_Original\": prec_orig,\n",
    "    \"Precision_Generated\": prec_bal,\n",
    "    \"Recall_Original\": recall_orig,\n",
    "    \"Recall_Generated\": recalls_bal,\n",
    "    \"F1_Original\": f1_orig,   \n",
    "    \"F1_Generated\": f1_bal,\n",
    "    \"Num_Fake_Samples\": len(augmented_df) - len(real_df),\n",
    "    \"Synthetic/Original_Ratio\":100*(len(augmented_df) - len(real_df))/len(real_minortiy)\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "    print(f\"Precision score (original data): {prec_orig:.4f}\")\n",
    "    print(f\"Precision score (generated data): {prec_bal:.4f}\")\n",
    "    print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "    print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "    print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "    print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "    print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "    print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "    print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.head()\n",
    "# Save the DataFrame to a CSV file\n",
    "metrics_df.to_csv(\"Auto_Diff_Forest_CM_different_datasets_metric.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
