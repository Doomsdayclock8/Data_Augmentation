{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion'  \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as ae\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Datasets\\Original Data\\diabetes.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strings_set = {'abalone', 'adult', 'Churn_Modelling','faults', 'indian_liver_patient', \n",
    "#                'insurance', 'News', 'Obesity', 'Shoppers', 'Titanic', 'wilt', 'Bean', 'nursery', 'Magic', 'HTRU'}\n",
    "Model = 'AutoDiff'\n",
    "\n",
    "dataset='diabetes'\n",
    "file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "# Read dataframe\n",
    "print(file_path)\n",
    "real_df = pd.read_csv(file_path)\n",
    "# print(real_df.head())\n",
    "#real_df = real_df.drop('url', axis=1)\n",
    "# # Step 2: Inspect the data and check for class imbalance\n",
    "# # Assuming the last column is the label, and the rest are features\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "#  # Separate the minority class\n",
    "real_minortiy = real_df[y == 1]\n",
    "real_minortiy.shape\n",
    "real_minortiy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d410a19e86aa469f82d9139ac3bb303b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 0.01 # Threshold for mixed-type variables\n",
    "parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "################################################################################################################\n",
    "# Auto-encoder hyper-parameters\n",
    "device = 'cuda' #@param {'type':'string'}\n",
    "n_epochs = 2000 #@param {'type':'integer'}\n",
    "eps = 1e-5 #@param {type:\"number\"}\n",
    "weight_decay = 1e-6 #@param {'type':'number'}\n",
    "maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "lr = 2e-4 #@param {'type':'number'}\n",
    "hidden_size = 250\n",
    "num_layers = 3\n",
    "batch_size = real_minortiy.shape[0] # Full batch\n",
    "\n",
    "ds = ae.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "latent_features = ds[1].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ForestDiffusion import ForestDiffusionModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to NumPy array\n",
    "array = latent_features.detach().cpu().numpy()\n",
    "array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = ForestDiffusionModel(array, label_y=None, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 13])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_fake = forest_model.generate(batch_size=len(real_minortiy))  # Adjust the batch size to create a balanced dataset\n",
    "sample=torch.tensor(minority_fake, dtype=torch.float32)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output = ds[0](sample, ds[2], ds[3])\n",
    "gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "\n",
    "# output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Synthetic Data\\\\'\n",
    "# filename = f'{Model}+Forest_{dataset}_Synthetic.csv'\n",
    "# output_file = os.path.join(output_directory, filename)\n",
    "# gen_df.to_csv(output_file, index=False) \n",
    "\n",
    "\n",
    "# Select a random sample of the generated data\n",
    "selected_samples = gen_df.sample(n=min(100,gen_df.shape[0]), random_state=42)  # For reproducibility\n",
    "# Syn _df will be the dataset after augmentation\n",
    "syn_df = pd.concat([real_df, selected_samples], ignore_index=True)\n",
    "\n",
    "\n",
    "# augmented_output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Augmented Data\\\\'\n",
    "# filename = f'{Model}+Forest_{dataset}_Augmented.csv'\n",
    "# augmented_output_file = os.path.join(augmented_output_directory, filename)\n",
    "# syn_df.to_csv(augmented_output_file, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Class distribution after augmentation: {0.0: 500, 1.0: 368}\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8250\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7674\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       151\n",
      "           1       0.72      0.82      0.77        80\n",
      "\n",
      "    accuracy                           0.83       231\n",
      "   macro avg       0.81      0.83      0.81       231\n",
      "weighted avg       0.84      0.83      0.83       231\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "# real_df = pd.read_csv(filename)\n",
    "# syn_filename = f'{string}/{Model}_{string}_Augmented.csv'\n",
    "\n",
    "# augmented_df = pd.read_csv(syn_filename)\n",
    "augmented_df=syn_df\n",
    "\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values \n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Check and print the Augmented class distribution\n",
    "unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "class_dist_after = dict(zip(unique, counts))\n",
    "print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
