{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "\n",
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(data, epsilon, delta, sensitivity):\n",
    "    sigma = (sensitivity / epsilon) * np.sqrt(2 * np.log(1.25 / delta))\n",
    "    noise = np.random.normal(0, sigma, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Parameters for differential privacy\n",
    "epsilon = 1  # Privacy budget\n",
    "delta = 1e-5   \n",
    "sensitivity = 0.5\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('mammography_dataset.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Class distribution before augmentation\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "# Separate minority class\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "# Train-test split for original data\n",
    "X_train_orig, X_test, y_train_orig, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest on the original data\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "# Predict and calculate recall and F1 score on original data\n",
    "y_pred_orig = clf_orig.predict(X_test)\n",
    "recall_orig = recall_score(y_test, y_pred_orig)\n",
    "f1_orig = f1_score(y_test, y_pred_orig)\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "\n",
    "# Loop over different values of n\n",
    "n_values = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25]  # Example range of n values\n",
    "results = []\n",
    "\n",
    "for n in n_values:\n",
    "    print(f\"\\nRunning for n = {n}...\")\n",
    "\n",
    "    # Forest Diffusion Model for data augmentation\n",
    "    forest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100,\n",
    "                                         bin_indexes=[], cat_indexes=[], int_indexes=[],\n",
    "                                         diffusion_type='flow', n_jobs=-1)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    num_samples = round((len(X_minority)) * n)\n",
    "    X_minority_fake = forest_model.generate(batch_size=num_samples)\n",
    "\n",
    "    # Add Gaussian noise to synthetic data for differential privacy\n",
    "    X_minority_fake_noisy = add_gaussian_noise(X_minority_fake, epsilon, delta, sensitivity)\n",
    "\n",
    "    # Combine the noisy synthetic data with original data\n",
    "    X_balanced = np.concatenate((X, X_minority_fake_noisy), axis=0)\n",
    "    y_balanced = np.concatenate((y, np.ones(X_minority_fake_noisy.shape[0])), axis=0)\n",
    "\n",
    "    # Class distribution after augmentation\n",
    "    unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "    class_dist_after = dict(zip(unique, counts))\n",
    "    print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "    # Train-test split for augmented data\n",
    "    X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "\n",
    "    # Train a Random Forest on the augmented data\n",
    "    clf_bal = RandomForestClassifier(random_state=42)\n",
    "    clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "    # Predict and calculate recall and F1 score on augmented data\n",
    "    y_pred_bal = clf_bal.predict(X_test)\n",
    "    recall_bal = recall_score(y_test, y_pred_bal)\n",
    "    f1_bal = f1_score(y_test, y_pred_bal)\n",
    "    print(f\"Recall score (generated data with DP, n={n}): {recall_bal:.4f}\")\n",
    "    print(f\"F1 score (generated data with DP, n={n}): {f1_bal:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    results.append((n, recall_bal, f1_bal))\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "print(\"n\\tRecall\\tF1\")\n",
    "for n, recall, f1 in results:\n",
    "    print(f\"{n}\\t{recall:.4f}\\t{f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise is added in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Transformer Autoencoder with separate feature handling\n",
    "class TabularTransformerAE(nn.Module):\n",
    "    def __init__(self, feature_info, embed_dim=8, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.feature_info = feature_info\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Feature processing modules\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        self.proj_binary = nn.Linear(1, embed_dim)\n",
    "        self.proj_numerical = nn.Linear(1, embed_dim)\n",
    "        \n",
    "        # Create embeddings for categorical features\n",
    "        for i, (ftype, params) in enumerate(feature_info):\n",
    "            if ftype == 'categorical':\n",
    "                self.embeddings[f'emb_{i}'] = nn.Embedding(params['num_classes'], embed_dim)\n",
    "                \n",
    "        # Transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=64),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Latent projection\n",
    "        self.num_features = len(feature_info)\n",
    "        self.latent_proj = nn.Linear(self.num_features * embed_dim, latent_dim)\n",
    "        \n",
    "        # Decoder components\n",
    "        self.decoder_input = nn.Linear(latent_dim, self.num_features * embed_dim)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=64),\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.heads = nn.ModuleList()\n",
    "        for ftype, params in feature_info:\n",
    "            if ftype == 'categorical':\n",
    "                self.heads.append(nn.Linear(embed_dim, params['num_classes']))\n",
    "            else:\n",
    "                self.heads.append(nn.Linear(embed_dim, 1))\n",
    "\n",
    "    def encode(self, x):\n",
    "        embeddings = []\n",
    "        for i, (ftype, params) in enumerate(self.feature_info):\n",
    "            feature = x[:, i].unsqueeze(1)\n",
    "            if ftype == 'categorical':\n",
    "                emb = self.embeddings[f'emb_{i}'](feature.long().squeeze())\n",
    "            elif ftype == 'binary':\n",
    "                emb = self.proj_binary(feature)\n",
    "            else:\n",
    "                emb = self.proj_numerical(feature)\n",
    "            embeddings.append(emb.unsqueeze(0))\n",
    "        \n",
    "        embeddings = torch.cat(embeddings)\n",
    "        encoded = self.transformer_encoder(embeddings)\n",
    "        latent = self.latent_proj(encoded.permute(1, 0, 2).flatten(1))\n",
    "        return latent\n",
    "\n",
    "    def decode(self, latent):\n",
    "        batch_size = latent.size(0)\n",
    "        x = self.decoder_input(latent)\n",
    "        x = x.view(batch_size, self.num_features, self.embed_dim).permute(1, 0, 2)\n",
    "        decoded = self.transformer_decoder(x, x)\n",
    "        \n",
    "        outputs = []\n",
    "        for i, head in enumerate(self.heads):\n",
    "            outputs.append(head(decoded[i]))\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        return self.decode(latent)\n",
    "\n",
    "# Data preparation and preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Identify feature types\n",
    "    feature_info = []\n",
    "    label_encoders = {}\n",
    "    scalers = {}\n",
    "    \n",
    "    categorical = []\n",
    "    binary = []\n",
    "    numerical = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique = df[col].nunique()\n",
    "        if unique > 2:\n",
    "            categorical.append(col)\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "            label_encoders[col] = le\n",
    "            feature_info.append(('categorical', {'num_classes': unique}))\n",
    "        elif unique == 2:\n",
    "            binary.append(col)\n",
    "            df[col] = df[col].astype(int)\n",
    "            feature_info.append(('binary', None))\n",
    "        else:\n",
    "            numerical.append(col)\n",
    "            feature_info.append(('numerical', None))\n",
    "    \n",
    "    # Scale numerical features\n",
    "    if numerical:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[numerical] = scaler.fit_transform(df[numerical])\n",
    "        scalers['numerical'] = scaler\n",
    "    \n",
    "    return df, feature_info, label_encoders, scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add Gaussian noise\n",
    "def add_gaussian_noise(data, epsilon, delta, sensitivity):\n",
    "    sigma = (sensitivity / epsilon) * np.sqrt(2 * np.log(1.25 / delta))\n",
    "    noise = np.random.normal(0, sigma, size=data.shape)\n",
    "    return data + noise\n",
    "\n",
    "# Parameters for differential privacy\n",
    "epsilon = 1  # Privacy budget\n",
    "delta = 1e-5   \n",
    "sensitivity = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for dataset: reduced_diabetes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multiplier=0.25\n",
      "Class distribution after augmentation: {0: 500, 1: 75}\n",
      "Recall score (generated data with DP, n=0.25): 0.8824\n",
      "F1 score (generated data with DP, n=0.25): 0.9375\n",
      "Results for multiplier=0.5\n",
      "Class distribution after augmentation: {0: 500, 1: 90}\n",
      "Recall score (generated data with DP, n=0.5): 0.9412\n",
      "F1 score (generated data with DP, n=0.5): 0.9697\n",
      "Results for multiplier=0.75\n",
      "Class distribution after augmentation: {0: 500, 1: 105}\n",
      "Recall score (generated data with DP, n=0.75): 0.8824\n",
      "F1 score (generated data with DP, n=0.75): 0.9375\n",
      "Results for multiplier=1\n",
      "Class distribution after augmentation: {0: 500, 1: 120}\n",
      "Recall score (generated data with DP, n=1): 0.9412\n",
      "F1 score (generated data with DP, n=1): 0.9412\n",
      "Results for multiplier=1.25\n",
      "Class distribution after augmentation: {0: 500, 1: 135}\n",
      "Recall score (generated data with DP, n=1.25): 0.9412\n",
      "F1 score (generated data with DP, n=1.25): 0.9697\n",
      "Results for multiplier=1.5\n",
      "Class distribution after augmentation: {0: 500, 1: 150}\n",
      "Recall score (generated data with DP, n=1.5): 0.9412\n",
      "F1 score (generated data with DP, n=1.5): 0.9697\n",
      "Results for multiplier=1.75\n",
      "Class distribution after augmentation: {0: 500, 1: 165}\n",
      "Recall score (generated data with DP, n=1.75): 1.0000\n",
      "F1 score (generated data with DP, n=1.75): 0.9714\n",
      "Results for multiplier=2\n",
      "Class distribution after augmentation: {0: 500, 1: 180}\n",
      "Recall score (generated data with DP, n=2): 1.0000\n",
      "F1 score (generated data with DP, n=2): 0.9444\n",
      "Results for multiplier=2.25\n",
      "Class distribution after augmentation: {0: 500, 1: 195}\n",
      "Recall score (generated data with DP, n=2.25): 1.0000\n",
      "F1 score (generated data with DP, n=2.25): 0.9714\n",
      "Results for multiplier=2.5\n",
      "Class distribution after augmentation: {0: 500, 1: 210}\n",
      "Recall score (generated data with DP, n=2.5): 1.0000\n",
      "F1 score (generated data with DP, n=2.5): 0.9714\n",
      "Results for multiplier=2.75\n",
      "Class distribution after augmentation: {0: 500, 1: 225}\n",
      "Recall score (generated data with DP, n=2.75): 1.0000\n",
      "F1 score (generated data with DP, n=2.75): 0.9444\n",
      "Results for multiplier=3\n",
      "Class distribution after augmentation: {0: 500, 1: 240}\n",
      "Recall score (generated data with DP, n=3): 1.0000\n",
      "F1 score (generated data with DP, n=3): 0.9444\n",
      "\n",
      "Summary of Results:\n",
      "n\tRecall\tF1\n",
      "0.25\t0.8824\t0.9375\n",
      "0.5\t0.9412\t0.9697\n",
      "0.75\t0.8824\t0.9375\n",
      "1\t0.9412\t0.9412\n",
      "1.25\t0.9412\t0.9697\n",
      "1.5\t0.9412\t0.9697\n",
      "1.75\t1.0000\t0.9714\n",
      "2\t1.0000\t0.9444\n",
      "2.25\t1.0000\t0.9714\n",
      "2.5\t1.0000\t0.9714\n",
      "2.75\t1.0000\t0.9444\n",
      "3\t1.0000\t0.9444\n",
      "Running for dataset: reduced_cardio_train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multiplier=0.25\n",
      "Class distribution after augmentation: {0: 35021, 1: 5000}\n",
      "Recall score (generated data with DP, n=0.25): 0.7681\n",
      "F1 score (generated data with DP, n=0.25): 0.8562\n",
      "Results for multiplier=0.5\n",
      "Class distribution after augmentation: {0: 35021, 1: 6000}\n",
      "Recall score (generated data with DP, n=0.5): 0.7765\n",
      "F1 score (generated data with DP, n=0.5): 0.8583\n",
      "Results for multiplier=0.75\n",
      "Class distribution after augmentation: {0: 35021, 1: 7000}\n",
      "Recall score (generated data with DP, n=0.75): 0.7874\n",
      "F1 score (generated data with DP, n=0.75): 0.8624\n",
      "Results for multiplier=1\n",
      "Class distribution after augmentation: {0: 35021, 1: 8000}\n",
      "Recall score (generated data with DP, n=1): 0.7882\n",
      "F1 score (generated data with DP, n=1): 0.8606\n",
      "Results for multiplier=1.25\n",
      "Class distribution after augmentation: {0: 35021, 1: 9000}\n",
      "Recall score (generated data with DP, n=1.25): 0.7748\n",
      "F1 score (generated data with DP, n=1.25): 0.8529\n",
      "Results for multiplier=1.5\n",
      "Class distribution after augmentation: {0: 35021, 1: 10000}\n",
      "Recall score (generated data with DP, n=1.5): 0.7714\n",
      "F1 score (generated data with DP, n=1.5): 0.8488\n",
      "Results for multiplier=1.75\n",
      "Class distribution after augmentation: {0: 35021, 1: 11000}\n",
      "Recall score (generated data with DP, n=1.75): 0.7849\n",
      "F1 score (generated data with DP, n=1.75): 0.8596\n",
      "Results for multiplier=2\n",
      "Class distribution after augmentation: {0: 35021, 1: 12000}\n",
      "Recall score (generated data with DP, n=2): 0.7807\n",
      "F1 score (generated data with DP, n=2): 0.8535\n",
      "Results for multiplier=2.25\n",
      "Class distribution after augmentation: {0: 35021, 1: 13000}\n",
      "Recall score (generated data with DP, n=2.25): 0.7891\n",
      "F1 score (generated data with DP, n=2.25): 0.8583\n",
      "Results for multiplier=2.5\n",
      "Class distribution after augmentation: {0: 35021, 1: 14000}\n",
      "Recall score (generated data with DP, n=2.5): 0.7874\n",
      "F1 score (generated data with DP, n=2.5): 0.8545\n",
      "Results for multiplier=2.75\n",
      "Class distribution after augmentation: {0: 35021, 1: 15000}\n",
      "Recall score (generated data with DP, n=2.75): 0.7933\n",
      "F1 score (generated data with DP, n=2.75): 0.8597\n",
      "Results for multiplier=3\n",
      "Class distribution after augmentation: {0: 35021, 1: 16000}\n",
      "Recall score (generated data with DP, n=3): 0.7882\n",
      "F1 score (generated data with DP, n=3): 0.8566\n",
      "\n",
      "Summary of Results:\n",
      "n\tRecall\tF1\n",
      "0.25\t0.7681\t0.8562\n",
      "0.5\t0.7765\t0.8583\n",
      "0.75\t0.7874\t0.8624\n",
      "1\t0.7882\t0.8606\n",
      "1.25\t0.7748\t0.8529\n",
      "1.5\t0.7714\t0.8488\n",
      "1.75\t0.7849\t0.8596\n",
      "2\t0.7807\t0.8535\n",
      "2.25\t0.7891\t0.8583\n",
      "2.5\t0.7874\t0.8545\n",
      "2.75\t0.7933\t0.8597\n",
      "3\t0.7882\t0.8566\n",
      "Running for dataset: reduced_smart_grid_stability\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multiplier=0.25\n",
      "Class distribution after augmentation: {0: 6380, 1: 1250}\n",
      "Recall score (generated data with DP, n=0.25): 0.8344\n",
      "F1 score (generated data with DP, n=0.25): 0.9097\n",
      "Results for multiplier=0.5\n",
      "Class distribution after augmentation: {0: 6380, 1: 1500}\n",
      "Recall score (generated data with DP, n=0.5): 0.8742\n",
      "F1 score (generated data with DP, n=0.5): 0.9279\n",
      "Results for multiplier=0.75\n",
      "Class distribution after augmentation: {0: 6380, 1: 1750}\n",
      "Recall score (generated data with DP, n=0.75): 0.8808\n",
      "F1 score (generated data with DP, n=0.75): 0.9333\n",
      "Results for multiplier=1\n",
      "Class distribution after augmentation: {0: 6380, 1: 2000}\n",
      "Recall score (generated data with DP, n=1): 0.9007\n",
      "F1 score (generated data with DP, n=1): 0.9379\n",
      "Results for multiplier=1.25\n",
      "Class distribution after augmentation: {0: 6380, 1: 2250}\n",
      "Recall score (generated data with DP, n=1.25): 0.9007\n",
      "F1 score (generated data with DP, n=1.25): 0.9396\n",
      "Results for multiplier=1.5\n",
      "Class distribution after augmentation: {0: 6380, 1: 2500}\n",
      "Recall score (generated data with DP, n=1.5): 0.9172\n",
      "F1 score (generated data with DP, n=1.5): 0.9470\n",
      "Results for multiplier=1.75\n",
      "Class distribution after augmentation: {0: 6380, 1: 2750}\n",
      "Recall score (generated data with DP, n=1.75): 0.9437\n",
      "F1 score (generated data with DP, n=1.75): 0.9628\n",
      "Results for multiplier=2\n",
      "Class distribution after augmentation: {0: 6380, 1: 3000}\n",
      "Recall score (generated data with DP, n=2): 0.9437\n",
      "F1 score (generated data with DP, n=2): 0.9628\n",
      "Results for multiplier=2.25\n",
      "Class distribution after augmentation: {0: 6380, 1: 3250}\n",
      "Recall score (generated data with DP, n=2.25): 0.9570\n",
      "F1 score (generated data with DP, n=2.25): 0.9698\n",
      "Results for multiplier=2.5\n",
      "Class distribution after augmentation: {0: 6380, 1: 3500}\n",
      "Recall score (generated data with DP, n=2.5): 0.9735\n",
      "F1 score (generated data with DP, n=2.5): 0.9767\n",
      "Results for multiplier=2.75\n",
      "Class distribution after augmentation: {0: 6380, 1: 3750}\n",
      "Recall score (generated data with DP, n=2.75): 0.9669\n",
      "F1 score (generated data with DP, n=2.75): 0.9701\n",
      "Results for multiplier=3\n",
      "Class distribution after augmentation: {0: 6380, 1: 4000}\n",
      "Recall score (generated data with DP, n=3): 0.9801\n",
      "F1 score (generated data with DP, n=3): 0.9737\n",
      "\n",
      "Summary of Results:\n",
      "n\tRecall\tF1\n",
      "0.25\t0.8344\t0.9097\n",
      "0.5\t0.8742\t0.9279\n",
      "0.75\t0.8808\t0.9333\n",
      "1\t0.9007\t0.9379\n",
      "1.25\t0.9007\t0.9396\n",
      "1.5\t0.9172\t0.9470\n",
      "1.75\t0.9437\t0.9628\n",
      "2\t0.9437\t0.9628\n",
      "2.25\t0.9570\t0.9698\n",
      "2.5\t0.9735\t0.9767\n",
      "2.75\t0.9669\t0.9701\n",
      "3\t0.9801\t0.9737\n",
      "Running for dataset: mammography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multiplier=0.25\n",
      "Class distribution after augmentation: {-1: 10923, 1: 325}\n",
      "Recall score (generated data with DP, n=0.25): 0.8514\n",
      "F1 score (generated data with DP, n=0.25): 0.9065\n",
      "Results for multiplier=0.5\n",
      "Class distribution after augmentation: {-1: 10923, 1: 390}\n",
      "Recall score (generated data with DP, n=0.5): 0.8649\n",
      "F1 score (generated data with DP, n=0.5): 0.9078\n",
      "Results for multiplier=0.75\n",
      "Class distribution after augmentation: {-1: 10923, 1: 455}\n",
      "Recall score (generated data with DP, n=0.75): 0.8514\n",
      "F1 score (generated data with DP, n=0.75): 0.8936\n",
      "Results for multiplier=1\n",
      "Class distribution after augmentation: {-1: 10923, 1: 520}\n",
      "Recall score (generated data with DP, n=1): 0.8378\n",
      "F1 score (generated data with DP, n=1): 0.8986\n",
      "Results for multiplier=1.25\n",
      "Class distribution after augmentation: {-1: 10923, 1: 585}\n",
      "Recall score (generated data with DP, n=1.25): 0.9189\n",
      "F1 score (generated data with DP, n=1.25): 0.9315\n",
      "Results for multiplier=1.5\n",
      "Class distribution after augmentation: {-1: 10923, 1: 650}\n",
      "Recall score (generated data with DP, n=1.5): 0.9189\n",
      "F1 score (generated data with DP, n=1.5): 0.9315\n",
      "Results for multiplier=1.75\n",
      "Class distribution after augmentation: {-1: 10923, 1: 715}\n",
      "Recall score (generated data with DP, n=1.75): 0.9189\n",
      "F1 score (generated data with DP, n=1.75): 0.9315\n",
      "Results for multiplier=2\n",
      "Class distribution after augmentation: {-1: 10923, 1: 780}\n",
      "Recall score (generated data with DP, n=2): 0.9189\n",
      "F1 score (generated data with DP, n=2): 0.9315\n",
      "Results for multiplier=2.25\n",
      "Class distribution after augmentation: {-1: 10923, 1: 845}\n",
      "Recall score (generated data with DP, n=2.25): 0.9189\n",
      "F1 score (generated data with DP, n=2.25): 0.9128\n",
      "Results for multiplier=2.5\n",
      "Class distribution after augmentation: {-1: 10923, 1: 910}\n",
      "Recall score (generated data with DP, n=2.5): 0.9189\n",
      "F1 score (generated data with DP, n=2.5): 0.9189\n",
      "Results for multiplier=2.75\n",
      "Class distribution after augmentation: {-1: 10923, 1: 975}\n",
      "Recall score (generated data with DP, n=2.75): 0.9189\n",
      "F1 score (generated data with DP, n=2.75): 0.9252\n",
      "Results for multiplier=3\n",
      "Class distribution after augmentation: {-1: 10923, 1: 1040}\n",
      "Recall score (generated data with DP, n=3): 0.9459\n",
      "F1 score (generated data with DP, n=3): 0.9333\n",
      "\n",
      "Summary of Results:\n",
      "n\tRecall\tF1\n",
      "0.25\t0.8514\t0.9065\n",
      "0.5\t0.8649\t0.9078\n",
      "0.75\t0.8514\t0.8936\n",
      "1\t0.8378\t0.8986\n",
      "1.25\t0.9189\t0.9315\n",
      "1.5\t0.9189\t0.9315\n",
      "1.75\t0.9189\t0.9315\n",
      "2\t0.9189\t0.9315\n",
      "2.25\t0.9189\t0.9128\n",
      "2.5\t0.9189\t0.9189\n",
      "2.75\t0.9189\t0.9252\n",
      "3\t0.9459\t0.9333\n",
      "Running for dataset: oil\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for multiplier=0.25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 114\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m             vals \u001b[38;5;241m=\u001b[39m synthetic_tensor[:, start_idx]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 114\u001b[0m             synthetic_df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mscalers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumerical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m         start_idx \u001b[38;5;241m=\u001b[39m end_idx\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Rest of the evaluation pipeline (same as original)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:581\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    571\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    573\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    574\u001b[0m     X,\n\u001b[0;32m    575\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    578\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    579\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[0;32m    582\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (10,1) doesn't match the broadcast shape (10,2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "# Main execution\n",
    "dataset_list = ['reduced_diabetes','reduced_cardio_train','reduced_smart_grid_stability','mammography','oil','coil_2000']\n",
    "for dataset in dataset_list:\n",
    "    print(f\"Running for dataset: {dataset}\")\n",
    "    file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Preprocess entire dataset\n",
    "    X_orig = data.iloc[:, :-1]\n",
    "    y_orig = data.iloc[:, -1]\n",
    "\n",
    "    # Process minority class\n",
    "    real_minority = data[y_orig == 1]\n",
    "    X_minority = real_minority.iloc[:, :-1]\n",
    "    y_minority = real_minority.iloc[:, -1]\n",
    "\n",
    "    # Preprocess minority data\n",
    "    X_processed, feature_info, label_encoders, scalers = preprocess_data(X_minority.copy())\n",
    "    input_dim = X_processed.shape[1]\n",
    "    # Initialize model\n",
    "    latent_dim = 4\n",
    "    num_epochs = 1000\n",
    "    batch_size = 32\n",
    "    model = TabularTransformerAE(feature_info, latent_dim=latent_dim)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert data to tensor\n",
    "    X_tensor = torch.tensor(X_processed.values, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            reconstructed = model(batch)\n",
    "            loss = 0\n",
    "            \n",
    "            # Calculate per-feature loss\n",
    "            start_idx = 0\n",
    "            for j, (ftype, params) in enumerate(feature_info):\n",
    "                if ftype == 'categorical':\n",
    "                    end_idx = start_idx + params['num_classes']\n",
    "                    loss += F.cross_entropy(reconstructed[:, start_idx:end_idx], \n",
    "                                            batch[:, j].long())\n",
    "                    start_idx = end_idx\n",
    "                else:\n",
    "                    end_idx = start_idx + 1\n",
    "                    if ftype == 'binary':\n",
    "                        loss += F.binary_cross_entropy_with_logits(\n",
    "                            reconstructed[:, start_idx], batch[:, j])\n",
    "                    else:\n",
    "                        loss += F.mse_loss(reconstructed[:, start_idx], batch[:, j])\n",
    "                    start_idx = end_idx\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Generate latent space\n",
    "    with torch.no_grad():\n",
    "        latent = model.encode(X_tensor).numpy()\n",
    "\n",
    "    # Apply Forest Diffusion\n",
    "    forest_model = ForestDiffusionModel(\n",
    "        X=latent,\n",
    "        n_t=50,\n",
    "        duplicate_K=100,\n",
    "        diffusion_type='flow',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Training parameters\n",
    "    multiplier_list = [0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3]\n",
    "    # multiplier_list = [0.25]\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for multiplier in multiplier_list:\n",
    "        print(f\"Results for multiplier={multiplier}\")\n",
    "        \n",
    "        # Generate synthetic latent samples\n",
    "        synthetic_latent = forest_model.generate(batch_size=round(multiplier * len(X_tensor)))\n",
    "        # Add Gaussian noise to synthetic data for differential privacy\n",
    "        synthetic_latent = add_gaussian_noise(synthetic_latent, epsilon, delta, sensitivity)\n",
    "        # Decode samples\n",
    "        with torch.no_grad():\n",
    "            synthetic_tensor = model.decode(torch.tensor(synthetic_latent, dtype=torch.float32))\n",
    "        \n",
    "        # Convert to original feature space\n",
    "        synthetic_df = pd.DataFrame()\n",
    "        start_idx = 0\n",
    "        for j, col in enumerate(X_minority.columns):\n",
    "            ftype, params = feature_info[j]\n",
    "            \n",
    "            if ftype == 'categorical':\n",
    "                end_idx = start_idx + params['num_classes']\n",
    "                probs = F.softmax(synthetic_tensor[:, start_idx:end_idx], dim=1)\n",
    "                preds = torch.argmax(probs, dim=1).numpy()\n",
    "                synthetic_df[col] = label_encoders[col].inverse_transform(preds)\n",
    "                start_idx = end_idx\n",
    "            else:\n",
    "                end_idx = start_idx + 1\n",
    "                if ftype == 'binary':\n",
    "                    preds = (torch.sigmoid(synthetic_tensor[:, start_idx]) > 0.5).numpy().astype(int)\n",
    "                    synthetic_df[col] = preds\n",
    "                else:\n",
    "                    vals = synthetic_tensor[:, start_idx].numpy()\n",
    "                    synthetic_df[col] = scalers['numerical'].inverse_transform(vals.reshape(-1, 1))\n",
    "                start_idx = end_idx\n",
    "        \n",
    "        # Rest of the evaluation pipeline (same as original)\n",
    "        synthetic_df[data.columns[-1]] = 1\n",
    "\n",
    "        augmented_dataset = pd.concat([data, synthetic_df], ignore_index=True)\n",
    "        \n",
    "        y_balanced = augmented_dataset.iloc[:, -1] \n",
    "        X_balanced= augmented_dataset.iloc[:, :-1]  \n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_orig, y_orig, test_size=0.3, random_state=42)\n",
    "        X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "        # Class distribution after augmentation\n",
    "        unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "        class_dist_after = dict(zip(unique, counts))\n",
    "        print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "        # Step 7: Train a simple classifier on both original and generated datasets\n",
    "        # Train-test split for augmented data\n",
    "        X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)\n",
    "\n",
    "        # Train a Random Forest on the augmented data\n",
    "        clf_bal = RandomForestClassifier(random_state=42)\n",
    "        clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "        # Predict and calculate recall and F1 score on augmented data\n",
    "        y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "        recall_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "        f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "        print(f\"Recall score (generated data with DP, n={multiplier}): {recall_bal:.4f}\")\n",
    "        print(f\"F1 score (generated data with DP, n={multiplier}): {f1_bal:.4f}\")\n",
    "\n",
    "        # Store results\n",
    "        results.append((multiplier, recall_bal, f1_bal))\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(\"n\\tRecall\\tF1\")\n",
    "    for multiplier, recall, f1 in results:\n",
    "        print(f\"{multiplier}\\t{recall:.4f}\\t{f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21914</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22113</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>93.0</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17668</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>71.0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40016</th>\n",
       "      <td>22515</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>56.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40017</th>\n",
       "      <td>19686</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>65.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40018</th>\n",
       "      <td>19104</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "      <td>56.0</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40019</th>\n",
       "      <td>17319</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>56.0</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40020</th>\n",
       "      <td>21292</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>56.0</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40021 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0      18393       2     168    62.0    110     80            1     1      0   \n",
       "1      17474       1     156    56.0    100     60            1     1      0   \n",
       "2      21914       1     151    67.0    120     80            2     2      0   \n",
       "3      22113       1     157    93.0    130     80            3     1      0   \n",
       "4      17668       1     158    71.0    110     70            1     1      0   \n",
       "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
       "40016  22515       1     165    56.0    130     90            1     1      0   \n",
       "40017  19686       1     166    65.0    120     80            2     2      0   \n",
       "40018  19104       1     159    56.0    120     90            1     1      0   \n",
       "40019  17319       1     165    56.0    120     90            1     1      0   \n",
       "40020  21292       1     150    56.0    120     90            1     1      0   \n",
       "\n",
       "       alco  active  cardio  Outcome  \n",
       "0         0       1       1      NaN  \n",
       "1         0       0       1      NaN  \n",
       "2         0       0       1      NaN  \n",
       "3         0       1       1      NaN  \n",
       "4         0       1       1      NaN  \n",
       "...     ...     ...     ...      ...  \n",
       "40016     0       0       1      1.0  \n",
       "40017     0       1       1      1.0  \n",
       "40018     0       0       1      1.0  \n",
       "40019     0       1       1      1.0  \n",
       "40020     0       1       1      1.0  \n",
       "\n",
       "[40021 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_dataset[data.columns[-1]] = 1\n",
    "augmented_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18393</td>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17474</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21914</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>67.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22113</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>93.0</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17668</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>71.0</td>\n",
       "      <td>110</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39016</th>\n",
       "      <td>20962</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>83.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39017</th>\n",
       "      <td>21151</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>69.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39018</th>\n",
       "      <td>17500</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "      <td>110.0</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39019</th>\n",
       "      <td>21074</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>80.0</td>\n",
       "      <td>150</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39020</th>\n",
       "      <td>22601</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39021 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0      18393       2     168    62.0    110     80            1     1      0   \n",
       "1      17474       1     156    56.0    100     60            1     1      0   \n",
       "2      21914       1     151    67.0    120     80            2     2      0   \n",
       "3      22113       1     157    93.0    130     80            3     1      0   \n",
       "4      17668       1     158    71.0    110     70            1     1      0   \n",
       "...      ...     ...     ...     ...    ...    ...          ...   ...    ...   \n",
       "39016  20962       2     174    83.0    120     80            3     1      0   \n",
       "39017  21151       1     178    69.0    130     90            1     1      0   \n",
       "39018  17500       2     182   110.0    130     90            2     2      0   \n",
       "39019  21074       1     165    80.0    150     80            1     1      0   \n",
       "39020  22601       1     158   126.0    140     90            2     2      0   \n",
       "\n",
       "       alco  active  cardio  \n",
       "0         0       1       0  \n",
       "1         0       0       0  \n",
       "2         0       0       0  \n",
       "3         0       1       0  \n",
       "4         0       1       0  \n",
       "...     ...     ...     ...  \n",
       "39016     0       1       1  \n",
       "39017     0       1       1  \n",
       "39018     0       1       1  \n",
       "39019     0       1       1  \n",
       "39020     0       1       1  \n",
       "\n",
       "[39021 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
