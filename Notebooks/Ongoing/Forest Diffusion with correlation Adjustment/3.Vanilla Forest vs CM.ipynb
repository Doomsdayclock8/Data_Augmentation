{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 8 is out of bounds for axis 1 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Step 4: Upsample the minority class using ForestDiffusionModel\u001b[39;00m\n\u001b[0;32m     43\u001b[0m forest_model \u001b[38;5;241m=\u001b[39m ForestDiffusionModel(X_minority, label_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, duplicate_K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, bin_indexes\u001b[38;5;241m=\u001b[39m[], cat_indexes\u001b[38;5;241m=\u001b[39m[], int_indexes\u001b[38;5;241m=\u001b[39mint_indexes, diffusion_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m X_minority_fake \u001b[38;5;241m=\u001b[39m \u001b[43mforest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust the batch size to create a balanced dataset\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Add generated samples to the main imbalanced dataset\u001b[39;00m\n\u001b[0;32m     47\u001b[0m X_balanced \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((X, X_minority_fake), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\ForestDiffusion\\diffusion_with_trees_class.py:433\u001b[0m, in \u001b[0;36mForestDiffusionModel.generate\u001b[1;34m(self, batch_size, n_t, X_covs)\u001b[0m\n\u001b[0;32m    431\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale(solution)\n\u001b[0;32m    432\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_onehot_data(solution)\n\u001b[1;32m--> 433\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_extremes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# Concatenate y label if needed\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\ForestDiffusion\\diffusion_with_trees_class.py:313\u001b[0m, in \u001b[0;36mForestDiffusionModel.clip_extremes\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_indexes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_indexes:\n\u001b[1;32m--> 313\u001b[0m     X[:,i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    314\u001b[0m small \u001b[38;5;241m=\u001b[39m (X \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_min)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m    315\u001b[0m X \u001b[38;5;241m=\u001b[39m small\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_min \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msmall)\u001b[38;5;241m*\u001b[39mX\n",
      "\u001b[1;31mIndexError\u001b[0m: index 8 is out of bounds for axis 1 with size 8"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "dataset='diabetes'\n",
    "file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "\n",
    "# Step 2: Inspect the data and check for class imbalance\n",
    "# Assuming the last column is the label, and the rest are features\n",
    "data=pd.read_csv(file_path)\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "# # Step 3: Plot the original imbalanced data (first two features for visualization)\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', label='Original Data', s=1)\n",
    "# plt.title('Original Imbalanced Data')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.show()\n",
    "\n",
    "# Separate the minority class\n",
    "X_minority = X[y == 1]\n",
    "# Identify integer columns\n",
    "int_columns = data.select_dtypes(include=['int']).columns\n",
    "int_indexes = []\n",
    "for col in int_columns:\n",
    "    col_index = data.columns.get_loc(col)\n",
    "    int_indexes.append(col_index)\n",
    "# Step 4: Upsample the minority class using ForestDiffusionModel\n",
    "forest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=int_indexes, diffusion_type='flow', n_jobs=-1)\n",
    "X_minority_fake = forest_model.generate(batch_size=len(X)//50 )  # Adjust the batch size to create a balanced dataset\n",
    "\n",
    "# Add generated samples to the main imbalanced dataset\n",
    "X_balanced = np.concatenate((X, X_minority_fake), axis=0)\n",
    "y_balanced = np.concatenate((y, np.ones(X_minority_fake.shape[0])), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# # Step 5: Plot the generated data (first two features for visualization)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.scatter(X_balanced[:, 0], X_balanced[:, 1], c=y_balanced, cmap='viridis', label='Generated Data', s=1)\n",
    "# plt.title('Data After Generation')\n",
    "# plt.xlabel('Feature 1')\n",
    "# plt.ylabel('Feature 2')\n",
    "# plt.show()\n",
    "\n",
    "# Check and print the class distribution after augmentation\n",
    "unique_bal, counts_bal = np.unique(y_balanced, return_counts=True)\n",
    "class_dist_after = dict(zip(unique_bal, counts_bal))\n",
    "print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "# Step 10: Print the number of fake samples generated\n",
    "print(f\"Number of fake samples generated: {len(X_minority_fake)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "file_path = 'creditcard.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Inspect the data and check for class imbalance\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")\n",
    "\n",
    "# Step 3: Calculate the correlation matrix for the original data\n",
    "correlation_matrix = pd.DataFrame(X).corr()\n",
    "print(\"Correlation Matrix of Original Data:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Step 4: Separate the minority class\n",
    "X_minority = X[y == 1]  # Assuming class '1' is the minority\n",
    "\n",
    "# Step 5: Upsample the minority class using ForestDiffusionModel\n",
    "forest_model = ForestDiffusionModel(X_minority, label_y=None, n_t=50, duplicate_K=100, \n",
    "                                     bin_indexes=[], cat_indexes=[], int_indexes=[], \n",
    "                                     diffusion_type='flow', n_jobs=-1)\n",
    "\n",
    "# Generate synthetic data\n",
    "X_minority_fake = forest_model.generate(batch_size=len(X) // 50)  # Adjust batch size\n",
    "\n",
    "# Function to adjust the correlations of the generated data\n",
    "def adjust_correlation(X, target_corr_matrix):\n",
    "    L = np.linalg.cholesky(target_corr_matrix)\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    X_transformed = np.dot(X_centered, L.T)\n",
    "    return X_transformed + np.mean(X, axis=0)\n",
    "\n",
    "# Step 6: Adjust the synthetic data to match the original data's correlation matrix\n",
    "X_minority_fake_adjusted = adjust_correlation(X_minority_fake, correlation_matrix)\n",
    "\n",
    "# Step 7: Combine the original and adjusted synthetic data\n",
    "X_balanced = np.concatenate((X, X_minority_fake_adjusted), axis=0)\n",
    "y_balanced = np.concatenate((y, np.ones(X_minority_fake_adjusted.shape[0])), axis=0)\n",
    "\n",
    "# Check and print the class distribution after augmentation\n",
    "unique_bal, counts_bal = np.unique(y_balanced, return_counts=True)\n",
    "class_dist_after = dict(zip(unique_bal, counts_bal))\n",
    "print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "# Step 8: Split the dataset into training and test sets\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 9: Train a Random Forest classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier()\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier()\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 10: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recall_bal = recall_score(y_test_bal, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_bal, y_pred_bal)\n",
    "\n",
    "# Step 11: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recall_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "\n",
    "# Step 12: Print the number of fake samples generated\n",
    "print(f\"Number of fake samples generated: {len(X_minority_fake)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
