{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Define the path to your folder\n",
    "folder_path = f'..\\\\..\\\\..\\\\Models\\\\AutoDiffusion_Tuning'  \n",
    "\n",
    "# Add the folder to sys.path\n",
    "sys.path.append(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import process_edited as pce\n",
    "import process_GQ as pce\n",
    "import autoencoder as aeb\n",
    "import diffusion as diff\n",
    "import TabDDPMdiff as TabDiff\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\..\\Datasets\\Original Data\\diabetes.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strings_set = {'abalone', 'adult', 'Churn_Modelling','faults', 'indian_liver_patient', \n",
    "#                'insurance', 'News', 'Obesity', 'Shoppers', 'Titanic', 'wilt', 'Bean', 'nursery', 'Magic', 'HTRU'}\n",
    "Model = 'AutoDiff'\n",
    "\n",
    "dataset='diabetes'\n",
    "file_path = f'..\\\\..\\\\..\\\\Datasets\\\\Original Data\\\\{dataset}.csv'\n",
    "# Read dataframe\n",
    "print(file_path)\n",
    "real_df = pd.read_csv(file_path)\n",
    "# print(real_df.head())\n",
    "#real_df = real_df.drop('url', axis=1)\n",
    "# # Step 2: Inspect the data and check for class imbalance\n",
    "# # Assuming the last column is the label, and the rest are features\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "#  # Separate the minority class\n",
    "real_minortiy = real_df[y == 1]\n",
    "real_minortiy.shape\n",
    "real_minortiy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84252e146407499db20db2142d0af2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [DeapStack] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     13\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m real_minortiy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Full batch\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43maeb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_minortiy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m latent_features \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Thesis\\Data_Augmentation\\Notebooks\\Ongoing\\Tuned AutoDiff+Forest\\..\\..\\..\\Models\\AutoDiffusion_Tuning\\autoencoder.py:131\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(df, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\u001b[0m\n\u001b[0;32m    128\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(all_indices, batch_size)\n\u001b[0;32m    129\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m data[batch_indices, :]\n\u001b[1;32m--> 131\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mDS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m l2_loss \u001b[38;5;241m=\u001b[39m auto_loss(batch_data, output, n_bins, n_nums, n_cats, cards)\n\u001b[0;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Python\\Environments\\ThesisEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:246\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Module [DeapStack] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "threshold = 0.01 # Threshold for mixed-type variables\n",
    "parser = pce.DataFrameParser().fit(real_minortiy, threshold)\n",
    "################################################################################################################\n",
    "# Auto-encoder hyper-parameters\n",
    "device = 'cuda' #@param {'type':'string'}\n",
    "n_epochs = 2000 #@param {'type':'integer'}\n",
    "eps = 1e-5 #@param {type:\"number\"}\n",
    "weight_decay = 1e-6 #@param {'type':'number'}\n",
    "maximum_learning_rate = 1e-2 #@param {'type':'number'}\n",
    "lr = 2e-4 #@param {'type':'number'}\n",
    "hidden_size = 250\n",
    "num_layers = 3\n",
    "batch_size = real_minortiy.shape[0] # Full batch\n",
    "\n",
    "ds = aeb.train_autoencoder(real_minortiy, hidden_size, num_layers, lr, weight_decay, n_epochs, batch_size, threshold)\n",
    "latent_features = ds[1].detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ForestDiffusion import ForestDiffusionModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to NumPy array\n",
    "array = latent_features.detach().cpu().numpy()\n",
    "array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1=pd.DataFrame(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = ForestDiffusionModel(array, label_y=None, n_t=50, duplicate_K=100, bin_indexes=[], cat_indexes=[], int_indexes=[], diffusion_type='flow', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([268, 13])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minority_fake = forest_model.generate(batch_size=len(real_minortiy))  # Adjust the batch size to create a balanced dataset\n",
    "sample=torch.tensor(minority_fake, dtype=torch.float32)\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output = ds[0](sample, ds[2], ds[3])\n",
    "gen_df = pce.convert_to_table(real_minortiy, gen_output, threshold)\n",
    "\n",
    "# output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Synthetic Data\\\\'\n",
    "# filename = f'{Model}+Forest_{dataset}_Synthetic.csv'\n",
    "# output_file = os.path.join(output_directory, filename)\n",
    "# gen_df.to_csv(output_file, index=False) \n",
    "\n",
    "\n",
    "# Select a random sample of the generated data\n",
    "selected_samples = gen_df.sample(n=min(100,gen_df.shape[0]), random_state=42)  # For reproducibility\n",
    "# Syn _df will be the dataset after augmentation\n",
    "syn_df = pd.concat([real_df, selected_samples], ignore_index=True)\n",
    "\n",
    "\n",
    "# augmented_output_directory =  f'..\\\\..\\\\..\\\\Datasets\\\\Augmented Data\\\\'\n",
    "# filename = f'{Model}+Forest_{dataset}_Augmented.csv'\n",
    "# augmented_output_file = os.path.join(augmented_output_directory, filename)\n",
    "# syn_df.to_csv(augmented_output_file, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(868, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Class distribution after augmentation: {0.0: 500, 1.0: 368}\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8250\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7674\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       151\n",
      "           1       0.72      0.82      0.77        80\n",
      "\n",
      "    accuracy                           0.83       231\n",
      "   macro avg       0.81      0.83      0.81       231\n",
      "weighted avg       0.84      0.83      0.83       231\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "# real_df = pd.read_csv(filename)\n",
    "# syn_filename = f'{string}/{Model}_{string}_Augmented.csv'\n",
    "\n",
    "# augmented_df = pd.read_csv(syn_filename)\n",
    "augmented_df=syn_df\n",
    "\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values \n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Check and print the Augmented class distribution\n",
    "unique, counts = np.unique(y_balanced, return_counts=True)\n",
    "class_dist_after = dict(zip(unique, counts))\n",
    "print(f\"Class distribution after augmentation: {class_dist_after}\")\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "metrics = {\n",
    "    \"Dataset\": dataset,\n",
    "    \"Precision_Original\": prec_orig,\n",
    "    \"Precision_Generated\": prec_bal,\n",
    "    \"Recall_Original\": recall_orig,\n",
    "    \"Recall_Generated\": recalls_bal,\n",
    "    \"F1_Original\": f1_orig,   \n",
    "    \"F1_Generated\": f1_bal,\n",
    "    \"Num_Fake_Samples\": len(augmented_df) - len(real_df)\n",
    "    \"Synthetic/Original_Ratio\":100*(len(augmented_df) - len(real_df))/len(real_minortiy)\n",
    "}\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
