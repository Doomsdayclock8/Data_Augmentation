{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0089\n",
      "Epoch [200/1000], Loss: 0.0082\n",
      "Epoch [300/1000], Loss: 0.0070\n",
      "Epoch [400/1000], Loss: 0.0057\n",
      "Epoch [500/1000], Loss: 0.0054\n",
      "Epoch [600/1000], Loss: 0.0053\n",
      "Epoch [700/1000], Loss: 0.0052\n",
      "Epoch [800/1000], Loss: 0.0051\n",
      "Epoch [900/1000], Loss: 0.0051\n",
      "Epoch [1000/1000], Loss: 0.0050\n",
      "\n",
      "Generated Synthetic Data:\n",
      "   Pregnancies     Glucose  BloodPressure  SkinThickness    Insulin  \\\n",
      "0     2.693395  188.197388      81.042862      36.024353   2.040438   \n",
      "1     2.344492  143.005814      78.279419      41.532654  66.812958   \n",
      "2     5.678009  199.000000      35.411930      31.237715   0.000000   \n",
      "3     4.492698  159.829376      78.797722      20.205566   0.000000   \n",
      "4     1.718907  182.068207      55.836700      30.258989   0.027148   \n",
      "\n",
      "         BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "0  40.926815                  1.042608  30.895632      1.0  \n",
      "1  39.627724                  1.664703  38.121571      1.0  \n",
      "2  43.193768                  1.061494  21.000000      1.0  \n",
      "3  28.730577                  1.021036  37.502739      1.0  \n",
      "4  39.211742                  0.078000  21.000000      1.0  \n",
      "\n",
      "Original Data Ranges:\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "min            0        0              0              0        0   0.0   \n",
      "max           17      199            122             99      846  67.1   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "min                     0.078   21        0  \n",
      "max                     2.420   81        1  \n",
      "\n",
      "Synthetic Data Ranges:\n",
      "     Pregnancies    Glucose  BloodPressure  SkinThickness    Insulin  \\\n",
      "min     0.000000   90.95874       0.000000      10.036611    0.00000   \n",
      "max     8.090948  199.00000      89.454247      72.408936  599.69812   \n",
      "\n",
      "           BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "min  24.417139                   0.07800  21.000000      0.0  \n",
      "max  58.515030                   2.38169  52.297211      1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:116: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_36180\\3282560335.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention_output = torch.matmul(attention_weights, V)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "# Define the Autoencoder with Attention\n",
    "class AutoencoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Encoder with attention\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        self.attention = SelfAttention(hidden_dim)\n",
    "\n",
    "        # Separate decoders for features and outcome\n",
    "        self.decoder_features = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, input_dim - 1)\n",
    "        )\n",
    "        self.decoder_outcome = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode, apply attention, and decode\n",
    "        encoded = self.encoder(x)\n",
    "        attended = self.attention(encoded.unsqueeze(1)).squeeze(1)  # Add and remove batch dimension for attention\n",
    "        decoded_features = self.decoder_features(attended)\n",
    "        decoded_outcome = self.decoder_outcome(attended)\n",
    "        return torch.cat([decoded_features, decoded_outcome], dim=1)\n",
    "\n",
    "    def generate_synthetic_data(self, num_samples):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, self.encoder[-1].out_features, device=device)\n",
    "            attended = self.attention(z.unsqueeze(1)).squeeze(1)\n",
    "            features = self.decoder_features(attended)\n",
    "            outcome = self.decoder_outcome(attended)\n",
    "            synthetic = torch.cat([features, outcome], dim=1)\n",
    "            # Round outcome\n",
    "            synthetic[:, -1] = torch.round(synthetic[:, -1])\n",
    "        return synthetic.cpu().numpy()\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_autoencoder(data, input_dim, hidden_dim, epochs=20, batch_size=32, lr=0.001):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    data_tensor = torch.FloatTensor(scaled_data)\n",
    "\n",
    "    model = AutoencoderWithAttention(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i:i+batch_size]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(data_tensor):.4f}\")\n",
    "\n",
    "    return model, scaler, data_min, data_max\n",
    "\n",
    "\n",
    "# Scale synthetic data back to original range\n",
    "def scale_to_original_range(synthetic_data, original_min, original_max):\n",
    "    \"\"\"Scale synthetic data to match original data ranges\"\"\"\n",
    "    df_synthetic = pd.DataFrame(synthetic_data)\n",
    "    for col in df_synthetic.columns[:-1]:  # Skip outcome column\n",
    "        min_val = original_min[col]\n",
    "        max_val = original_max[col]\n",
    "        df_synthetic[col] = df_synthetic[col].clip(lower=min_val, upper=max_val)\n",
    "    # Ensure outcome is binary\n",
    "    df_synthetic.iloc[:, -1] = df_synthetic.iloc[:, -1].round().clip(0, 1)\n",
    "    return df_synthetic.values\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load sample tabular data\n",
    "    path = '../Datasets/diabetes.csv'\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    input_dim = df.shape[1]  # Get number of features\n",
    "    hidden_dim = 5  # Latent space dimension\n",
    "\n",
    "    # Train the autoencoder with attention\n",
    "    autoencoder, scaler, data_min, data_max = train_autoencoder(df, input_dim, hidden_dim, epochs=1000)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    num_synthetic_instances = 100\n",
    "    synthetic_data = autoencoder.generate_synthetic_data(num_synthetic_instances)\n",
    "\n",
    "    # Inverse transform the synthetic data\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "\n",
    "    # Scale to original ranges\n",
    "    synthetic_data = scale_to_original_range(synthetic_data, data_min, data_max)\n",
    "\n",
    "    # Convert synthetic data to dataframe\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    print(\"\\nGenerated Synthetic Data:\")\n",
    "    print(synthetic_df.head())\n",
    "\n",
    "    # Print ranges comparison\n",
    "    print(\"\\nOriginal Data Ranges:\")\n",
    "    print(df.agg(['min', 'max']))\n",
    "    print(\"\\nSynthetic Data Ranges:\")\n",
    "    print(synthetic_df.agg(['min', 'max']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv('diabetes_autoencoder_attention.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8000\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7805\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       151\n",
      "           1       0.76      0.80      0.78        80\n",
      "\n",
      "    accuracy                           0.84       231\n",
      "   macro avg       0.83      0.83      0.83       231\n",
      "weighted avg       0.85      0.84      0.85       231\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "real_df = df\n",
    "augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values \n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
