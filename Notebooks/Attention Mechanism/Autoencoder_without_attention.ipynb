{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0069\n",
      "Epoch [200/1000], Loss: 0.0065\n",
      "Epoch [300/1000], Loss: 0.0063\n",
      "Epoch [400/1000], Loss: 0.0063\n",
      "Epoch [500/1000], Loss: 0.0060\n",
      "Epoch [600/1000], Loss: 0.0057\n",
      "Epoch [700/1000], Loss: 0.0056\n",
      "Epoch [800/1000], Loss: 0.0056\n",
      "Epoch [900/1000], Loss: 0.0056\n",
      "Epoch [1000/1000], Loss: 0.0055\n",
      "\n",
      "Generated Synthetic Data:\n",
      "   Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "0     4.137497  129.704895      64.045044      24.056353   99.633102   \n",
      "1     6.095989  103.327438      67.251556      24.571205   63.307220   \n",
      "2     0.000000  171.529541      55.871586       0.000000  413.077667   \n",
      "3     0.806957  139.542511      47.475578       4.704753  264.075989   \n",
      "4     0.383999  122.763748      65.841385      22.157076   63.391430   \n",
      "\n",
      "         BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "0  33.584423                     0.078  31.330311      1.0  \n",
      "1  43.038231                     0.078  39.475033      0.0  \n",
      "2  29.430025                     0.078  26.267796      1.0  \n",
      "3  26.889769                     0.078  25.525965      0.0  \n",
      "4  38.817791                     0.078  21.765387      1.0  \n",
      "\n",
      "Original Data Ranges:\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "min            0        0              0              0        0   0.0   \n",
      "max           17      199            122             99      846  67.1   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "min                     0.078   21        0  \n",
      "max                     2.420   81        1  \n",
      "\n",
      "Synthetic Data Ranges:\n",
      "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "min     0.000000   99.512932      27.982727       0.000000    0.000000   \n",
      "max     8.136045  172.852295      78.934212      30.741945  439.491821   \n",
      "\n",
      "           BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "min  19.244970                     0.078  21.000000      0.0  \n",
      "max  45.598564                     0.078  42.491745      1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:75: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\887945308.py:76: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        )\n",
    "        # Separate decoders for features and outcome\n",
    "        self.decoder_features = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, input_dim-1)\n",
    "        )\n",
    "        self.decoder_outcome = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim*2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded_features = self.decoder_features(encoded)\n",
    "        decoded_outcome = self.decoder_outcome(encoded)\n",
    "        return torch.cat([decoded_features, decoded_outcome], dim=1)\n",
    "    \n",
    "    def generate_synthetic_data(self, num_samples):\n",
    "        device = next(self.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, self.encoder[-1].out_features, device=device)\n",
    "            features = self.decoder_features(z)\n",
    "            outcome = self.decoder_outcome(z)\n",
    "            synthetic = torch.cat([features, outcome], dim=1)\n",
    "            # Round outcome\n",
    "            synthetic[:, -1] = torch.round(synthetic[:, -1])\n",
    "        return synthetic.cpu().numpy()\n",
    "\n",
    "def train_autoencoder(data, input_dim, hidden_dim, epochs=20, batch_size=32, lr=0.001):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    data_tensor = torch.FloatTensor(scaled_data)\n",
    "    \n",
    "    model = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(data_tensor):.4f}\")\n",
    "    \n",
    "    return model, scaler, data_min, data_max\n",
    "\n",
    "def scale_to_original_range(synthetic_data, original_min, original_max):\n",
    "    \"\"\"Scale synthetic data to match original data ranges\"\"\"\n",
    "    df_synthetic = pd.DataFrame(synthetic_data)\n",
    "    for col in df_synthetic.columns[:-1]:  # Skip outcome column\n",
    "        min_val = original_min[col]\n",
    "        max_val = original_max[col]\n",
    "        df_synthetic[col] = df_synthetic[col].clip(lower=min_val, upper=max_val)\n",
    "    # Ensure outcome is binary\n",
    "    df_synthetic.iloc[:, -1] = df_synthetic.iloc[:, -1].round().clip(0, 1)\n",
    "    return df_synthetic.values\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load sample tabular data\n",
    "    path = '../Datasets/diabetes.csv'\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    input_dim = df.shape[1]  # Get number of features\n",
    "    hidden_dim = 5  # Latent space dimension\n",
    "\n",
    "    # Train the autoencoder with all return values\n",
    "    autoencoder, scaler, data_min, data_max = train_autoencoder(df, input_dim, hidden_dim, epochs=1000)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    num_synthetic_instances = 100\n",
    "    synthetic_data = autoencoder.generate_synthetic_data(num_synthetic_instances)\n",
    "    \n",
    "    # Inverse transform the synthetic data\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "    \n",
    "    # Scale to original ranges\n",
    "    synthetic_data = scale_to_original_range(synthetic_data, data_min, data_max)\n",
    "    \n",
    "    # Convert synthetic data to dataframe\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    print(\"\\nGenerated Synthetic Data:\")\n",
    "    print(synthetic_df.head())\n",
    "    \n",
    "    # Print ranges comparison\n",
    "    print(\"\\nOriginal Data Ranges:\")\n",
    "    print(df.agg(['min', 'max']))\n",
    "    print(\"\\nSynthetic Data Ranges:\")\n",
    "    print(synthetic_df.agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv('diabetes_autoencoder.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8375\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7976\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88       151\n",
      "           1       0.76      0.84      0.80        80\n",
      "\n",
      "    accuracy                           0.85       231\n",
      "   macro avg       0.84      0.85      0.84       231\n",
      "weighted avg       0.86      0.85      0.85       231\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "real_df = df\n",
    "augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values \n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0066\n",
      "Epoch [200/1000], Loss: 0.0058\n",
      "Epoch [300/1000], Loss: 0.0054\n",
      "Epoch [400/1000], Loss: 0.0052\n",
      "Epoch [500/1000], Loss: 0.0051\n",
      "Epoch [600/1000], Loss: 0.0051\n",
      "Epoch [700/1000], Loss: 0.0050\n",
      "Epoch [800/1000], Loss: 0.0050\n",
      "Epoch [900/1000], Loss: 0.0050\n",
      "Epoch [1000/1000], Loss: 0.0050\n",
      "\n",
      "Generated Synthetic Data:\n",
      "   Pregnancies     Glucose  BloodPressure  SkinThickness    Insulin  \\\n",
      "0          0.0  190.954681      71.938553      12.902526   9.399074   \n",
      "1          0.0  159.229172      68.740921      21.334652   6.453776   \n",
      "2          0.0  158.607544      76.263916       3.243194  14.481685   \n",
      "3          0.0  131.857071      80.641312       4.763150  13.026881   \n",
      "4          0.0  143.806610      82.653435       2.294254   9.106364   \n",
      "\n",
      "         BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "0  28.166277                  0.252532  21.000000      1.0  \n",
      "1  26.751492                  0.506460  21.509159      1.0  \n",
      "2  27.542755                  0.078000  21.000000      1.0  \n",
      "3  36.936371                  0.368456  21.000000      1.0  \n",
      "4  30.912041                  0.078000  21.000000      0.0  \n",
      "\n",
      "Original Data Ranges:\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "min            0        0              0              0        0   0.0   \n",
      "max           17      199            122             99      846  67.1   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "min                     0.078   21        0  \n",
      "max                     2.420   81        1  \n",
      "\n",
      "Synthetic Data Ranges:\n",
      "     Pregnancies     Glucose  BloodPressure  SkinThickness    Insulin  \\\n",
      "min     0.000000  107.182457      57.236176       0.978561   0.000000   \n",
      "max     3.975621  199.000000     105.648636      40.905426  52.785717   \n",
      "\n",
      "           BMI  DiabetesPedigreeFunction        Age  Outcome  \n",
      "min  20.395573                  0.078000  21.000000      0.0  \n",
      "max  54.334652                  0.857784  36.873569      1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:84: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  min_val = original_min[col]\n",
      "C:\\Users\\Tawfique\\AppData\\Local\\Temp\\ipykernel_34984\\3816964035.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  max_val = original_max[col]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class AutoencoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        # Decoders\n",
    "        self.decoder_features = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, input_dim - 1)\n",
    "        )\n",
    "        self.decoder_outcome = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        # Reshape for attention layer\n",
    "        encoded = encoded.unsqueeze(1)  # Shape: [batch_size, seq_len=1, hidden_dim]\n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(encoded, encoded, encoded)\n",
    "        attn_output = attn_output.squeeze(1)  # Shape: [batch_size, hidden_dim]\n",
    "        # Decode\n",
    "        decoded_features = self.decoder_features(attn_output)\n",
    "        decoded_outcome = self.decoder_outcome(attn_output)\n",
    "        return torch.cat([decoded_features, decoded_outcome], dim=1)\n",
    "\n",
    "def train_autoencoder(data, input_dim, hidden_dim, epochs=20, batch_size=32, lr=0.001):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    data_tensor = torch.FloatTensor(scaled_data)\n",
    "    \n",
    "    model = Autoencoder(input_dim, hidden_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i:i+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = criterion(output, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(data_tensor):.4f}\")\n",
    "    \n",
    "    return model, scaler, data_min, data_max\n",
    "\n",
    "def scale_to_original_range(synthetic_data, original_min, original_max):\n",
    "    \"\"\"Scale synthetic data to match original data ranges\"\"\"\n",
    "    df_synthetic = pd.DataFrame(synthetic_data)\n",
    "    for col in df_synthetic.columns[:-1]:  # Skip outcome column\n",
    "        min_val = original_min[col]\n",
    "        max_val = original_max[col]\n",
    "        df_synthetic[col] = df_synthetic[col].clip(lower=min_val, upper=max_val)\n",
    "    # Ensure outcome is binary\n",
    "    df_synthetic.iloc[:, -1] = df_synthetic.iloc[:, -1].round().clip(0, 1)\n",
    "    return df_synthetic.values\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load sample tabular data\n",
    "    path = '../Datasets/diabetes.csv'\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    input_dim = df.shape[1]  # Get number of features\n",
    "    hidden_dim = 5  # Latent space dimension\n",
    "\n",
    "    # Train the autoencoder with all return values\n",
    "    autoencoder, scaler, data_min, data_max = train_autoencoder(df, input_dim, hidden_dim, epochs=1000)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    num_synthetic_instances = 100\n",
    "    synthetic_data = autoencoder.generate_synthetic_data(num_synthetic_instances)\n",
    "    \n",
    "    # Inverse transform the synthetic data\n",
    "    synthetic_data = scaler.inverse_transform(synthetic_data)\n",
    "    \n",
    "    # Scale to original ranges\n",
    "    synthetic_data = scale_to_original_range(synthetic_data, data_min, data_max)\n",
    "    \n",
    "    # Convert synthetic data to dataframe\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    print(\"\\nGenerated Synthetic Data:\")\n",
    "    print(synthetic_df.head())\n",
    "    \n",
    "    # Print ranges comparison\n",
    "    print(\"\\nOriginal Data Ranges:\")\n",
    "    print(df.agg(['min', 'max']))\n",
    "    print(\"\\nSynthetic Data Ranges:\")\n",
    "    print(synthetic_df.agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv('diabetes_autoencoder_attention.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before augmentation: {0: 500, 1: 268}\n",
      "Recall score (original data): 0.6625\n",
      "Recall score (generated data): 0.8125\n",
      "F1 score (original data): 0.6503\n",
      "F1 score (generated data): 0.7784\n",
      "Classification Report (original data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       151\n",
      "           1       0.64      0.66      0.65        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.73      0.73       231\n",
      "weighted avg       0.76      0.75      0.75       231\n",
      "\n",
      "Classification Report (generated data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       151\n",
      "           1       0.75      0.81      0.78        80\n",
      "\n",
      "    accuracy                           0.84       231\n",
      "   macro avg       0.82      0.83      0.83       231\n",
      "weighted avg       0.84      0.84      0.84       231\n",
      "\n",
      "Number of fake samples generated: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "\n",
    "real_df = df\n",
    "augmented_df = pd.concat([real_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "X = real_df.iloc[:, :-1].values  # Features\n",
    "y = real_df.iloc[:, -1].values \n",
    "# Check and print the original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_dist_before = dict(zip(unique, counts))\n",
    "print(f\"Class distribution before augmentation: {class_dist_before}\")# Labels (binary classification)\n",
    "\n",
    "X_balanced = augmented_df.iloc[:, :-1].values  # Features\n",
    "y_balanced = augmented_df.iloc[:, -1].values  # Labels (binary classification)\n",
    "\n",
    "# Step 6: Split the dataset into training and test sets (original and balanced)\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a simple classifier on both original and generated datasets\n",
    "clf_orig = RandomForestClassifier(random_state=42)\n",
    "clf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "clf_bal = RandomForestClassifier(random_state=42)\n",
    "clf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Step 8: Predict and calculate recall and F1 scores\n",
    "y_pred_orig = clf_orig.predict(X_test_orig)\n",
    "y_pred_bal = clf_bal.predict(X_test_orig)\n",
    "\n",
    "recall_orig = recall_score(y_test_orig, y_pred_orig)\n",
    "recalls_bal = recall_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "f1_orig = f1_score(y_test_orig, y_pred_orig)\n",
    "f1_bal = f1_score(y_test_orig, y_pred_bal)\n",
    "\n",
    "# Step 9: Print the performance metrics\n",
    "print(f\"Recall score (original data): {recall_orig:.4f}\")\n",
    "print(f\"Recall score (generated data): {recalls_bal:.4f}\")\n",
    "print(f\"F1 score (original data): {f1_orig:.4f}\")\n",
    "print(f\"F1 score (generated data): {f1_bal:.4f}\")\n",
    "print(\"Classification Report (original data):\\n\", classification_report(y_test_orig, y_pred_orig))\n",
    "print(\"Classification Report (generated data):\\n\", classification_report(y_test_orig, y_pred_bal))\n",
    "\n",
    "\n",
    "print(f\"Number of fake samples generated: {len(augmented_df)-len(real_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
