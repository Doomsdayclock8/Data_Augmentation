{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m X_minority \u001b[38;5;241m=\u001b[39m X[y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Initialize the correlation-adjusted diffusion model\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m correlation_diffusion_model \u001b[38;5;241m=\u001b[39m \u001b[43mCorrelationAdjustedForestDiffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_minority\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Generate synthetic samples\u001b[39;00m\n\u001b[0;32m    120\u001b[0m num_samples_to_generate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_minority)  \u001b[38;5;66;03m# Generate the same number of samples as the minority class\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mCorrelationAdjustedForestDiffusion.__init__\u001b[1;34m(self, X, n_t, n_estimators, max_depth, n_jobs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorrelation_matrix)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize the Forest Diffusion model\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforest_diffusion \u001b[38;5;241m=\u001b[39m \u001b[43mForestDiffusionModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No labels for generation\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiffusion_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduplicate_K\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbin_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming no binning needed\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Assuming no categorical features\u001b[39;49;00m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mint_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Assuming no integer-specific processing\u001b[39;49;00m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Thesis\\Llama3_8B\\Llama-3.1-8B-Instruct\\LlamaENVpip\\Lib\\site-packages\\ForestDiffusion\\diffusion_with_trees_class.py:185\u001b[0m, in \u001b[0;36mForestDiffusionModel.__init__\u001b[1;34m(self, X, X_covs, label_y, n_t, model, diffusion_type, max_depth, n_estimators, eta, tree_method, reg_alpha, reg_lambda, subsample, num_leaves, duplicate_K, bin_indexes, cat_indexes, int_indexes, remove_miss, p_in_one, true_min_max_values, gpu_hist, n_z, eps, beta_min, beta_max, n_jobs, n_batch, seed, **xgboost_kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    184\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_batch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Data iterator, no need to duplicate, not make xt yet\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregr \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_splitted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_covs_splitted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_levels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_uniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregr \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)( \u001b[38;5;66;03m# using all cpus\u001b[39;00m\n\u001b[0;32m    188\u001b[0m             delayed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_parallel)(\n\u001b[0;32m    189\u001b[0m               X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduplicate_K, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_all)[i][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_y[j], :], \n\u001b[0;32m    190\u001b[0m               y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduplicate_K, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_y[j], :]\n\u001b[0;32m    191\u001b[0m               ) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_uniques\n\u001b[0;32m    192\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Thesis\\Llama3_8B\\Llama-3.1-8B-Instruct\\LlamaENVpip\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Thesis\\Llama3_8B\\Llama-3.1-8B-Instruct\\LlamaENVpip\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tawfique\\Thesis\\Llama3_8B\\Llama-3.1-8B-Instruct\\LlamaENVpip\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ForestDiffusion import ForestDiffusionModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings about feature names mismatch in KDE\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names\")\n",
    "\n",
    "class CorrelationAdjustedForestDiffusion:\n",
    "    def __init__(self, X, n_t=50, n_estimators=100, max_depth=10, n_jobs=-1):\n",
    "        self.X = X\n",
    "        self.n_t = n_t  # Number of time steps in diffusion\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "\n",
    "        # Standardize data using StandardScaler\n",
    "        self.scaler = StandardScaler().fit(X)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "\n",
    "        # Calculate the correlation matrix of the original data\n",
    "        self.correlation_matrix = np.corrcoef(X_scaled, rowvar=False)\n",
    "        \n",
    "        # Perform Cholesky decomposition to get the lower triangular matrix\n",
    "        self.L = np.linalg.cholesky(self.correlation_matrix)\n",
    "\n",
    "        # Initialize the Forest Diffusion model\n",
    "        self.forest_diffusion = ForestDiffusionModel(\n",
    "            X=X_scaled,\n",
    "            label_y=None,  # No labels for generation\n",
    "            n_t=n_t,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            n_jobs=n_jobs,\n",
    "            diffusion_type='flow',\n",
    "            duplicate_K=100,\n",
    "            bin_indexes=[],  # Assuming no binning needed\n",
    "            cat_indexes=[],  # Assuming no categorical features\n",
    "            int_indexes=[]   # Assuming no integer-specific processing\n",
    "        )\n",
    "\n",
    "    def apply_whitening(self, X):\n",
    "        \"\"\"Applies the whitening transformation using the Cholesky matrix.\"\"\"\n",
    "        return X @ np.linalg.inv(self.L.T)\n",
    "\n",
    "    def revert_whitening(self, X):\n",
    "        \"\"\"Reverts the whitening transformation.\"\"\"\n",
    "        return X @ self.L.T\n",
    "\n",
    "    def kde_filter(self, generated_X):\n",
    "        \"\"\"Filter generated samples using Kernel Density Estimation to retain high-density samples.\"\"\"\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(self.X)\n",
    "        log_probs = kde.score_samples(generated_X)\n",
    "\n",
    "        # Keep only the samples with density scores above the 50th percentile\n",
    "        threshold = np.percentile(log_probs, 50)\n",
    "        filtered_samples = generated_X[log_probs >= threshold]\n",
    "\n",
    "        return filtered_samples\n",
    "\n",
    "    def post_sampling_noise_reduction(self, generated_X):\n",
    "        \"\"\"Reduces noise in generated data by smoothing and ensuring consistency.\"\"\"\n",
    "        # Clip extreme values to reduce outliers\n",
    "        generated_X = np.clip(generated_X, -1e5, 1e5)\n",
    "\n",
    "        # Smooth data if needed (optional)\n",
    "        smoothed_X = np.zeros_like(generated_X)\n",
    "        for i in range(generated_X.shape[1]):\n",
    "            smoothed_X[:, i] = np.interp(np.arange(generated_X.shape[0]), np.arange(generated_X.shape[0]), generated_X[:, i])\n",
    "\n",
    "        return smoothed_X\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        \"\"\"Generate samples using Forest Diffusion in the whitened space and apply correlation adjustments.\"\"\"\n",
    "        # Generate samples with Forest Diffusion model in the whitened space\n",
    "        try:\n",
    "            samples = self.forest_diffusion.generate(batch_size=num_samples, n_t=self.n_t)\n",
    "        except AttributeError:\n",
    "            print(\"generate method not found. Please check the available methods.\")\n",
    "            return np.empty((num_samples, self.n_features))\n",
    "\n",
    "        # Apply whitening to transform the data into uncorrelated space\n",
    "        whitened_samples = self.apply_whitening(samples)\n",
    "\n",
    "        # After generating in the whitened space, we need to revert to the original space (with correlations)\n",
    "        reverted_samples = self.revert_whitening(whitened_samples)\n",
    "\n",
    "        # Inverse scaling to original scale\n",
    "        reverted_samples = self.scaler.inverse_transform(reverted_samples)\n",
    "\n",
    "        # Apply KDE filtering to retain high-density samples\n",
    "        reverted_samples = self.kde_filter(reverted_samples)\n",
    "\n",
    "        # Reduce post-sampling noise\n",
    "        reverted_samples = self.post_sampling_noise_reduction(reverted_samples)\n",
    "\n",
    "        return reverted_samples\n",
    "\n",
    "\n",
    "# Load the Pima Indians Diabetes dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "                'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "data = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# Separate features and minority class (Outcome == 1)\n",
    "X = data.drop('Outcome', axis=1)\n",
    "y = data['Outcome']\n",
    "X_minority = X[y == 1]\n",
    "\n",
    "# Initialize the correlation-adjusted diffusion model\n",
    "correlation_diffusion_model = CorrelationAdjustedForestDiffusion(X_minority, n_t=50)\n",
    "\n",
    "# Generate synthetic samples\n",
    "num_samples_to_generate = len(X_minority)  # Generate the same number of samples as the minority class\n",
    "synthetic_samples = correlation_diffusion_model.generate_samples(num_samples_to_generate)\n",
    "\n",
    "# Integrate synthetic samples back into the dataset\n",
    "X_final = np.vstack((X, synthetic_samples))\n",
    "y_final = np.concatenate((y, np.ones(len(synthetic_samples))))  # Label generated samples as '1'\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "X_final_df = pd.DataFrame(X_final, columns=X.columns)\n",
    "y_final_df = pd.Series(y_final, name=\"Outcome\")\n",
    "final_data = pd.concat([X_final_df, y_final_df], axis=1)\n",
    "final_data.to_csv('generated_pima_diabetes_data_with_whitened_diffusion.csv', index=False)\n",
    "\n",
    "print(\"Generated data saved to 'generated_pima_diabetes_data_with_whitened_diffusion.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaENVpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
