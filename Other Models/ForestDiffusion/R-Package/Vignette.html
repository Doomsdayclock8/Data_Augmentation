<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Alexia Jolicoeur-Martineau" />

<meta name="date" content="2023-09-22" />

<title>How to use Diffusion Forests to generate and impute missing data (from basic to advanced usage)</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">How to use Diffusion Forests to generate
and impute missing data (from basic to advanced usage)</h1>
<h4 class="author">Alexia Jolicoeur-Martineau</h4>
<h4 class="date">2023-09-22</h4>



<p>You can cite this work as:</p>
<p><em>Jolicoeur-Martineau, A, Fatras, K., Kachman, T. (2023).
Generating and Imputing Tabular Data via Diffusion and Flow-based
Gradient-Boosted Trees. arXiv preprint arXiv:2309.09968.</em></p>
<div id="score-based-diffusion-and-flow-based-models-high-level-idea" class="section level2">
<h2>Score-based diffusion and flow-based models (high-level idea)</h2>
<p>You can refer to the paper if you want to know more about the
mathematics and the algorithm. In this vignette, I will stay at a high
level.</p>
<p>The idea behind score-based diffusion models is that we can define a
forward process that adds increasing amounts of Gaussian noise over time
to slowly move from a real data sample (at <span class="math inline">\(t=0\)</span>) to pure Gaussian noise (at <span class="math inline">\(t=1\)</span>). The magic is that it can be shown
that this process is reversible, which means that we can go in reverse
from pure noise (<span class="math inline">\(t=1\)</span>) to real data
(<span class="math inline">\(t=0\)</span>) and thus generate new data
samples from pure noise. To reverse the process, we need to learn the
score-function (gradient log density) with a function approximator
(XGBoost is used in this case).</p>
<p>Alternatively, flow-based models define a deterministic forward
process moving from real data to pure Gaussian noise. Then, they learn
the gradient flow, which can be used to move in reverse (from noise to
data).</p>
<p>Both diffusion (stochastic SDE-based) and flow (deterministic
ODE-based) methods are available in this package. To my knowledge, this
is the first R package implementing diffusion and flow models.</p>
</div>
<div id="hyperparameters" class="section level2">
<h2>Hyperparameters</h2>
<p>You must set the maximum amount of cores used by ForestDiffusion
through the argument <span class="math inline">\(n_{cores}\)</span>. The
amount of cores you have depends on your CPU. The training of the <span class="math inline">\(pn_t\)</span> models (where <span class="math inline">\(p\)</span> is the number of variables and <span class="math inline">\(n_t\)</span> the number of noise levels, which
defaults to 50) is parallelized over the <span class="math inline">\(n_{cores}\)</span> cores. The get the maximum
performance, set the argument <span class="math inline">\(n_{cores}\)</span> to NULL in order to use all
available cores. The more cores you use, the faster it will be. On the
other hand, memory can be a problem, especially if you have a lot of
cores; please see the section below on memory management.</p>
<p>We list the important hyperparameters below, their default values,
and how to choose them:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>n_cores <span class="ot">=</span> <span class="cn">NULL</span> <span class="co"># maximum amount of cores used; leaving it at NULL will use all availables cores; higher values increase training speed, but also memory cost</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>X <span class="co"># your dataset </span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>label_y <span class="ot">=</span> None <span class="co"># provide the outcome variable if it is categorical for improved performance by training separate models per class (training will be slower); cannot contain missing values</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>name_y <span class="ot">=</span> <span class="st">&#39;y&#39;</span> <span class="co"># Name of label_y variable if provided</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>n_t <span class="ot">=</span> <span class="dv">50</span> <span class="co"># number of noise levels (and sampling steps); increasing it could (maybe) improve performance, but it slows down training and sampling</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>flow <span class="ot">=</span> <span class="cn">TRUE</span> <span class="co"># type of process (flow = ODE, vp = SDE); vp generally has slightly worse performance, but it is the only method that can be used for imputation</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>duplicate_K <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of noise per sample (or equivalently the number of times the rows of the dataset are duplicated); higher values lead to better performance, but also increase the memory demand</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>seed <span class="ot">=</span> <span class="dv">666</span> <span class="co"># random seed value</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>max_depth <span class="ot">=</span> <span class="dv">7</span> <span class="co"># max depth of the tree</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>n_estimators <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of trees per XGBoost model</span></span></code></pre></div>
<p>Regarding the imputation with REPAINT, there are two important
hyperparameters:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>r <span class="ot">=</span> <span class="dv">10</span> <span class="co"># number of repaints, 5 or 10 is good</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>j <span class="ot">=</span> <span class="fl">0.1</span> <span class="co"># percentage of the jump size; should be around 10% of n_t</span></span></code></pre></div>
</div>
<div id="potential-memory-problems-and-solutions" class="section level2">
<h2>Potential memory problems and solutions 😭</h2>
<p>Our method trains <span class="math inline">\(pn_t\)</span> models in
parallel using CPU cores, where p is the number of variables and n_t is
the number of noise levels. Furthermore, we make the dataset much bigger
by duplicating the rows many times (100 times is the default). To speed
up the training, you will need as many cores as possible. Training the
multiple models using only 4 cores could take a long time. However, the
more cores you use, the higher the memory cost will be! This is because
each worker/CPU will train its own model, which will require its own
amount of memory (RAM). So, there is a balance to be reached between
enough cores for speed but not too much so that it doesn’t blow up the
memory.</p>
<p>We provide below some hyperparameters that can be changed to reduce
the memory load:</p>
<pre><code>n_cores = NULL # this can be used to limit the maximum number of cores and thus the memory
duplicate_K = 100 # lowering this value will reduce memory demand and possibly performance (memory is proportional to this value)
n_t = 50 # reducing this value could reduce memory demand and performance (ideally stay at n_t=50 or higher)
label_y = None # using None will reduce memory demand (since using this will train n_classes times more models)
max_depth = 7 # reducing the depth of trees will reduce memory demand
n_estimators = 100 # reducing the number of trees will reduce memory demand</code></pre>
</div>
<div id="generating-data" class="section level2">
<h2>Generating data</h2>
<p>Let’s use the Iris dataset as an example. Since it performs better,
we will use the flow method to generate fake samples. Note that the
dataset can contain missing values since XGBoost can handle NAs, yet the
generated data will never have missing values (isn’t it great?).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">library</span>(ForestDiffusion)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># Load iris</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="fu">data</span>(iris)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co"># variables 1 to 4 are the input X</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co"># variable 5 (iris$Species) is the outcome (class with 3 labels)</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co"># Add NAs (but not to label) to emulate having a dataset with missing values</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> missForest<span class="sc">::</span><span class="fu">prodNA</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">noNA =</span> <span class="fl">0.2</span>)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co"># Setup data</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">data.frame</span>(iris[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>])</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>y <span class="ot">=</span> iris<span class="sc">$</span>Species</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>Xy <span class="ot">=</span> iris</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a><span class="fu">plot</span>(Xy)</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a><span class="co"># When you do not want to train a seperate model per model (or you have a regression problem), you can provide the dataset together</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>forest_model <span class="ot">=</span> <span class="fu">ForestDiffusion</span>(<span class="at">X=</span>Xy, <span class="at">n_cores=</span><span class="dv">4</span>, <span class="at">n_t=</span><span class="dv">50</span>, <span class="at">duplicate_K=</span><span class="dv">100</span>, <span class="at">flow=</span><span class="cn">TRUE</span>, <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>Xy_fake <span class="ot">=</span> <span class="fu">ForestDiffusion.generate</span>(forest_model, <span class="at">batch_size=</span><span class="fu">NROW</span>(Xy), <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a><span class="fu">plot</span>(Xy_fake)</span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a><span class="co"># When the outcome y is categorical, you can provide it seperately to construct a seperate model per label (this can improve performance, but it will be slower)</span></span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>forest_model <span class="ot">=</span> <span class="fu">ForestDiffusion</span>(<span class="at">X=</span>X, <span class="at">n_cores=</span><span class="dv">4</span>, <span class="at">label_y=</span>y, <span class="at">name_y=</span><span class="st">&#39;Species&#39;</span>, <span class="at">n_t=</span><span class="dv">50</span>, <span class="at">duplicate_K=</span><span class="dv">100</span>, <span class="at">flow=</span><span class="cn">TRUE</span>, <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>Xy_fake <span class="ot">=</span> <span class="fu">ForestDiffusion.generate</span>(forest_model, <span class="at">batch_size=</span><span class="fu">NROW</span>(Xy), <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a><span class="fu">plot</span>(Xy_fake)</span></code></pre></div>
<p>Now that you have your fake data, you can use it in your own models
directly or combine it with the real data.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Use the real data to fit a GLM</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glm</span>(Species <span class="sc">~</span> Sepal.Length, <span class="at">family =</span> <span class="st">&#39;binomial&#39;</span>, <span class="at">data=</span>Xy)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># Use fake data to fit a GLM</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glm</span>(Species <span class="sc">~</span> Sepal.Length, <span class="at">family =</span> <span class="st">&#39;binomial&#39;</span>, <span class="at">data=</span>Xy_fake)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="fu">summary</span>(fit)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co"># Use data augmentation (equal real with equal fake data) to fit a GLM</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>X_combined <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">rbind</span>(Xy, Xy_fake))</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">glm</span>(Species <span class="sc">~</span> Sepal.Length, <span class="at">family =</span> <span class="st">&#39;binomial&#39;</span>, <span class="at">data=</span>X_combined)</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
</div>
<div id="data-augmentation" class="section level2">
<h2>Data augmentation</h2>
<p>One possible application of our method is data augmentation
(augmenting real data with additional fake samples) to improve various
methods. Here is an example of data augmentation with missForest.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">library</span>(missForest)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="co"># Normally, you would use missForest as follows</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>mf <span class="ot">=</span> missForest<span class="sc">::</span><span class="fu">missForest</span>(Xy, <span class="at">verbose =</span> <span class="cn">TRUE</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co"># Instead, you can now use data augmentation</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>Xy_fake <span class="ot">=</span> <span class="fu">ForestDiffusion.generate</span>(forest_model, <span class="at">batch_size=</span><span class="fu">NROW</span>(Xy), <span class="at">seed=</span><span class="dv">666</span>) <span class="co"># generates as much fake as real data</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>X_combined <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="fu">rbind</span>(Xy, Xy_fake)) <span class="co"># combine real and fake data</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>mf_dataug <span class="ot">=</span> <span class="fu">missForest</span>(X_combined, <span class="at">verbose =</span> <span class="cn">TRUE</span>) <span class="co"># train missForest with augmented data</span></span></code></pre></div>
</div>
<div id="accounting-for-uncertainty-using-multiple-fake-datasets-akin-to-multiple-imputations" class="section level2">
<h2>Accounting for uncertainty using multiple fake datasets (akin to
multiple imputations)</h2>
<p>Training a single model with fake or data-augmented data is nice, but
it might not account for uncertainty since you trained a single model.
When imputing data, we generally want to use multiple imputed datasets,
train our model on each imputed dataset, and then pool the results in
order to account for the different possible imputations. We can apply
the same idea here but with fake data! Let me show you how.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">library</span>(mice)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co"># Generate fake data</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>ngen <span class="ot">=</span> <span class="dv">9</span> <span class="co"># number of generated datasets we want</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>Xy_fake <span class="ot">=</span> <span class="fu">ForestDiffusion.generate</span>(forest_model, <span class="at">batch_size=</span>ngen<span class="sc">*</span><span class="fu">NROW</span>(Xy), <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co"># Make a list of fake datasets</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>data_list <span class="ot">=</span> <span class="fu">split</span>(Xy_fake, <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>ngen, <span class="at">each=</span><span class="fu">NROW</span>(Xy)))</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co"># Fit a model per fake dataset</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>fits <span class="ot">&lt;-</span> <span class="fu">with_datasets</span>(data_list, <span class="fu">glm</span>(Species <span class="sc">~</span> Sepal.Length, <span class="at">family =</span> <span class="st">&#39;binomial&#39;</span>))</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="co"># Pool the results</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>mice<span class="sc">::</span><span class="fu">pool</span>(fits) </span></code></pre></div>
</div>
<div id="multiple-imputation" class="section level2">
<h2>Multiple imputation</h2>
<p>Below, we show how to impute missing data using ForestDiffusion.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">library</span>(mice)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>nimp <span class="ot">=</span> <span class="dv">5</span> <span class="co"># number of imputations needed</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co"># Must train a VP diffusion model (instead of a Flow model) to be able to impute data</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>forest_model_vp <span class="ot">=</span> <span class="fu">ForestDiffusion</span>(<span class="at">X=</span>Xy, <span class="at">n_cores=</span><span class="dv">4</span>, <span class="at">n_t=</span><span class="dv">50</span>, <span class="at">duplicate_K=</span><span class="dv">100</span>, <span class="at">flow=</span><span class="cn">FALSE</span>, <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>Xy_imp <span class="ot">=</span> <span class="fu">ForestDiffusion.impute</span>(forest_model_vp, <span class="at">k=</span>nimp, <span class="at">seed=</span><span class="dv">666</span>) <span class="co"># regular imputations (fast)</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>Xy_imp <span class="ot">=</span> <span class="fu">ForestDiffusion.impute</span>(forest_model_vp, <span class="at">repaint=</span><span class="cn">TRUE</span>, <span class="at">r=</span><span class="dv">10</span>, <span class="at">j=</span><span class="dv">5</span>, <span class="at">k=</span>nimp, <span class="at">seed=</span><span class="dv">666</span>) <span class="co"># REPAINT imputations (slow, but better)</span></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="fu">plot</span>(Xy_imp[[<span class="dv">1</span>]]) <span class="co"># plot the first imputed dataset</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="co"># When the outcome y is categorical, you can provide it seperately to construct a seperate model per label (this can improve performance, but it will be slower)</span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>forest_model_vp <span class="ot">=</span> <span class="fu">ForestDiffusion</span>(<span class="at">X=</span>X, <span class="at">n_cores=</span><span class="dv">4</span>, <span class="at">label_y=</span>y, <span class="at">name_y=</span><span class="st">&#39;Species&#39;</span>, <span class="at">n_t=</span><span class="dv">50</span>, <span class="at">duplicate_K=</span><span class="dv">100</span>, <span class="at">flow=</span><span class="cn">TRUE</span>, <span class="at">seed=</span><span class="dv">666</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>Xy_imp <span class="ot">=</span> <span class="fu">ForestDiffusion.impute</span>(forest_model_vp, <span class="at">k=</span>nimp, <span class="at">seed=</span><span class="dv">666</span>) <span class="co"># regular imputations (fast)</span></span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>Xy_imp <span class="ot">=</span> <span class="fu">ForestDiffusion.impute</span>(forest_model_vp, <span class="at">repaint=</span><span class="cn">TRUE</span>, <span class="at">r=</span><span class="dv">10</span>, <span class="at">j=</span><span class="dv">5</span>, <span class="at">k=</span>nimp, <span class="at">seed=</span><span class="dv">666</span>) <span class="co"># REPAINT imputations (slow, but better)</span></span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a><span class="fu">plot</span>(Xy_imp[[<span class="dv">1</span>]]) <span class="co"># plot the first imputed dataset</span></span></code></pre></div>
<p>Now that you have created multiple imputations, you can use fit one
model per imputation and pool the results.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Fit a model per imputed dataset</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>fits <span class="ot">&lt;-</span> <span class="fu">with_datasets</span>(Xy_imp, <span class="fu">glm</span>(Species <span class="sc">~</span> Sepal.Length, <span class="at">family =</span> <span class="st">&#39;binomial&#39;</span>))</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co"># Pool the results</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>mice<span class="sc">::</span><span class="fu">pool</span>(fits) </span></code></pre></div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
